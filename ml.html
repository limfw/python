<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Basic Machine Learning with SAS Viya & Python</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="header"> 
	            <p>Basic - Machine Learning Techniques for Data Mining</p>
    </div>
    <div class="main-container">
        <div class="sidebar">
            <h2 class="orange-accent">Course Content</h2>
			<div class="menu-item"><a href="index.html" style="text-decoration: none; color: inherit;">Basic Python</a></div>
            <div class="menu-item" onclick="showContent('welcome')">Course Overview</div>
			
            <div class="menu-item" onclick="toggleSubMenu('ml_basics')">Machine Learning Basics</div>
            <div id="ml_basics" class="sub-menu">
                <div class="menu-item" onclick="toggleSubMenu('intro_ml')">Introduction to Machine Learning</div>
                <div id="intro_ml" class="sub-menu">
                    <div onclick="showContent('ml_overview')">What is Machine Learning?</div>
                    <div onclick="showContent('ml_types')">Types of Machine Learning</div>
                    <div onclick="showContent('ml_applications')">ML Applications</div>
                </div>
                
                <div class="menu-item" onclick="toggleSubMenu('ml_workflow')">Machine Learning Workflow</div>
                <div id="ml_workflow" class="sub-menu">
                    <div onclick="showContent('ml_pipeline')">ML Pipeline</div>
                    <div onclick="showContent('data_preprocessing')">Data Preprocessing</div>
                    <div onclick="showContent('feature_engineering')">Feature Engineering</div>
					<div onclick="showContent('model_selection')">Model Selection</div>
					<div onclick="showContent('model_evaluation')">Model Evaluation</div>
                </div>
                
                <div class="menu-item" onclick="toggleSubMenu('ml_algorithms')">Key Machine Learning Algorithms</div>
                <div id="ml_algorithms" class="sub-menu">
                    <div onclick="showContent('regression')">Regression</div>
                    <div onclick="showContent('knn')">k-Nearest Neighbors (k-NN)</div>
                    <div onclick="showContent('svm')">Support Vector Machines (SVM)</div>
                    <div onclick="showContent('kernel_regression')">Kernel Regression</div>
                    <div onclick="showContent('expectation_maximization')">Expectation Maximization</div>
                </div>
                
                <div class="menu-item" onclick="toggleSubMenu('decision_trees')">Decision Tree Learning</div>
                <div id="decision_trees" class="sub-menu">
                    <div onclick="showContent('entropy')">Entropy Measures</div>
                    <div onclick="showContent('id3')">ID3 Algorithm</div>
                </div>
                
                <div class="menu-item" onclick="toggleSubMenu('ensemble_learning')">Ensemble Learning & Advanced ML</div>
                <div id="ensemble_learning" class="sub-menu">
                    <div onclick="showContent('bootstraping')">Bootstraping</div>
                    <div onclick="showContent('bagging')">Bagging</div>
                    <div onclick="showContent('random_forest')">Random Forest</div>
                </div>
                
                <div class="menu-item" onclick="toggleSubMenu('ann')">Artificial Neural Networks (ANN)</div>
                <div id="ann" class="sub-menu">
                    <div onclick="showContent('mlp')">Multilayer Networks</div>
                    <div onclick="showContent('backpropagation')">Backpropagation Algorithm</div>
                    <div onclick="showContent('gradient_descent')">Gradient Descent</div>
                    <div onclick="showContent('rbf')">Radial Basis Function Network</div>
                    <div onclick="showContent('autoregressive_ann')">Autoregressive Time Series Using ANN</div>
                </div>
                
                <div class="menu-item" onclick="toggleSubMenu('reinforcement_learning')">Reinforcement Learning</div>
                <div id="reinforcement_learning" class="sub-menu">
                    <div onclick="showContent('reward_signal')">Reward Signal</div>
                    <div onclick="showContent('action_policy')">Action Policy</div>
                    <div onclick="showContent('mdp')">Markov Decision Process</div>
                    <div onclick="showContent('q_learning')">Q-Learning</div>
                </div>
            </div>
			<div class="menu-item" onclick="toggleSubMenu('sas_ml')">Practice with SAS viya </div>
            <div id="sas_ml" class="sub-menu">
                    <div onclick="showContent('sas_viya_intro')">Introduction to SAS Viya</div>
						<div onclick="showContent('viyaP1')">Practical 1</div>
						<div onclick="showContent('viyaP2')">Practical 2</div>
						<div onclick="showContent('viyaP3')">Practical 3</div>
						<div onclick="showContent('viyaP4')">Practical 4</div>
						<div onclick="showContent('viyaP5')">Practical 5</div>
						<div onclick="showContent('viyaP6')">Practical 6</div>
						<div onclick="showContent('viyaP7')">Practical 7</div>
						<div onclick="showContent('viyaP8')">Practical 8</div>
						<div onclick="showContent('viyaP9')">Practical 9</div>
						<div onclick="showContent('viyaP10')">Practical 10</div>
						<div onclick="showContent('viyaP11')">Practical 11</div>
						<div onclick="showContent('viyaP12')">Practical 12</div>
						<div onclick="showContent('viyaP13')">Practical 13</div>
						<div onclick="showContent('viyaP14')">Practical 14</div>						
                </div>
			<div class="menu-item" onclick="toggleSubMenu('python_ml')">Practice with Python</div>
            <div id="python_ml" class="sub-menu">
                    <div onclick="showContent('python_libraries')">Python Libraries for ML</div>
						<div onclick="showContent('pythonP1')">Practical 1</div>
						<div onclick="showContent('pythonP2')">Practical 2</div>
						<div onclick="showContent('pythonP3')">Practical 3</div>
						<div onclick="showContent('pythonP4')">Practical 4</div>
						<div onclick="showContent('pythonP5')">Practical 5</div>
						<div onclick="showContent('pythonP6')">Practical 6</div>
						<div onclick="showContent('pythonP7')">Practical 7</div>
						<div onclick="showContent('pythonP8')">Practical 8</div>
						<div onclick="showContent('pythonP9')">Practical 9</div>
						<div onclick="showContent('pythonP10')">Practical 10</div>
						<div onclick="showContent('pythonP11')">Practical 11</div>
						<div onclick="showContent('pythonP12')">Practical 12</div>
						<div onclick="showContent('pythonP13')">Practical 13</div>
						<div onclick="showContent('pythonP14')">Practical 14</div>					
                </div>			
			<div class="menu-item"><a href="master_ml.html" style="text-decoration: none; color: inherit;">Machine Learning : Intermediate</a></div>
        </div>
		
		
        <div class="content" id="content-area">
            <h1 class="blue-bolt">Basic Machine Learning Techniques for Data Mining</h1>
				<p>
					The course focuses on the application of machine learning techniques to extracting information from data.
					The aim is to provide you with a set of practical tools that can be applied to solve real-world problems.
				</p>

				<p>
					The main topics that will be discussed include:
					decision tree learning, artificial neural networks, Bayesian learning, clustering, density estimation,
					ensemble learners, boosting, bagging, random forest, and reinforcement learning.
				</p>

				<p>
					This course also emphasizes practical implementation using <strong>Python</strong> and <strong>SAS Viya</strong>.
					You will learn how to apply machine learning techniques using Python's powerful libraries (such as 
					Scikit-learn, TensorFlow, and Pandas) and SAS Viya’s advanced analytics tools for large-scale data processing.
				</p>

				<p>
					By the end of this course, you will be able to leverage <strong>Python</strong> and <strong>SAS Viya</strong> to build, train, and deploy 
					machine learning models efficiently.
				</p>
        </div>
    </div>
    
        <div class="footer">
            <p>Prepare by 😀 version 1.5 - 2025 ! &  Build a Better Future begin from 2025 !</p>
        </div>
    <script>
	function toggleSubMenu(id) {
		var submenu = document.getElementById(id);
		if (submenu) {
			submenu.style.display = submenu.style.display === 'block' ? 'none' : 'block';
		}
	}

	function showContent(topic, element) {
		let contentArea = document.getElementById('content-area');

		let content = {
			'welcome': `
				<h1 class="blue-bolt">Course Overview</h1>
				<p>This course is designed to provide a practical approach to machine learning by leveraging the power of 
				   <strong>Python</strong> and <strong>SAS Viya</strong>. You will learn key techniques used in real-world applications, 
				   including data preprocessing, model training, and performance evaluation.</p>
				<p>Throughout this course, we will cover essential topics such as:</p>
				<ul>
					<li>Supervised and Unsupervised Learning</li>
					<li>Decision Trees and Neural Networks</li>
					<li>Ensemble Methods like Bagging & Boosting</li>
					<li>Reinforcement Learning & Markov Decision Processes</li>
					<li>Hands-on implementation using Python & SAS Viya</li>
				</ul>
				<p>By the end of this course, you will have the skills to build and deploy machine learning models, 
				   extract insights from data, and apply AI techniques to solve business problems.</p>
				<p> Get ready to explore the world of Machine Learning with <strong>Python & SAS Viya</strong>!</p>
			`,
			
			'ml_overview': `
				<div class="definition-box">
					💡 <strong class="blue-bolt">Simple Definition:</strong><br>
					<p>
					Machine Learning is a process where computers learn from experience (data) to perform tasks automatically, 
					without human intervention. 
					</p>
				</div>
				
				<p>
				Machine learning is used in many industries. Here are some applications:
				</p>
					<table class="styled-table">
						<thead>
							<tr>
								<th> 🌍 Examples of Machine Learning:</th>
								<th> Usage </th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>📧 Email Spam Detection</td>
								<td>Identifies and filters out spam emails.</td>
							</tr>
							<tr>
								<td>💳 Fraud Detection</td>
								<td>Banks use ML to detect fraudulent transactions.</td>
							</tr>
							<tr>
								<td>🏥 Medical Diagnosis</td>
								<td>ML helps in detecting diseases like cancer from medical images.</td>
							</tr>
							<tr>
								<td>🎥 Recommendation Systems</td>
								<td>Netflix, YouTube, and Spotify suggest content based on user behavior.</td>
							</tr>
							<tr>
								<td>🚗 Self-Driving Cars</td>
								<td>Tesla uses ML to recognize objects and make driving decisions.</td>
							</tr>
							<tr>
								<td>📞 Customer Support AI</td>
								<td>Chatbots analyze text and voice to provide automated support.</td>
							</tr>
						</tbody>
					</table>
								
				<h2> ML Related Topics in AI 🤖</h2>
				<p> Machine Learning is a subset of AI, here are certain topics related to it:</p>
					<div class="related-topics">
						<div class="topic-box">
							<h3>&#x31;&#xFE0F;&#x20E3; Artificial Intelligence (AI) 🧠</h3>
							<p>The broader field that includes <strong>machine learning, deep learning, robotics, and expert systems.</strong></p>
							<p>AI systems can simulate <strong>human reasoning, problem-solving, and decision-making.</strong></p>
						</div>

						<div class="topic-box">
							<h3>&#x32;&#xFE0F;&#x20E3; Deep Learning (DL) 🏗️</h3>
							<p>A specialized type of ML that uses <strong>artificial neural networks (ANNs)</strong> to learn from large amounts of data.</p>
							<p>Example: Facial recognition in smartphones (<strong>Face ID 📱</strong>).</p>
						</div>

						<div class="topic-box">
							<h3>&#x33;&#xFE0F;&#x20E3; Natural Language Processing (NLP) 💬</h3>
							<p>A branch of AI that helps machines <strong>understand, interpret, and generate human language.</strong></p>
							<p>Example: <strong>ChatGPT, Gemini, Copilot</strong>, and voice assistants like <strong>Siri and Alexa</strong>.</p>
						</div>

						<div class="topic-box">
							<h3>&#x34;&#xFE0F;&#x20E3; Generative AI 🎨📝</h3>
							<p>A field in AI where models can <strong>create new content</strong> (text, images, music, etc.).</p>
							<p>Example: <strong>ChatGPT</strong> generates human-like text; <strong>DALL·E</strong> creates images from text descriptions.</p>
						</div>
					</div>
			`,




			'ml_types': `
					<div class="ml-types">
						<h1 class='blue-bolt'> Types of Machine Learning</h1>

						<div class="ml-box">
							<h3>Supervised Learning 🏫</h3>

							<p><strong>Definition:</strong> Supervised learning is a type of ML where the model is trained on <strong>labeled data</strong>. 
							The algorithm learns by mapping input data to known output labels.</p>

							<p><strong>💡 Example Applications:</strong></p>
							<ul>
								<li> <strong>Spam Email Detection 📧</strong> (Label: Spam or Not Spam)</li>
								<li> <strong>Fraud Detection 💳</strong> (Label: Fraudulent or Legitimate Transaction)</li>
								<li> <strong>Disease Prediction 🏥</strong> (Label: "Positive" or "Negative" diagnosis)</li>
							</ul>

							<p><strong>🔹 How It Works:</strong></p>
							<ul>
								<li>The model is given <strong>input data (X)</strong> and the <strong>correct output (Y)</strong>.</li>
								<li>It <strong>learns the relationship</strong> between input and output.</li>
								<li>When given <strong>new data</strong>, it predicts the output.</li>
							</ul>

							<p><strong> Example: Predicting House Prices 🏡</strong></p>
							<p>If trained with data like <strong>house size, location, and price</strong>, the model can predict <strong>the price of a new house</strong>.</p>
						</div>

						<div class="ml-box">
							<h3>Unsupervised Learning 🔍</h3>

							<p><strong>Definition:</strong> Unsupervised learning is used when there are <strong>no labels in the data</strong>. 
							The algorithm tries to <strong>find patterns, clusters, or relationships</strong> in the dataset.</p>

							<p><strong>💡 Example Applications:</strong></p>
							<ul>
								<li> <strong>Customer Segmentation 🎯</strong> (Grouping similar customers for marketing)</li>
								<li> <strong>Anomaly Detection 🚨</strong> (Detecting unusual patterns in network security)</li>
								<li> <strong>Image Compression 🖼️</strong> (Grouping similar pixels to reduce file size)</li>
							</ul>

							<p><strong>🔹 How It Works:</strong></p>
							<ul>
								<li>The algorithm explores the structure of the data <strong>without predefined labels</strong>.</li>
								<li>It identifies <strong>hidden patterns</strong> and groups data points accordingly.</li>
							</ul>

							<p><strong> Example: Clustering Customers 🎯</strong></p>
							<p>If a shopping website has <strong>customer data (age, purchase history, location)</strong>, an unsupervised model can 
							group them into <strong>different customer segments</strong>.</p>
						</div>

						<div class="ml-box">
							<h3>Reinforcement Learning 🎮</h3>

							<p><strong>Definition:</strong> Reinforcement learning (RL) is <strong>learning by interaction</strong>. 
							The model learns from <strong>rewards and punishments</strong> to make better decisions over time.</p>

							<p><strong>💡 Example Applications:</strong></p>
							<ul>
								<li> <strong>Self-Driving Cars 🚗</strong> (Learning how to drive by trial and error)</li>
								<li> <strong>Game AI 🕹️</strong> (AI learning to play chess or video games)</li>
								<li> <strong>Robotics 🤖</strong> (Training a robot to pick up objects efficiently)</li>
							</ul>

							<p><strong>🔹 How It Works:</strong></p>
							<ul>
								<li>An <strong>agent</strong> interacts with an <strong>environment</strong>.</li>
								<li>It performs <strong>actions</strong> and receives <strong>rewards or penalties</strong>.</li>
								<li>It <strong>learns from experience</strong> to maximize rewards over time.</li>
							</ul>

							<p><strong> Example: Training a Robot 🤖</strong></p>
							<p>A robot in a factory learns <strong>the best way to pick up and place objects</strong> by receiving 
							<strong>positive rewards</strong> for correct actions and <strong>penalties for mistakes</strong>.</p>
						</div>
					</div>
			`,			

			'ml_applications': `
					<div class="ml-application">
						<h1 class='blue-bolt'>1. Spam Email Detection 📧</h1>

						<!-- Background Section -->
						<div class="ml-box">
							<h3>Background: How Spam Detection Works?</h3>
							<p><strong>Definition:</strong> Spam email detection is a classification problem in ML where the system learns to differentiate between 
							   <strong>spam (junk)</strong> and <strong>non-spam (legitimate)</strong> emails.</p>
							
							<p><strong>💡 How It Works:</strong></p>
							<ul>
								<li>The system analyzes <strong>content, metadata, and sender details</strong>.</li>
								<li>It assigns <strong>probability scores</strong> to classify emails.</li>
								<li>If the probability exceeds a threshold (e.g., <strong>80% spam</strong>), the email is sent to the spam folder.</li>
							</ul>

							<p><strong>🔹 Algorithms Used:</strong></p>
							<ul>
								<li>✅ <strong>Naïve Bayes Classifier</strong> (Probabilistic text classification)</li>
								<li>✅ <strong>Decision Trees</strong> (Pattern recognition from training data)</li>
								<li>✅ <strong>Neural Networks (ANNs & LSTMs)</strong> (Deep Learning for spam detection)</li>
							</ul>
						</div>

						<!-- Code Section -->
						<div class="ml-box">
							<h3> Idea Demonstration : Spam Detection in Python</h3>
							<pre class="code-block">
		import pandas as pd
		import numpy as np
		from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
		from sklearn.model_selection import train_test_split
		from sklearn.naive_bayes import MultinomialNB
		from sklearn.pipeline import Pipeline
		from sklearn.metrics import accuracy_score, classification_report

		df = pd.read_csv("spam_dataset.csv")
		df['label'] = df['label'].map({'spam': 1, 'ham': 0})

		X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.2, random_state=42)

		spam_detector = Pipeline([
			('vectorizer', CountVectorizer()),  
			('tfidf', TfidfTransformer()),     
			('classifier', MultinomialNB())     
		])

		spam_detector.fit(X_train, y_train)
		y_pred = spam_detector.predict(X_test)

		print("Accuracy:", accuracy_score(y_test, y_pred))
		print("Classification Report:", classification_report(y_test, y_pred))
							</pre>
						</div>

						<!-- Results Section -->
						<div class="ml-box">
							<h3> Results Analysis</h3>
							<p><strong>Expected Output:</strong></p>
							<pre class="code-block">
		Accuracy: 97.5%
		Classification Report:
					  precision    recall  f1-score   support
			   0       0.98      0.99      0.98       882
			   1       0.96      0.94      0.95       118
							</pre>

							<p><strong>🔹 Key Findings:</strong></p>
							<ul>
								<li>✅ <strong>High accuracy (~97%)</strong> shows effective spam filtering.</li>
								<li>✅ <strong>Precision (0.96 for spam)</strong> means few false positives.</li>
								<li>✅ <strong>Recall (0.94 for spam)</strong> means most actual spam is detected.</li>
							</ul>
						</div>

						<!-- Discussion Section -->
						<div class="ml-box">
							<h3> Spam Detection - Current Trends</h3>
							<p><strong>Modern email systems use hybrid AI techniques to improve spam detection.</strong></p>

							<p><strong>🔹 Trends in Spam Detection:</strong></p>
							<ul>
								<li>✅ <strong>Deep Learning (LSTMs, Transformers)</strong> → Helps detect <strong>phishing emails</strong>.</li>
								<li>✅ <strong>AI-based Personalized Spam Filters</strong> → Tailors spam detection for each user.</li>
								<li>✅ <strong>Real-time Spam Filtering</strong> → Uses <strong>cloud AI services</strong>.</li>
								<li>✅ <strong>Adversarial AI Attacks</strong> → Spammers create emails that "trick" spam classifiers.</li>
							</ul>

							<p><strong>🔹 Challenges:</strong></p>
							<ul>
								<li>🧐 <strong>Evolving Spam Techniques</strong> → Spammers use <strong>mimicked human writing</strong>.</li>
								<li>🧐 <strong>False Positives</strong> → Important emails sometimes land in <strong>Spam folders</strong>.</li>
								<li>🧐 <strong>Privacy Issues</strong> → ML models must scan email <strong>content</strong>, raising ethical concerns.</li>
							</ul>
						</div>

						<!-- Research & Future Trends -->
						<div class="ml-box">
							<h3> Research Potential & Future Improvements</h3>
							<p><strong>Future research in spam detection focuses on improving accuracy, security, and adaptability.</strong></p>

							<p><strong>🔹 Research Areas:</strong></p>
							<ul>
								<li> <strong>Deep Learning + NLP</strong>: Using <strong>BERT, GPT-based AI models</strong> to understand <strong>email context</strong>.</li>
								<li> <strong>Blockchain for Email Security</strong>: Prevents email <strong>spoofing & phishing attacks</strong>.</li>
								<li> <strong>Federated Learning</strong>: Trains spam models <strong>without sharing user emails</strong> (preserving privacy).</li>
								<li> <strong>Multi-layer AI Defense</strong>: Combining <strong>AI + Heuristics + Human Feedback</strong>.</li>
							</ul>
						</div>
					</div>			
					

				<!-- 2. Customer Segmentation -->
				<h1 class='blue-bolt'>2. Customer Segmentation 🎯</h1>
				<div class="ml-box">
					<h3>Background: What is Customer Segmentation?</h3>
					<p><strong>Definition:</strong> Customer segmentation is the process of dividing a customer base into distinct groups based on shared characteristics such as behavior, demographics, or spending habits.</p>

					<p><strong>💡 How It Works:</strong></p>
					<ul>
						<li>Companies analyze <strong>purchase history, demographics, and web activity</strong>.</li>
						<li>ML algorithms group customers into segments using <strong>unsupervised learning</strong>.</li>
						<li>Segments are used to <strong>personalize marketing, offers, or service experiences</strong>.</li>
					</ul>

					<p><strong>🔹 Algorithms Used:</strong></p>
					<ul>
						<li>✅ <strong>K-Means Clustering</strong> (Most common)</li>
						<li>✅ <strong>DBSCAN</strong> (Density-based clustering)</li>
						<li>✅ <strong>Gaussian Mixture Models</strong> (Probabilistic clustering)</li>
						<li>✅ <strong>Hierarchical Clustering</strong></li>
					</ul>
				</div>

				<div class="ml-box">
					<h3>Idea Demonstration: Segment Customers with K-Means</h3>
					<pre class="code-block">
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

df = pd.read_csv("customer_data.csv")
X = df[['Annual Income (k$)', 'Spending Score (1-100)']]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

kmeans = KMeans(n_clusters=4, random_state=42)
df['Segment'] = kmeans.fit_predict(X_scaled)

plt.scatter(X['Annual Income (k$)'], X['Spending Score (1-100)'], c=df['Segment'], cmap='rainbow')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.title('Customer Segmentation with K-Means')
plt.show()
					</pre>
				</div>

				<div class="ml-box">
					<h3>Results Analysis</h3>
					<ul>
						<li>✅ 4 clusters show separation between low/high income and spending patterns.</li>
						<li>✅ Enables personalized recommendations and targeted campaigns.</li>
					</ul>
				</div>

				<div class="ml-box">
					<h3>Trends & Challenges</h3>
					<p><strong>🔹 Trends:</strong></p>
					<ul>
						<li>✅ <strong>Real-time AI-driven segmentation</strong></li>
						<li>✅ <strong>Customer Lifetime Value (CLV) prediction</strong></li>
						<li>✅ <strong>Text embedding-based segmentation</strong> using NLP on customer reviews</li>
						<li>✅ <strong>Data fusion</strong> from online + in-store behavior</li>
					</ul>

					<p><strong>🔹 Challenges:</strong></p>
					<ul>
						<li>🧐 <strong>No ground truth</strong> — validating segment quality is difficult</li>
						<li>🧐 <strong>Dynamic behavior</strong> — customer preferences change over time</li>
						<li>🧐 <strong>Privacy regulations</strong> may restrict available data</li>
					</ul>
				</div>

				<div class="ml-box">
					<h3>Research Potential & Future Improvements</h3>
					<ul>
						<li>✅ Graph-based customer clustering (e.g., transaction networks)</li>
						<li>✅ AutoML for optimal clustering models</li>
						<li>✅ Online clustering on streaming customer data</li>
						<li>✅ Combining supervised + unsupervised for smart segmentation</li>
					</ul>
				</div>

				<!-- 3. Self-Driving Cars -->
				<h1 class='blue-bolt'>3. Self-Driving Cars 🚗 (Reinforcement Learning)</h1>

				<div class="ml-box">
					<h3>Background: What Makes a Car "Self-Driving"?</h3>
					<p><strong>Definition:</strong> Self-driving cars use AI to perceive their environment and make autonomous driving decisions. Reinforcement Learning (RL) teaches an AI agent to take actions based on rewards and penalties.</p>

					<p><strong>💡 How It Works:</strong></p>
					<ul>
						<li>The car receives <strong>sensors input</strong> (camera, LiDAR, radar).</li>
						<li>It takes <strong>actions</strong> (steer, accelerate, brake).</li>
						<li>It receives <strong>rewards</strong> (for lane-following, avoiding obstacles).</li>
						<li>Through many episodes, it <strong>learns optimal driving strategies</strong>.</li>
					</ul>

					<p><strong>🔹 Algorithms Used:</strong></p>
					<ul>
						<li>✅ <strong>Deep Q-Networks (DQN)</strong></li>
						<li>✅ <strong>Proximal Policy Optimization (PPO)</strong></li>
						<li>✅ <strong>Advantage Actor-Critic (A2C)</strong></li>
						<li>✅ <strong>Imitation Learning</strong> combined with RL</li>
					</ul>
				</div>

				<div class="ml-box">
					<h3>Idea Demonstration: Use OpenAI Gym Car Racing</h3>
					<pre class="code-block">
import gym

env = gym.make('CarRacing-v2', render_mode="human")
episodes = 5

for ep in range(episodes):
	state = env.reset()
	done = False
	total_reward = 0

	while not done:
		action = env.action_space.sample()
		next_state, reward, done, _, _ = env.step(action)
		total_reward += reward

	print(f"Episode {ep + 1} — Total Reward: {total_reward}")

env.close()
					</pre>
				</div>

			<div class="ml-box">
				<h3>Results Analysis</h3>
				<ul>
					<li>✅ <strong>Random agent performs poorly</strong> → Since it takes actions without learning, the car often crashes, veers off-road, or spins uncontrollably. This confirms that untrained models cannot drive effectively.</li>
					<li>✅ <strong>Reinforcement Learning agent improves with training</strong> → Over time, the RL agent starts to stay on the track longer, take smoother turns, and maximize rewards. This demonstrates the power of trial-and-error learning through environmental feedback.</li>
					<li>✅ <strong>Learning is visible through cumulative reward</strong> → You can observe total reward increasing across episodes as the policy refines. This acts as a measure of the agent's driving skill.</li>
					<li>✅ <strong>Action patterns become strategic</strong> → The trained agent learns to accelerate on straight paths, brake before sharp turns, and recover when drifting — all without being explicitly programmed.</li>
				</ul>
				<p><strong>🧠 Discussion:</strong></p>
				<ul>
					<li>What happens if we change the reward function? Could we train the agent to prefer safety over speed?</li>
					<li>What are the risks of training directly in the real world instead of simulation?</li>
					<li>How could sensor noise or adversarial environments affect the learned driving policy?</li>
				</ul>
			</div>


				<div class="ml-box">
					<h3>Trends & Challenges</h3>
					<p><strong>🔹 Trends:</strong></p>
					<ul>
						<li>✅ <strong>Sim2Real transfer</strong> — training in simulators and applying in real cars</li>
						<li>✅ <strong>Curriculum learning</strong> — learning basic tasks first</li>
						<li>✅ <strong>Multi-agent RL</strong> — cars learning together on the road</li>
						<li>✅ <strong>Sensor fusion</strong> — combining radar, camera, GPS</li>
					</ul>

					<p><strong>🔹 Challenges:</strong></p>
					<ul>
						<li>🧐 <strong>High sample complexity</strong> — needs millions of training steps</li>
						<li>🧐 <strong>Safety & ethics</strong> — must avoid learning harmful behaviors</li>
						<li>🧐 <strong>Expensive compute</strong> — real-time learning needs GPUs/TPUs</li>
						<li>🧐 <strong>Complex environments</strong> — require robust simulations like CARLA</li>
					</ul>
				</div>

				<div class="ml-box">
					<h3>Research Potential & Future Improvements</h3>
					<ul>
						<li>✅ Combine <strong>Imitation Learning + RL</strong> for safety and efficiency</li>
						<li>✅ RL with <strong>uncertainty modeling</strong> for safer decision-making</li>
						<li>✅ Use <strong>neuro-symbolic reasoning</strong> for logical driving decisions</li>
						<li>✅ Enable <strong>vehicle-to-vehicle (V2V) communication</strong> for collaborative driving</li>
					</ul>
				</div>
			</div>	
			`,	
			
			
			
			'ml_pipeline': `
				<h1 class="blue-bolt">Machine Learning Workflow</h1>
				<div class="workflow-box">
					<h3>&#x31;&#xFE0F;&#x20E3 ML Pipeline</h3>
					<p><strong>Definition:</strong> A structured and systematic process for developing, training, evaluating, and deploying machine learning models efficiently.</p>
					<p><strong>The ML pipeline typically involves the following key steps:</strong></p>
					<ul>
						<li>✅ Data Collection – Gathering relevant data from various sources.</li>
						<li>✅ Data Preprocessing – Cleaning, normalizing, and handling missing values in the dataset.</li>
						<li>✅ Feature Engineering – Extracting and selecting meaningful features to improve model performance.</li>
						<li>✅ Model Selection & Training – Choosing the appropriate algorithm and training the model on the dataset.</li>
						<li>✅ Model Evaluation – Assessing model accuracy, precision, recall, and overall performance using validation techniques.</li>
						<li>✅ Model Deployment – Integrating the trained model into a production environment for real-world usage</li>
					</ul>
				</div>			
				<div class="workflow-box">
					<h3>&#x32;&#xFE0F;&#x20E3 Data Preprocessing</h3>
					<p><strong>Definition:</strong> Data preprocessing is the crucial step of transforming raw data into a clean, structured, and usable format by addressing inconsistencies, handling missing values, and preparing it for model training. Proper preprocessing ensures data quality, improves model accuracy, and enhances performance.</p>
					<p><strong>Key Techniques:</strong></p>
					<ul>
						<li>✅ Handling Missing Values – Strategies like mean/median imputation, mode substitution, or removing incomplete records.</li>
						<li>✅ Removing Duplicates & Outliers – Identifying and eliminating redundant or extreme values to prevent model bias.</li>
						<li>✅ Normalization & Standardization – Scaling numerical features using Min-Max Scaling (0-1 range) or Z-score Standardization (mean=0, std=1).</li>
						<li>✅ Categorical Encoding – Converting categorical variables into numerical representations using One-Hot Encoding or Label Encoding.</li>
						<li>✅ Feature Extraction & Transformation – Applying log transformation, binning, or polynomial features to enhance data representation.</li>
					</ul>
					<p>By applying these preprocessing techniques, we ensure the dataset is well-structured and optimized for machine learning models.</p>
				</div>

				<div class="workflow-box">
					<h3>&#x33;&#xFE0F;&#x20E3 Feature Engineering</h3>
					<p><strong>Definition:</strong> Feature Engineering is the process of creating, modifying, and selecting the most relevant features to enhance a machine learning model’s predictive performance. It helps improve model accuracy, reduces noise, and ensures meaningful input representations for learning algorithms.</p>
					<p><strong>Common Techniques:</strong></p>
					<ul>
						<li>✅ Feature Selection – Identifying and retaining the most important features to avoid redundancy and improve efficiency.</li>
							<ul class = "no-bullets">
								<li><span class="blue-arrow">&#9654;</span>Filter Methods (e.g., correlation analysis, chi-square test)</li>
								<li><span class="blue-arrow">&#9654;</span> Wrapper Methods (e.g., recursive feature elimination)</li>
								<li><span class="blue-arrow">&#9654;</span> Embedded Methods (e.g., LASSO, decision tree feature importance)</li>
							</ul>
						<li>✅ Feature Transformation – Modifying existing features to improve model interpretability and performance.</li>
							<ul class = "no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Principal Component Analysis (PCA) – Reducing dimensionality while preserving essential information.</li>
								<li><span class="blue-arrow">&#9654;</span> Polynomial Features – Creating higher-degree features to capture complex relationships.</li>
								<li><span class="blue-arrow">&#9654;</span> Log/Square Root Transformation – Handling skewed distributions and improving model behavior.</li>
							</ul>
						<li>✅ Text & Image Feature Extraction – Converting unstructured text and image data into structured numerical features.</li>
							<ul class = "no-bullets">
								<li><span class="blue-arrow">&#9654;</span> TF-IDF (Term Frequency-Inverse Document Frequency) – Assigning importance to words in a corpus.</li>
								<li><span class="blue-arrow">&#9654;</span> Word Embeddings (Word2Vec, GloVe, BERT) – Representing words in high-dimensional space based on context.</li>
								<li><span class="blue-arrow">&#9654;</span> Image Feature Extraction (CNN, Edge Detection) – Identifying patterns and key features from image data.</li>
							</ul>
					</ul>
					<p>Why Feature Engineering Matters?</p>
					<ul>
						<li> Enhances model accuracy by providing more relevant input data.</li>
						<li> Reduces overfitting by eliminating unnecessary or noisy features.</li>
						<li> Improves interpretability by ensuring meaningful representations.</li>
						<li> Optimizes computational efficiency by reducing data complexity.</li>
					</ul>
					<p>By applying effective feature engineering techniques, models can generalize better to unseen data, leading to more robust and accurate predictions</p>
				</div>


				<div class="workflow-box">
					<h3>&#x34;&#xFE0F;&#x20E3 Model Selection & Training</h3>
					<p><strong>Definition:</strong> Model selection and training involve identifying the most suitable machine learning algorithm for a given task and optimizing it using labeled or structured data. The choice of model significantly impacts predictive accuracy, generalization, and computational efficiency.</p>
					<p><strong>Common ML Models:</strong></p>
					<ul>
						<li>✅ Supervised Learning – Models trained on labeled data to map inputs to outputs.</li>
							<ul class = "no-bullets">
								<li> <span class="blue-arrow">&#9654;</span>Decision Trees – Hierarchical models for classification and regression tasks.</li>
								<li> <span class="blue-arrow">&#9654;</span>Support Vector Machines (SVM) – Effective for high-dimensional spaces and boundary separation.</li>
								<li> <span class="blue-arrow">&#9654;</span>Neural Networks – Deep learning architectures capable of capturing complex patterns.</li>
							</ul>						
						<li>✅ Unsupervised Learning – Models that find patterns in unlabeled data.</li>
							<ul class = "no-bullets">
								<li> <span class="blue-arrow">&#9654;</span>k-Means Clustering – Partitions data into k clusters based on similarity.</li>
								<li> <span class="blue-arrow">&#9654;</span>DBSCAN (Density-Based Spatial Clustering) – Identifies clusters of varying density without predefining the number of clusters.</li>
								<li> <span class="blue-arrow">&#9654;</span>Autoencoders – Neural networks that learn compressed representations of data.</li>
							</ul>						
						
						<li>✅ Reinforcement Learning – Learning through interaction with an environment using rewards.</li>
							<ul class = "no-bullets">
								<li> <span class="blue-arrow">&#9654;</span>Q-Learning – A value-based approach that learns optimal policies.</li>
								<li> <span class="blue-arrow">&#9654;</span>Deep Q Networks (DQN) – Uses deep learning to approximate Q-values for decision-making.</li>
							</ul>						
					</ul>
					<p>Why Model Selection & Training Matter?</p>
					<ul>
						<li> Ensures the chosen model aligns with the problem type and dataset characteristics.</li>
						<li> Helps balance bias-variance tradeoff to improve generalization on unseen data.</li>
						<li> Optimizes computational efficiency, ensuring scalability and real-world deployment feasibility.</li>
					</ul>
					<p>By selecting the right model and applying proper training techniques, machine learning systems can achieve higher accuracy, stability, and interpretability in real-world applications.</p>
				</div>


				<div class="workflow-box">
					<h3>&#x35;&#xFE0F;&#x20E3 Model Evaluation</h3>
					<p><strong>Definition:</strong> Model evaluation is the process of assessing a machine learning model’s performance on test data to ensure its reliability, accuracy, and generalization to unseen data. It helps determine whether the model is well-optimized or suffers from issues like overfitting or underfitting..</p>
					<p><strong>Key Metrics:</strong></p>
					<ul>
						<li>✅ Classification Metrics – Used for evaluating classification models that predict categorical outputs:</li>
							<ul class = "no-bullets">
								<li><span class="blue-arrow">&#9654;</span>Accuracy – Measures the overall correctness of predictions.</li>
								<li><span class="blue-arrow">&#9654;</span>Precision – Indicates how many of the predicted positive cases were actually positive.</li>
								<li><span class="blue-arrow">&#9654;</span>Recall (Sensitivity) – Measures how well the model captures actual positive cases.</li>
								<li><span class="blue-arrow">&#9654;</span>F1-Score – A balanced measure that combines precision and recall for imbalanced datasets.</li>
							</ul>						
						<li>✅ Regression Metrics – Used for evaluating models that predict continuous values:</li>
							<ul class = "no-bullets">
								<li><span class="blue-arrow">&#9654;</span>Root Mean Squared Error (RMSE) – Measures the average squared differences between actual and predicted values, giving more weight to larger errors.</li>
								<li><span class="blue-arrow">&#9654;</span>Mean Absolute Error (MAE) – Measures the absolute differences between actual and predicted values, making it more interpretable.</li>
								<li><span class="blue-arrow">&#9654;</span>R-squared (R²) – Indicates how well the model explains the variance in the data (higher is better).</li>
							</ul>						
						<li>✅ Overfitting Prevention Strategies – Ensuring models generalize well to new data:</li>
							<ul class = "no-bullets">
								<li><span class="blue-arrow">&#9654;</span>Cross-Validation – Splitting the dataset into multiple subsets to evaluate model stability.</li>
								<li><span class="blue-arrow">&#9654;</span>Regularization (L1/L2) – Penalizing complex models to reduce overfitting.</li>
								<li><span class="blue-arrow">&#9654;</span>Early Stopping – Stopping training when performance on validation data starts degrading.)</li>
							</ul>						
					</ul>
					<p>Why Model Evaluation Matters?</p>
					<ul>
						<li> Ensures the model is accurate, reliable, and unbiased for real-world deployment.</li>
						<li> Helps compare multiple models and select the best-performing one.</li>
						<li>  Identifies potential bias, overfitting, and underfitting issues before production use.</li>
					</ul>
					<p>By using the right evaluation metrics and techniques, machine learning models can achieve better generalization, stability, and interpretability in practical applications.</p>
				</div>					
				</div>


				<div class="workflow-box">
					<h3>&#x36;&#xFE0F;&#x20E3 Model Deployment</h3>
					<p><strong>Definition:</strong>Model deployment is the process of integrating a trained machine learning model into a production environment, making it accessible for real-world applications. This step ensures that predictions can be made on new, unseen data efficiently, reliably, and at scale.</p>
					<p><strong>Common Deployment Methods:</strong></p>
					<ul>
						<li>✅ API Deployment – Exposing the model via web services for easy integration with applications:</li>
							<ul class = "no-bullets">
								<li><span class="blue-arrow">&#9654;</span>Flask & FastAPI – Lightweight Python frameworks for building RESTful APIs.</li>
								<li><span class="blue-arrow">&#9654;</span>Django – A full-stack framework with built-in API support for scalable applications.</li>
								<li><span class="blue-arrow">&#9654;</span>GraphQL – An alternative approach for flexible, efficient API queries.</li>
							</ul>							
						<li>✅ Cloud Deployment – Hosting models on cloud platforms for scalability and remote access</li>
							<ul class = "no-bullets">
								<li><span class="blue-arrow">&#9654;</span>AWS SageMaker – Fully managed service for model training and deployment.</li>
								<li><span class="blue-arrow">&#9654;</span>Google AI Platform – Cloud-based ML model serving with integration into Google Cloud.</li>
								<li><span class="blue-arrow">&#9654;</span>Azure ML – Microsoft's cloud-based ML workflow for enterprise AI solutions.</li>
								<li><span class="blue-arrow">&#9654;</span>Docker & Kubernetes – Containerization and orchestration for scalable deployments.</li>
							</ul>							
						<li>✅ Edge Deployment – Running models on decentralized devices for low-latency applications</li>
							<ul class = "no-bullets">
								<li><span class="blue-arrow">&#9654;</span>Embedded AI – Deploying models on mobile devices, wearables, and IoT devices.</li>
								<li><span class="blue-arrow">&#9654;</span>ONNX & TensorFlow Lite – Optimized formats for efficient on-device inference.</li>
								<li><span class="blue-arrow">&#9654;</span>Raspberry Pi & NVIDIA Jetson – Hardware platforms supporting AI-powered edge computing.</li>
							</ul>							
					</ul>
					<p>Why Model Deployment Matters?</p>
					<ul>
						<li> Enables real-time inference and decision-making in production environments.</li>
						<li> Ensures scalability, allowing models to handle high traffic efficiently.</li>
						<li> Reduces latency by leveraging cloud and edge-based AI solutions.</li>
						<li> Supports continuous monitoring & updates, ensuring model performance stays optimal over time.</li>
					</ul>
					<p>By selecting the right deployment method, machine learning models can seamlessly transition from development to real-world impact.</p>
					
				</div>				
			`,	
			
			'data_preprocessing': `
					<h1 class="blue-bolt"> Data Preprocessing</h1>
					
					<div class="content-box">
						<h3>&#x31;&#xFE0F;&#x20E3  Handling Missing Values</h3>
						<p><strong>Definition:</strong> Missing values can introduce bias, distort data distributions, and reduce model accuracy. Proper handling of missing data is essential to maintain data integrity, prevent misleading insights, and ensure optimal model performance</p>
					<ul>
						<li>✅ Statistical Imputation – Replacing missing values with representative statistics to preserve dataset consistency:</li>
							<ul class = "no-bullets">
								<li><span class="blue-arrow">&#9654;</span>Mean Imputation – Filling gaps with the average value (suitable for normally distributed data).</li>
								<li><span class="blue-arrow">&#9654;</span>Median Imputation – Using the middle value to handle skewed distributions.</li>
								<li><span class="blue-arrow">&#9654;</span>Mode Imputation – Replacing missing categorical values with the most frequent occurrence.</li>
							</ul>							
						<li>✅ Data Reduction Techniques – Removing instances where missing values are excessive:</li>
							<ul class = "no-bullets">
								<li><span class="blue-arrow">&#9654;</span>Dropping Rows – Eliminating records with too many missing values to reduce noise.</li>
								<li><span class="blue-arrow">&#9654;</span>Dropping Columns – Removing features with a high percentage of missing values to prevent distortion.</li>
							</ul>							
						<li>✅ Advanced Estimation Methods – Inferring missing values using intelligent techniques:</li>
							<ul class = "no-bullets">
								<li><span class="blue-arrow">&#9654;</span>Linear & Polynomial Interpolation – Estimating missing values based on patterns in existing data (effective for time series).</li>
								<li><span class="blue-arrow">&#9654;</span>Predictive Imputation – Using machine learning models (e.g., KNN, Regression, Random Forest) to predict and fill missing values.</li>
								<li><span class="blue-arrow">&#9654;</span>Multiple Imputation – Generating multiple plausible values and averaging predictions to reduce uncertainty.</li>
							</ul>							
					</ul>
					<p>Why Handling Missing Values Matters?</p>
					<ul>
						<li> Prevents biased predictions and loss of valuable information.</li>
						<li> Improves model stability and ensures better generalization.</li>
						<li> Enhances data reliability, leading to more accurate and meaningful insights.</li>
					</ul>
					<p>By selecting the appropriate technique based on data characteristics, missing values can be effectively managed, ensuring a more robust and high-performing machine learning model.</p>
					</div>

					<div class="content-box">
						<h3>&#x32;&#xFE0F;&#x20E3  Normalization & Standardization</h3>
						<p><strong>Definition:</strong> Scaling numerical data is essential for ensuring model stability, improving convergence speed, and enhancing performance in machine learning models. Proper scaling prevents features with large magnitudes from dominating the learning process and ensures fair comparisons across different variables.</p>
						<p><strong>Key Scaling Techniques:</strong></p>
					<ul>
						<li>✅ Min-Max Scaling – Rescales values to a fixed range, typically [0,1] or [-1,1], ensuring all features contribute equally to the model.</li>
							<ul class = "no-bullets">
								<li><span class="blue-arrow">&#9654;</span> formula : 
									<math>
									  <mrow>
										<msup><mi>X</mi><mo>'</mo></msup>
										<mo>=</mo>
										<mfrac>
										  <mrow>
											<mi>X</mi>
											<mo>-</mo>
											<msub><mi>X</mi><mi>min</mi></msub>
										  </mrow>
										  <mrow>
											<msub><mi>X</mi><mi>max</mi></msub>
											<mo>-</mo>
											<msub><mi>X</mi><mi>min</mi></msub>
										  </mrow>
										</mfrac>
									  </mrow>
									</math>
								</li>
								<li><span class="blue-arrow">&#9654;</span>Best for: Neural networks, gradient-based models where bounded values improve learning efficiency.</li>
							</ul>							
						<li>✅ Z-Score Standardization (Standard Scaling) – Transforms data to have zero mean and unit variance, making it ideal for normally distributed data.</li>
							<ul class = "no-bullets">
								<li><span class="blue-arrow">&#9654;</span> formula : 
									<math>
									  <mrow>
										<msup><mi>X</mi><mo>'</mo></msup>
										<mo>=</mo>
										<mfrac>
										  <mrow>
											<mi>X</mi>
											<mo>-</mo>
											<mi>&mu;</mi>
										  </mrow>
										  <mi>&sigma;</mi>
										</mfrac>
									  </mrow>
									</math>
								</li>
								<li><span class="blue-arrow">&#9654;</span>Best for: Linear regression, PCA, SVM, k-Means clustering, where standardization enhances performance.</li>
							</ul>							
						<li>✅ Robust Scaling – Uses median and interquartile range (IQR) instead of mean and variance to handle skewed distributions and outliers effectively</li>
							<ul class = "no-bullets">
								<li><span class="blue-arrow">&#9654;</span> formula : 
									<math>
									  <mrow>
										<msup><mi>X</mi><mo>'</mo></msup>
										<mo>=</mo>
										<mfrac>
										  <mrow>
											<mi>X</mi>
											<mo>-</mo>
											<mi>median</mi>
										  </mrow>
										  <mi>IQR</mi>
										</mfrac>
									  </mrow>
									</math>
								</li>
								<li><span class="blue-arrow">&#9654;</span>Best for: Datasets with extreme values or heavy-tailed distributions (e.g., financial data, sensor readings).</li>
							</ul>							
					</ul>
					<p>Why Normalization & Standardization Matter?</p>
					<ul>
						<li> Ensures fair feature comparisons, preventing high-magnitude variables from dominating the model.</li>
						<li> Ensures scalability, allowing models to handle high traffic efficiently.</li>
						<li> Enhances clustering and distance-based models (e.g., k-Means, KNN) by preventing biased distance measurements.</li>
						<li> Improves model interpretability and numerical stability, reducing computational issues.</li>
					</ul>
					<p>By selecting the right scaling technique based on the dataset characteristics, machine learning models can achieve higher accuracy, faster training, and better generalization.</p>
					</div>

					<div class="content-box">
						<h3>&#x33;&#xFE0F;&#x20E3 Categorical Data Encoding</h3>
						<p><strong>Definition:</strong> Machine learning models require numerical input, so categorical features must be converted into numerical representations to enable proper learning.</p>
						
						<p><strong>Encoding Methods:</strong></p>
						<ul>
							<li>✅ One-Hot Encoding:</li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Creates binary columns for each category.</li>
								<li><span class="blue-arrow">&#9654;</span> Suitable for nominal categorical variables.</li>
								<li><span class="blue-arrow">&#9654;</span> Can lead to high-dimensional data for many unique categories.</li>
							</ul>
							
							<li>✅ Label Encoding:</li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Assigns unique numeric values to each category (e.g., Red → 1, Blue → 2).</li>
								<li><span class="blue-arrow">&#9654;</span> Efficient for ordinal categorical data where order matters.</li>
								<li><span class="blue-arrow">&#9654;</span> May introduce unintended ordinal relationships in nominal data.</li>
							</ul>
							
							<li>✅ Target Encoding:</li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Replaces categories with the mean target value of that category.</li>
								<li><span class="blue-arrow">&#9654;</span> Useful in high-cardinality categorical data.</li>
								<li><span class="blue-arrow">&#9654;</span> Can introduce target leakage if not applied correctly.</li>
							</ul>
						</ul>
						
						<p><strong>Why Categorical Encoding Matters?</strong></p>
						<ul>
							<li>Ensures that machine learning models can interpret categorical features effectively.</li>
							<li>Reduces dimensionality while preserving important categorical relationships.</li>
							<li>Prevents models from misinterpreting nominal categories as ordinal values.</li>
							<li>Improves model generalization by representing categorical features in a meaningful way.</li>
						</ul>
						
						<p>By selecting the appropriate encoding method, categorical variables can be transformed into a form that maximizes model accuracy and efficiency.</p>
					</div>

					<div class="content-box">
						<h3>&#x34;&#xFE0F;&#x20E3 Handling Outliers</h3>
						<p><strong>Definition:</strong> Outliers are extreme values that significantly deviate from the overall pattern of data. They can distort statistical analyses, impact model accuracy, and lead to unreliable predictions. Detecting and addressing outliers ensures more robust and stable machine learning models.</p>
						
						<p><strong>Common Detection & Handling Techniques:</strong></p>
						<ul>
							<li>✅ Z-Score Method:</li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Identifies outliers by measuring how far a value deviates from the mean in terms of standard deviations.</li>
								<li><span class="blue-arrow">&#9654;</span> Any value beyond ±3 standard deviations is typically considered an outlier.</li>
								<li><span class="blue-arrow">&#9654;</span> Best suited for normally distributed data.</li>
							</ul>
							
							<li>✅ IQR (Interquartile Range) Method:</li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Uses the 25th percentile (Q1) and 75th percentile (Q3) to define the interquartile range.</li>
								<li><span class="blue-arrow">&#9654;</span> Values below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR are considered outliers.</li>
								<li><span class="blue-arrow">&#9654;</span> Works well with skewed distributions and non-normal data.</li>
							</ul>
							
							<li>✅ Winsorization & Data Transformation:</li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Winsorization: Replaces extreme values with a defined percentile (e.g., replacing top 5% values with the 95th percentile).</li>
								<li><span class="blue-arrow">&#9654;</span> Log Transformation: Reduces the effect of large values by converting them into logarithmic scales.</li>
								<li><span class="blue-arrow">&#9654;</span> Box-Cox Transformation: Normalizes skewed data to stabilize variance and reduce the impact of outliers.</li>
							</ul>
						</ul>
						
						<p><strong>Why Handling Outliers Matters?</strong></p>
						<ul>
							<li>Prevents skewed model predictions caused by extreme values.</li>
							<li>Improves model accuracy and robustness by ensuring a more representative dataset.</li>
							<li>Enhances generalization, making the model perform better on unseen data.</li>
							<li>Reduces sensitivity to noise, ensuring more stable and reliable results.</li>
						</ul>
						
						<p>By carefully identifying and managing outliers, machine learning models can achieve better performance, avoid biases, and ensure more reliable predictions in real-world applications.</p>
					</div>

					<div class="content-box">
						<h3>&#x35;&#xFE0F;&#x20E3 Splitting Data for Training & Testing</h3>
						<p><strong>Definition:</strong> Properly splitting data into training, validation, and testing sets is crucial for evaluating model performance, preventing overfitting, and ensuring reliable generalization to unseen data.</p>
						
						<p><strong>Data Split Components:</strong></p>
						<ul>
							<li>✅ Training Set:</li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Used for model learning by adjusting weights and parameters.</li>
								<li><span class="blue-arrow">&#9654;</span> Typically comprises 70-80% of the dataset.</li>
								<li><span class="blue-arrow">&#9654;</span> Provides the foundation for understanding patterns and relationships in data.</li>
							</ul>
							
							<li>✅ Validation Set:</li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Used for hyperparameter tuning and model selection.</li>
								<li><span class="blue-arrow">&#9654;</span> Typically comprises 10-15% of the dataset.</li>
								<li><span class="blue-arrow">&#9654;</span> Helps prevent overfitting by evaluating performance on unseen data before final testing.</li>
							</ul>
							
							<li>✅ Test Set:</li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Used for final model evaluation after training and validation.</li>
								<li><span class="blue-arrow">&#9654;</span> Typically comprises 10-20% of the dataset.</li>
								<li><span class="blue-arrow">&#9654;</span> Provides an unbiased assessment of real-world performance.</li>
							</ul>
						</ul>
						
						<p><strong>Why Splitting Data Matters?</strong></p>
						<ul>
							<li>Ensures models generalize well to unseen data and are not overfitted to training data.</li>
							<li>Improves model selection by enabling performance comparison across different configurations.</li>
							<li>Prevents misleading results by ensuring models are tested on data they have never seen before.</li>
							<li>Facilitates better decision-making when deploying models in real-world scenarios.</li>
						</ul>
						
						<p>By properly partitioning the dataset, machine learning models can achieve better performance, robustness, and reliability when deployed in production environments.</p>
					</div>
			`,	
			
			
			

			'feature_engineering': `
						<h1 class="blue-bolt">Feature Engineering</h1>
						<div class="content-box">
							<h3>Feature Extraction</h3>
							<p><strong>Definition:</strong> Feature extraction is the process of transforming raw data into a more meaningful and structured representation, allowing machine learning models to learn more efficiently. It reduces data complexity while retaining the most relevant information, enabling better pattern recognition and improving model performance. Feature extraction is especially useful when dealing with high-dimensional data, such as text, images, audio, and time-series signals.</p>
							<p><strong>Common Techniques:</strong></p>
							<ul>
								<li>✅  TF-IDF (Term Frequency-Inverse Document Frequency)</strong></li>
								<ul class="no-bullets">
									<li><span class="blue-arrow">&#9654;</span> Assigns weights to words based on their frequency within a document and their rarity across a corpus.</li>
									<li><span class="blue-arrow">&#9654;</span> Helps highlight important words that contribute the most to understanding document content.</li>
									<li><span class="blue-arrow">&#9654;</span> Commonly used in text classification, information retrieval, and search engines to improve keyword relevance.</li>
									<li><span class="blue-arrow">&#9654;</span> Works well in sentiment analysis, spam detection, and topic modeling where word importance matters.</li>
								</ul>

								<li>✅ <strong>Edge Detection in Images</strong></li>
								<ul class="no-bullets">
									<li><span class="blue-arrow">&#9654;</span> Identifies significant object boundaries by detecting sudden intensity changes in images.</li>
									<li><span class="blue-arrow">&#9654;</span> Enhances key features like shapes and contours, making it easier for models to recognize objects.</li>
									<li><span class="blue-arrow">&#9654;</span> Essential in facial recognition, medical imaging (MRI/CT scans), self-driving cars, and industrial defect detection..</li>
									<li><span class="blue-arrow">&#9654;</span>  Common edge detection algorithms include Sobel, Canny, and Laplacian filters.</li>
								</ul>

								<li>✅ <strong>Fourier Transform</strong></li>
								<ul class="no-bullets">
									<li><span class="blue-arrow">&#9654;</span> Converts signals from the time domain to the frequency domain, allowing analysis of periodic components.</li>
									<li><span class="blue-arrow">&#9654;</span> Helps in identifying patterns, filtering noise, and compressing signals.</li>
									<li><span class="blue-arrow">&#9654;</span> Widely used in audio processing (speech recognition, music analysis), signal compression, and finance (stock trend analysis).</li>
									<li><span class="blue-arrow">&#9654;</span> The Fast Fourier Transform (FFT) is an optimized algorithm for efficiently computing Fourier transforms on large datasets.</li>									
								</ul>
							</ul>
							<p><strong>Why Feature Extraction Matters?</strong></p>
							<ul>
								<li>Reduces data dimensionality while preserving crucial information, improving computational efficiency.</li>
								<li>Enhances feature quality, allowing models to focus on the most informative attributes.</li>
								<li>Boosts model accuracy and interpretability by eliminating irrelevant or redundant data.</li>
								<li>Essential in high-dimensional datasets (e.g., text, image, and audio) where direct input processing is impractical.</li>
							</ul>
							
							<p>By applying effective feature extraction techniques, machine learning models can perform better, learn faster, and generalize well to unseen data.</p>							
						</div>





						<div class="content-box">
							<h3>Feature Selection</h3>
							<p><strong>Definition:</strong> Feature selection is the process of identifying and retaining the most relevant features in a dataset while removing redundant, noisy, or irrelevant variables. This step enhances model efficiency, prevents overfitting, reduces computational costs, and improves generalization to unseen data. By eliminating unnecessary features, models can focus on the most meaningful information, leading to faster training times and better interpretability.</p>
							<p><strong>Techniques:</strong></p>
							<ul>
								<li>✅ <strong>Filter Methods</strong></li>
								<ul class="no-bullets">
									<li><span class="blue-arrow">&#9654;</span> Uses statistical tests and correlation measures to rank features based on their relevance to the target variable.</li>
									<li><span class="blue-arrow">&#9654;</span> Works independently of any specific machine learning algorithm, making it fast and scalable for large datasets.</li>
									<li><span class="blue-arrow">&#9654;</span> Common techniques include:</li>
										<ul>
											<li>Chi-square Test – Measures the independence between categorical features and the target variable.</li>
											<li>ANOVA (Analysis of Variance) – Evaluates the relationship between numerical features and categorical target variables.</li>
											<li>Mutual Information – Assesses how much information one feature contributes to predicting the target variable.</li>
												<ul>
													<li>Well-suited for high-dimensional datasets where computational efficiency is a priority</li>
												</ul>
										</ul>
								</ul>

								<li>✅ <strong>Wrapper Methods</strong></li>
								<ul class="no-bullets">
									<li><span class="blue-arrow">&#9654;</span> Uses an iterative approach to evaluate different feature subsets based on model performance.</li>
									<li><span class="blue-arrow">&#9654;</span> More computationally expensive than filter methods but provides better feature interactions and selection accuracy.</li>
									<li><span class="blue-arrow">&#9654;</span> Common techniques include:</li>
										<ul>
											<li>Recursive Feature Elimination (RFE) – Systematically removes the least important features and retrains the model to identify the optimal feature set.</li>
											<li>Forward Selection – Starts with no features and gradually adds the most impactful ones based on performance.</li>
											<li>Backward Elimination – Begins with all features and removes the least relevant one at a time.</li>
												<ul>
													<li> Effective when the dataset has complex relationships between features that are difficult to capture with filter methods.</li>
												</ul>
										</ul>
								</ul>		
								<li>✅ Embedded Methods</strong></li>
								<ul class="no-bullets">
									<li><span class="blue-arrow">&#9654;</span> Performs feature selection during model training by incorporating selection as part of the learning process.</li>
									<li><span class="blue-arrow">&#9654;</span> Efficient because it simultaneously selects features while optimizing model parameters.</li>
									<li><span class="blue-arrow">&#9654;</span> Common techniques include:</li>
										<ul>
											<li>LASSO (L1 Regularization) – Shrinks coefficients of less important features to zero, effectively eliminating them.</li>
											<li>Tree-Based Feature Selection – Decision trees and random forests naturally rank feature importance based on their contribution to reducing impurity.</li>
											<li>Elastic Net – A combination of L1 (LASSO) and L2 (Ridge) regularization, balancing feature selection and stability.</li>
												<ul>
													<li> Ideal for scenarios where model accuracy and feature interpretability are both important.</li>
												</ul>
										</ul>
								</ul>										
								</ul>
							</ul>
							<p><strong>Why Feature Selection Matters?</strong></p>
							<ul>
								<li>Reduces computational complexity by eliminating redundant or irrelevant features.</li>
								<li>Prevents overfitting, ensuring the model generalizes well to unseen data.</li>
								<li>Boosts training speed and efficiency, especially in high-dimensional datasets./li>
								<li>Helps models focus on the most meaningful features, improving predictive accuracy.</li>
							</ul>
							
							<p>By applying the right feature selection technique, machine learning models become more efficient, accurate, and interpretable, leading to better decision-making and performance. </p>														
						</div>





						<div class="content-box">
							<h3>Feature Transformation</h3>
							<p><strong>Definition:</strong> Feature transformation involves modifying existing features to improve their compatibility with machine learning models. It helps handle skewed distributions, capture complex relationships, and improve model interpretability and performance. This technique is particularly useful when raw features do not exhibit a strong correlation with the target variable or when certain transformations can better reveal patterns in the data.</p>
							<p><strong>Methods:</strong></p>
							<ul>
								<li>✅ <strong>Logarithmic Transformation</strong></li>
								<ul class="no-bullets">
									<li><span class="blue-arrow">&#9654;</span> Reduces the impact of extreme values, making models less sensitive to outliers.</li>
									<li><span class="blue-arrow">&#9654;</span> Stabilizes variance in datasets with heavy-tailed distributions</li>
									<li><span class="blue-arrow">&#9654;</span> Improves normality, making data more symmetric and closer to a Gaussian distribution.</li>
									<li><span class="blue-arrow">&#9654;</span> Commonly applied in financial data (stock prices), scientific measurements, and count-based data to normalize skewed distributions.</li>
									<li><span class="blue-arrow">&#9654;</span> Formula :</li>
								</ul>

								<li>✅ <strong>Polynomial Features</strong></li>
								<ul class="no-bullets">
									<li><span class="blue-arrow">&#9654;</span>Enhances the ability to model nonlinear relationships between input variables and the target.</li>
									<li><span class="blue-arrow">&#9654;</span> Expands feature space by introducing interaction terms and higher-degree transformations (e.g., quadratic, cubic)..</li>
									<li><span class="blue-arrow">&#9654;</span> Commonly used in regression models where simple linear relationships are insufficient to capture patterns in the data. </li>
									<li><span class="blue-arrow">&#9654;</span> Useful in polynomial regression, kernel-based models, and deep learning feature engineering..</li>									
								</ul>
								
								<li>✅ <strong>Embedding Layers in Neural Networks</strong></li>
								<ul class="no-bullets">
									<li><span class="blue-arrow">&#9654;</span> Converts categorical data, such as words or discrete labels, into continuous vectors in a high-dimensional space.</li>
									<li><span class="blue-arrow">&#9654;</span> Enables deep learning models to capture semantic relationships and feature similarities.</li>
									<li><span class="blue-arrow">&#9654;</span> Used extensively in natural language processing (word embeddings like Word2Vec, GloVe, BERT) and recommendation systems.</li>							
								</ul>
								
							</ul>
							<p><strong>Why Feature Transformation Matters?</strong></p>
							<ul>
								<li>Enhances model compatibility by ensuring input data meets statistical assumptions.</li>
								<li>Improves predictive power, allowing models to capture nonlinear and complex relationships.</li>
								<li> Reduces the effect of outliers, preventing large values from disproportionately influencing model training.</li>
								<li>Ensures numerical stability, making training more efficient for models that assume normally distributed inputs.</li>
								<li>Expands feature representation, particularly in deep learning models where embeddings enable meaningful feature interactions.</li>
							</ul>
							
							<p>By selecting the right feature transformation techniques, machine learning models can achieve greater accuracy, stability, and interpretability, leading to better generalization and real-world applicability.</p>														
						</div>



						<div class="content-box">
							<h3>Dimensionality Reduction</h3>
							<p><strong>Definition:</strong> Dimensionality reduction is the process of reducing the number of input variables while preserving as much critical information as possible. High-dimensional datasets can lead to increased computational complexity, overfitting, and difficulty in visualizing data patterns. By applying dimensionality reduction techniques, we improve model efficiency, generalization, and interpretability.</p>
							<p><strong>Techniques:</strong></p>
							<ul>
								<li>✅ <strong>Principal Component Analysis (PCA)</strong></li>
								<ul class="no-bullets">
									<li><span class="blue-arrow">&#9654;</span> Projects high-dimensional data onto fewer dimensions while preserving maximum variance.</li>
									<li><span class="blue-arrow">&#9654;</span> Identifies the principal components (uncorrelated features) that capture the most important variations in the dataset.</li>
									<li><span class="blue-arrow">&#9654;</span> Helps reduce multicollinearity in regression models by transforming correlated features into independent ones.</li>
									<li><span class="blue-arrow">&#9654;</span> Useful for visualization, as reducing data to 2D or 3D makes it easier to interpret relationships.</li>
									<li><span class="blue-arrow">&#9654;</span> Formula: PCA finds eigenvectors of the covariance matrix to create new features:</li>
									<li>
									<div style="text-align: center;">
										<math>
										  <msup>
											<mi>X</mi>
											<mo>'</mo>
										  </msup>
										  <mo>=</mo>
										  <mi>X</mi>
										  <mo>&#x22C5;</mo> <!-- Dot operator for multiplication -->
										  <mi>W</mi>
										</math>
									</div>										
									</li>
									<p>where W is the matrix od principal components.</p>
								</ul>

								<li>✅ <strong>t-SNE (t-Distributed Stochastic Neighbor Embedding)</strong></li>
								<ul class="no-bullets">
									<li><span class="blue-arrow">&#9654;</span> A non-linear dimensionality reduction technique that is highly effective for visualizing high-dimensional data in 2D or 3D.</li>
									<li><span class="blue-arrow">&#9654;</span> Unlike PCA, which maintains global structure, t-SNE focuses on preserving local structure, making it ideal for clustering applications.</li>
									<li><span class="blue-arrow">&#9654;</span> Commonly used in image processing, genomics, and NLP applications to reveal hidden patterns in complex datasets.</li>
								</ul>
								
								<li>✅ <strong>Autoencoders</strong></li>
								<ul class="no-bullets">
									<li><span class="blue-arrow">&#9654;</span> A deep learning-based dimensionality reduction technique that learns a compressed representation of data using neural networks.</li>
									<li><span class="blue-arrow">&#9654;</span> Consists of an encoder (which reduces dimensionality) and a decoder (which reconstructs the original data).</li>
									<li><span class="blue-arrow">&#9654;</span> Learns to extract important latent features while discarding noise and redundant information.</li>
									<li><span class="blue-arrow">&#9654;</span> Commonly used in anomaly detection, image compression, and unsupervised learning tasks.</li>									
								</ul>

								<li>✅ <strong>Linear Discriminant Analysis (LDA)</strong></li>
								<ul class="no-bullets">
									<li><span class="blue-arrow">&#9654;</span> Unlike PCA, which focuses on variance, LDA aims to maximize class separability by finding the best linear combinations of features.</li>
									<li><span class="blue-arrow">&#9654;</span> Commonly used in classification tasks where dimensionality reduction is needed while preserving decision boundaries.</li>
									<li><span class="blue-arrow">&#9654;</span>  Works well in text classification, face recognition, and medical diagnosis application.</li>
								</ul>
							<p><strong>Why Dimensionality Reduction Matters?</strong></p>
							<ul>
								<li> Reduces computational cost, making models train and infer faster.</li>
								<li> Prevents overfitting by removing irrelevant and redundant features.</li>
								<li> Enhances data visualization, enabling better exploration of high-dimensional datasets.</li>
								<li> Improves model interpretability by simplifying complex data structures.</li>
								<li> Optimizes storage and memory usage, which is essential for large-scale datasets.</li>
							</ul>
							
							<p>By leveraging the right dimensionality reduction techniques, machine learning models become more efficient, scalable, and interpretable, leading to better insights and performance.</p>														
							
							</ul>
						</div>

						<div class="content-box">
							<h3>Feature Engineering in Time-Series Data</h3>
							<p><strong>Definition:</strong> Feature engineering in time-series data involves transforming raw sequential data into informative features that help models capture temporal dependencies and trends. Since time-series data exhibits patterns such as seasonality, trends, and cyclic behaviors, creating meaningful features can improve forecasting accuracy, anomaly detection, and trend analysis.</p>
							<p><strong>Common Features:</strong></p>
							<ul>
								<li>✅ <strong>Lag Features</strong></li>
								<ul class="no-bullets">
									<li><span class="blue-arrow">&#9654;</span> Incorporates past observations as input variables to predict future values, enabling models to recognize trends and patterns.</li>
									<li><span class="blue-arrow">&#9654;</span> Essential for autoregressive models (AR, ARIMA), LSTMs, and recurrent neural networks (RNNs) that rely on sequential dependencies.</li>
									<li><span class="blue-arrow">&#9654;</span> Applied in financial markets, demand forecasting, and weather prediction.</li>
									<li><span class="blue-arrow">&#9654;</span> Example: Predicting today's stock price using prices from the past 5 days:</li>
										<div style="text-align: center;">
										  <math>
											<msub>
											  <mi>X</mi>
											  <mi>t</mi>
											</msub>
											<mo>=</mo>
											<mo>[</mo>
											<msub>
											  <mi>X</mi>
											  <mrow>
												<mi>t</mi>
												<mo>-</mo>
												<mn>1</mn>
											  </mrow>
											</msub>
											<mo>,</mo>
											<msub>
											  <mi>X</mi>
											  <mrow>
												<mi>t</mi>
												<mo>-</mo>
												<mn>2</mn>
											  </mrow>
											</msub>
											<mo>, ... ,</mo>
											<msub>
											  <mi>X</mi>
											  <mrow>
												<mi>t</mi>
												<mo>-</mo>
												<mi>n</mi>
											  </mrow>
											</msub>
											<mo>]</mo>
										  </math>
										</div>
								</ul>

								<li>✅ <strong>Rolling Window Statistics</strong></li>
								<ul class="no-bullets">
									<li><span class="blue-arrow">&#9654;</span> Computes statistical summaries (mean, standard deviation, min, max, etc.) over a rolling time window to smooth fluctuations and highlight trends.</li>
									<li><span class="blue-arrow">&#9654;</span> Helps in trend detection, noise reduction, and feature smoothing for better predictions.</li>
									<li><span class="blue-arrow">&#9654;</span> Commonly used in economic forecasting, sales analytics, and sensor data analysis.</li>
									<li><span class="blue-arrow">&#9654;</span> Example: Calculating a 7-day moving average for sales forecasting:</li>
									<li>
										<div style="text-align: center;">
										  <math>
											<mrow>
											  <mi>Rolling</mi>
											  <mo>&#xA0;</mo> <!-- Non-breaking space -->
											  <mi>Mean</mi>
											  <mo>=</mo>
											  <mfrac>
												<mrow>
												  <msub>
													<mi>X</mi>
													<mrow><mi>t</mi> <mo>-</mo> <mn>6</mn></mrow>
												  </msub>
												  <mo>+</mo>
												  <msub>
													<mi>X</mi>
													<mrow><mi>t</mi> <mo>-</mo> <mn>5</mn></mrow>
												  </msub>
												  <mo>+</mo>
												  <mo>...</mo>
												  <mo>+</mo>
												  <msub>
													<mi>X</mi>
													<mi>t</mi>
												  </msub>
												</mrow>
												<mn>7</mn>
											  </mfrac>
											</mrow>
										  </math>
										</div>						
									</li>
								</ul>

								<li>✅ <strong>Fourier Transform & Frequency Features</strong></li>
								<ul class="no-bullets">
									<li><span class="blue-arrow">&#9654;</span>Converts time-domain data into the frequency domain to identify dominant periodic signals..</li>
									<li><span class="blue-arrow">&#9654;</span>Helps detect seasonality patterns, cyclic behaviors, and hidden periodic trends.</li>
									<li><span class="blue-arrow">&#9654;</span>Used in power grid monitoring, speech recognition, and financial time-series analysis.</li>
								</ul>

								<li>✅ <strong>Seasonality Indicators</strong></li>
								<ul class="no-bullets">							
									<li><span class="blue-arrow">&#9654;</span> Encodes recurring seasonal patterns, such as hour of the day, day of the week, or month of the year, to help models learn time-based effects.</li>
									<li><span class="blue-arrow">&#9654;</span> Example: Adding binary or cyclical features to represent days of the week in sales forecasting.</li>
									<li><span class="blue-arrow">&#9654;</span> Applied in energy consumption prediction, retail sales trends, and temperature forecasting.</li>
								</ul>

								<li>✅ <strong> Exponential Weighted Moving Averages (EWMA)</strong></li>
								<ul class="no-bullets">
									<li><span class="blue-arrow">&#9654;</span> Assigns exponentially decreasing weights to past observations, giving more importance to recent data points.</li>
									<li><span class="blue-arrow">&#9654;</span> Helps in trend forecasting and anomaly detection by adapting to recent changes faster than simple moving averages..</li>
									<li><span class="blue-arrow">&#9654;</span> Common in financial modeling, stock price analysis, and real-time monitoring.</li>							
								</ul>
							<p><strong>Why Feature Engineering in Time-Series Matters?</strong></p>
							<ul>
								<li> Captures temporal dependencies, making models aware of past patterns and trends.</li>
								<li> Enhances predictive accuracy by adding informative features that improve signal-to-noise ratio.</li>
								<li> Reduces overfitting by replacing raw timestamps with meaningful time-related features.</li>
								<li> Improves seasonality detection, allowing models to recognize recurring patterns..</li>
								<li> Transforms raw data into structured inputs, making it easier for machine learning models to process.</li>
							</ul>
							<p>By leveraging time-series feature engineering techniques, machine learning models become more accurate, interpretable, and effective in real-world forecasting applications.</p>														
							</ul>							
						</div>
			`,	

			
			'model_selection': `
					<h1 class="blue-bolt">Choosing the Right Machine Learning Approach</h1>

					<div class="content-box">
						<h3>Choosing Between Supervised & Unsupervised Learning</h3>
						<p><strong>Definition:</strong> The first step in machine learning is determining whether the dataset contains labeled outputs. The presence or absence of labels dictates the learning approach: supervised learning (with labeled outputs) or unsupervised learning (without labels).</p>
						
						<p><strong>Categories:</strong></p>
						<ul>
							<li>✅ <strong>Supervised Learning</strong></li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Used when the dataset contains explicit labels.</li>
								<li><span class="blue-arrow">&#9654;</span> Helps models learn mapping functions from input to output.</li>
								<li><span class="blue-arrow">&#9654;</span> Applied in predictive tasks like spam detection, price prediction, and medical diagnosis.</li>
							</ul>

							<li>✅ <strong>Unsupervised Learning</strong></li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Used when no predefined labels exist, focusing on finding hidden patterns.</li>
								<li><span class="blue-arrow">&#9654;</span> Useful for tasks like customer segmentation and anomaly detection.</li>
								<li><span class="blue-arrow">&#9654;</span> Commonly applied in exploratory data analysis and clustering problems.</li>
							</ul>
						</ul>
					</div>

					<div class="content-box">
						<h3>Comparing ML Models</h3>
						<p><strong>Definition:</strong> Different machine learning models perform best based on dataset size, feature types, and problem complexity. Choosing the right model improves accuracy and computational efficiency.</p>
						
						<p><strong>Model Selection Based on Problem Type:</strong></p>
						<ul>
							<li>✅ <strong>Regression (Predicting Continuous Values)</strong></li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Suitable for predicting numeric outputs like prices and temperature.</li>
								<li><span class="blue-arrow">&#9654;</span> Common models include Linear Regression, Random Forest, and Neural Networks.</li>
							</ul>
							
							<li>✅ <strong>Classification (Predicting Categories)</strong></li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Used for categorical outcomes like fraud detection and disease diagnosis.</li>
								<li><span class="blue-arrow">&#9654;</span> Algorithms include Logistic Regression, Decision Trees, SVM, and Naïve Bayes.</li>
							</ul>
							
							<li>✅ <strong>Clustering (Grouping Similar Items)</strong></li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Used in segmentation tasks like customer grouping.</li>
								<li><span class="blue-arrow">&#9654;</span> Common methods include k-Means, DBSCAN, and Gaussian Mixture Models.</li>
							</ul>
						</ul>
					</div>

					<div class="content-box">
						<h3>Using Cross-Validation</h3>
						<p><strong>Definition:</strong> Cross-validation prevents overfitting by testing models on multiple data splits. This ensures the model generalizes well to new data.</p>
						
						<p><strong>Common Methods:</strong></p>
						<ul>
							<li>✅ <strong>K-Fold Cross-Validation</strong></li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Splits data into k subsets and trains on different combinations.</li>
								<li><span class="blue-arrow">&#9654;</span> Provides a robust estimate of model performance.</li>
							</ul>
							
							<li>✅ <strong>Stratified Cross-Validation</strong></li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Ensures class distribution remains balanced across folds.</li>
							</ul>
							
							<li>✅ <strong>Leave-One-Out (LOO) Cross-Validation</strong></li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Uses one sample as a test set and the rest for training.</li>
								<li><span class="blue-arrow">&#9654;</span> Computationally expensive but highly precise.</li>
							</ul>
						</ul>
					</div>

					<div class="content-box">
						<h3>Example: Choosing a Model for Predicting House Prices 🏡</h3>
						<p><strong>Scenario:</strong> You have a dataset with features such as house prices, square footage, number of bedrooms, and location.</p>
						
						<p><strong>Steps:</strong></p>
						<ul>
							<li>✅ <strong>Step 1: Identify the Task</strong></li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Since house price prediction requires a numeric output, it's a regression problem.</li>
							</ul>
							
							<li>✅ <strong>Step 2: Select Candidate Models</strong></li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Choose regression models like Linear Regression, Decision Trees, or Random Forest.</li>
							</ul>
							
							<li>✅ <strong>Step 3: Perform Cross-Validation</strong></li>
							<ul class="no-bullets">
								<li><span class="blue-arrow">&#9654;</span> Apply k-Fold Cross-Validation and evaluate performance based on error metrics.</li>
								<li><span class="blue-arrow">&#9654;</span> Select the model with the lowest error and best generalization.</li>
							</ul>
						</ul>

					<p><strong>Why These Steps Matter?</strong></p>
					<ul>
						<li>Ensures correct learning approach by distinguishing between supervised and unsupervised tasks.</li>
						<li>Selects the best model based on dataset structure and problem type.</li>
						<li>Prevents overfitting through cross-validation techniques.</li>
						<li>Improves model accuracy by systematically evaluating different algorithms.</li>
						<li>Enhances real-world performance by ensuring generalization beyond training data.</li>
					</ul>

					<p>By following these structured steps, machine learning models can deliver better predictions, improved reliability, and more meaningful insights for decision-making! </p>
				</div>
			`,	
						
			
			
			'model_evaluation': `
				<h1 class="blue-bolt">Model Evaluation</h1>

				<div class="content-box">
					<h3>Evaluating Classification Models</h3>
					<p><strong>Definition:</strong> Classification models are assessed based on their ability to correctly predict categorical labels. The key metrics focus on accuracy, precision, recall, and F1-score to ensure balanced performance.</p>
					
					<p><strong>Common Metrics:</strong></p>
					<ul>
						<li>✅ <strong>Accuracy</strong></li>
						<ul class="no-bullets">
							<li><span class="blue-arrow">&#9654;</span> Measures the overall percentage of correct predictions.</li>
							<li><span class="blue-arrow">&#9654;</span> Best used when class distribution is balanced.</li>
						</ul>
						
						<li>✅ <strong>Precision</strong></li>
						<ul class="no-bullets">
							<li><span class="blue-arrow">&#9654;</span> Measures the proportion of correctly predicted positive cases out of all predicted positives.</li>
							<li><span class="blue-arrow">&#9654;</span> Important when <strong>false positives need to be minimized</strong>, such as in fraud detection.</li>
						</ul>
						
						<li>✅ <strong>Recall (Sensitivity)</strong></li>
						<ul class="no-bullets">
							<li><span class="blue-arrow">&#9654;</span> Measures the proportion of actual positive cases correctly identified.</li>
							<li><span class="blue-arrow">&#9654;</span> Crucial in medical diagnosis where <strong>missing a positive case is costly</strong>.</li>
						</ul>
						
						<li>✅ <strong>F1-Score</strong></li>
						<ul class="no-bullets">
							<li><span class="blue-arrow">&#9654;</span> Harmonic mean of precision and recall, balancing both metrics.</li>
							<li><span class="blue-arrow">&#9654;</span> Useful when there is <strong>an imbalance between positive and negative classes</strong>.</li>
						</ul>
					</ul>
				</div>

				<div class="content-box">
					<h3>Evaluating Regression Models</h3>
					<p><strong>Definition:</strong> Regression models are evaluated based on how accurately they predict continuous numerical values. The focus is on error metrics that measure deviation from actual values.</p>
					
					<p><strong>Common Metrics:</strong></p>
					<ul>
						<li>✅ <strong>Mean Absolute Error (MAE)</strong></li>
						<ul class="no-bullets">
							<li><span class="blue-arrow">&#9654;</span> Measures the average absolute difference between predicted and actual values.</li>
							<li><span class="blue-arrow">&#9654;</span> Provides an intuitive measure of prediction error in original units.</li>
						</ul>
						
						<li>✅ <strong>Mean Squared Error (MSE)</strong></li>
						<ul class="no-bullets">
							<li><span class="blue-arrow">&#9654;</span> Squares errors before averaging, penalizing larger errors more heavily.</li>
							<li><span class="blue-arrow">&#9654;</span> Suitable for models where <strong>large deviations need to be avoided</strong>.</li>
						</ul>
						
						<li>✅ <strong>R-Squared (R²)</strong></li>
						<ul class="no-bullets">
							<li><span class="blue-arrow">&#9654;</span> Measures how well the independent variables explain the variance in the target variable.</li>
							<li><span class="blue-arrow">&#9654;</span> Values range from <strong>0 to 1</strong>, with higher values indicating better model fit.</li>
						</ul>
					</ul>
				</div>

				<div class="content-box">
					<h3>Example: Evaluating a Spam Detection Model 📧</h3>
					<p><strong>Scenario:</strong> A classifier is predicting whether an email is spam or not. Model performance is assessed using a confusion matrix.</p>
					
					<p><strong>Confusion Matrix:</strong></p>
					<table>
						<tr>
							<th></th>
							<th>Actual Spam</th>
							<th>Actual Not Spam</th>
						</tr>
						<tr>
							<td><strong>Predicted Spam</strong></td>
							<td>TP (True Positives)</td>
							<td>FP (False Positives)</td>
						</tr>
						<tr>
							<td><strong>Predicted Not Spam</strong></td>
							<td>FN (False Negatives)</td>
							<td>TN (True Negatives)</td>
						</tr>
					</table>
					
					<p><strong>Calculation:</strong></p>
					<ul>
						<li>✅ <strong>Precision</strong> = TP / (TP + FP) → Measures correctness of positive predictions.</li>
						<li>✅ <strong>Recall</strong> = TP / (TP + FN) → Measures completeness of actual positives detected.</li>
						<li>✅ <strong>F1-Score</strong> = 2 × (Precision × Recall) / (Precision + Recall) → Balances precision and recall.</li>
					</ul>
				

				<p><strong>Why Model Evaluation Matters?</strong></p>
				<ul>
					<li>Ensures models are accurately capturing patterns in data.</li>
					<li>Helps in comparing different models before final selection.</li>
					<li>Prevents overfitting by assessing generalization to new data.</li>
					<li>Ensures ethical and reliable decision-making in real-world applications.</li>
				</ul>

				<p>By applying the right evaluation metrics, machine learning models can be fine-tuned for <strong>higher accuracy, fairness, and efficiency</strong>, leading to better decision-making.</p>
				
			</div>
			`,	



			'regression': `	
					<div class="ml-section">
						<h2> Linear Regression</h2>

						<div class="content-box">
							<h3>Introduction</h3>
							<p><strong>Definition:</strong> Linear Regression is a supervised learning algorithm used for predicting continuous values by modeling a linear relationship between input variables and output.</p>
							<p><strong>Mathematical Formula:</strong></p>
							<p>
								<div style="text-align: center;">
								  <math>
									<mi>y</mi>
									<mo>=</mo>
									<mi>m</mi>
									<mo>&#x22C5;</mo> <!-- Dot operator for multiplication -->
									<mi>x</mi>
									<mo>+</mo>
									<mi>b</mi>
								  </math>
								</div>						
							</p>
							<ul>
								<li>✅ <math><mi> y </mi></math> → Predicted output (dependent variable)</li>
								<li>✅ <math><mi> x </mi></math>  → Input feature (independent variable)</li>
								<li>✅ <math><mi> m </mi></math> → Slope of the line</li>
								<li>✅ <math><mi> b </mi></math> → Intercept</li>
							</ul>
						</div>

						<div class="content-box">
							<h3>Python Implementation</h3>
							<p>We will use Scikit-Learn to implement Linear Regression.</p>
							<pre class="code-block">
	import numpy as np
	import matplotlib.pyplot as plt
	from sklearn.linear_model import LinearRegression
	from sklearn.model_selection import train_test_split

	# Generate synthetic data
	np.random.seed(42)
	X = 2 * np.random.rand(100, 1)
	y = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3x + noise

	#  Split data into training and testing sets
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	#  Train Linear Regression Model
	model = LinearRegression()
	model.fit(X_train, y_train)

	#  Make predictions
	y_pred = model.predict(X_test)

	#  Plot the results
	plt.scatter(X_test, y_test, color="blue", label="Actual Data")
	plt.plot(X_test, y_pred, color="red", linewidth=2, label="Predicted Line")
	plt.xlabel("Input Feature (X)")
	plt.ylabel("Target Output (y)")
	plt.legend()
	plt.show()
							</pre>
						</div>

						<div class="content-box">
							<h3>Advanced Concepts & Research Extensions</h3>
							<ul>
								<li>✅ Limitations: Assumes a linear relationship, sensitive to outliers.</li>
								<li>✅ Advanced Improvements:
									<ul>
										<li>✅ Ridge Regression (L2 Regularization): Adds penalty for large coefficients.</li>
										<li>✅ LASSO Regression (L1 Regularization): Shrinks coefficients to zero for feature selection.</li>
										<li>✅ Polynomial Regression: Extends linear regression for non-linear data.</li>
									</ul>
								</li>
							</ul>
						</div>

						<div class="content-box">
							<h3>Pros & Cons</h3>
							<table>
								<tr>
									<th>Pros ✅</th>
									<th>Cons ❌</th>
								</tr>
								<tr>
									<td>✅ Simple and easy to interpret</td>
									<td>❌ Assumes linearity between input and output</td>
								</tr>
								<tr>
									<td>✅ Computationally efficient</td>
									<td>❌ Sensitive to outliers</td>
								</tr>
								<tr>
									<td>✅ Works well with small datasets</td>
									<td>❌ Cannot capture complex relationships</td>
								</tr>
							</table>
						</div>
					</div>
			`,	
			
			'knn': `
					<div class="ml-section">
						<h2> k-Nearest Neighbors (k-NN)</h2>

						<div class="content-box">
							<h3>1️⃣ Introduction</h3>
							<p><strong>Definition:</strong> k-NN is a simple, non-parametric supervised learning algorithm that classifies data points based on their similarity to nearby neighbors.</p>
							<p><strong>Mathematical Formula:</strong> The distance between two points is calculated using Euclidean Distance:</p>
							<p>
								<div style="text-align: center;">
								  <math>
									<mi>d</mi>
									<mo>(</mo>
									<mi>p</mi>
									<mo>,</mo>
									<mi>q</mi>
									<mo>)</mo>
									<mo>=</mo>
									<msqrt>
									  <mrow>
										<munderover>
										  <mo>&#8721;</mo> <!-- Summation symbol -->
										  <mrow>
											<mi>i</mi>
											<mo>=</mo>
											<mn>1</mn>
										  </mrow>
										  <mi>n</mi>
										</munderover>
										<mo>(</mo>
										<msub>
										  <mi>p</mi>
										  <mi>i</mi>
										</msub>
										<mo>-</mo>
										<msub>
										  <mi>q</mi>
										  <mi>i</mi>
										</msub>
										<msup>
										  <mo>)</mo>
										  <mn>2</mn>
										</msup>
									  </mrow>
									</msqrt>
								  </math>
								</div>						
															</p>
						</div>

						<div class="content-box">
				<h3>2️⃣ Python Implementation</h3>
				<pre class="code-block">
		from sklearn.neighbors import KNeighborsClassifier
		from sklearn.model_selection import train_test_split
		from sklearn.datasets import load_iris
		from sklearn.metrics import accuracy_score

		#  Load dataset
		iris = load_iris()
		X, y = iris.data, iris.target

		#  Split data
		X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

		#  Train k-NN Model
		knn = KNeighborsClassifier(n_neighbors=3)
		knn.fit(X_train, y_train)

		#  Make predictions
		y_pred = knn.predict(X_test)

		#  Evaluate model
		print("Accuracy:", accuracy_score(y_test, y_pred))
							</pre>
						</div>

						<div class="content-box">
							<h3>3️⃣ Advanced Concepts & Research Extensions</h3>
							<ul>
								<li>✅ Limitations: Computationally expensive for large datasets.</li>
								<li>✅ Advanced Improvements:
									<ul>
										<li>✅ KD-Tree & Ball-Tree: Faster search for high-dimensional data.</li>
										<li>✅ Weighted k-NN: Assigns different weights to neighbors based on distance.</li>
									</ul>
								</li>
							</ul>
						</div>

						<div class="content-box">
							<h3>4️⃣ Pros & Cons</h3>
							<table>
								<tr><th>Pros ✅</th><th>Cons ❌</th></tr>
								<tr><td>✅ Simple and effective</td><td>❌ Slow for large datasets</td></tr>
								<tr><td>✅ No training phase required</td><td>❌ Sensitive to irrelevant features</td></tr>
								<tr><td>✅ Works well with non-linear data</td><td>❌ Requires tuning of 'k' parameter</td></tr>
							</table>
						</div>
					</div>
								
			`,	
			'svm': `
				<div class="ml-section">
					<h2> Support Vector Machines (SVM)</h2>

					<div class="content-box">
						<h3>1️⃣ Introduction</h3>
						<p><strong>Definition:</strong> SVM is a supervised learning algorithm that finds the optimal hyperplane to separate different classes in high-dimensional space.</p>
						<p><strong>Mathematical Formula:</strong> The decision boundary is given by:</p>
						<p>
							<div style="text-align: center;">
							  <math>
								<msup>
								  <mi>w</mi>
								  <mi>T</mi>
								</msup>
								<mo>&#x22C5;</mo>
								<mi>x</mi>
								<mo>+</mo>
								<mi>b</mi>
								<mo>=</mo>
								<mn>0</mn>
							  </math>
							</div>
						</p>
					</div>

					<div class="content-box">
						<h3>2️⃣ Python Implementation</h3>
						<pre class="code-block">
		from sklearn.svm import SVC
		from sklearn.datasets import make_classification
		from sklearn.model_selection import train_test_split
		from sklearn.metrics import accuracy_score

		#  Generate synthetic dataset
		X, y = make_classification(n_samples=100, n_features=2, random_state=42)

		#  Split data
		X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

		#  Train SVM Model
		svm = SVC(kernel='linear')
		svm.fit(X_train, y_train)

		#  Make predictions
		y_pred = svm.predict(X_test)

		#  Evaluate model
		print("Accuracy:", accuracy_score(y_test, y_pred))
						</pre>
					</div>

					<div class="content-box">
						<h3>3️⃣ Advanced Concepts & Research Extensions</h3>
						<ul>
							<li>✅ Limitations: Computationally expensive for large datasets.</li>
							<li>✅ Advanced Improvements:
								<ul>
									<li>✅ Kernel Trick: Enables SVM to handle non-linear data.</li>
									<li>✅ Soft Margin SVM: Allows some misclassification for better generalization.</li>
								</ul>
							</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>4️⃣ Pros & Cons</h3>
						<table>
							<tr><th>Pros ✅</th><th>Cons ❌</th></tr>
							<tr><td>✅ Works well with high-dimensional data</td><td>❌ Slow for large datasets</td></tr>
							<tr><td>✅ Effective with small sample sizes</td><td>❌ Sensitive to parameter selection</td></tr>
						</table>
					</div>
				</div>
			`,	

			'kernel_regression': `	
					<div class="ml-section">
						<h2> Kernel Regression</h2>

						<div class="content-box">
							<h3>1️⃣ Introduction</h3>
							<p><strong>Definition:</strong> Kernel Regression is a non-parametric technique that estimates the relationship between variables using kernel functions.</p>
							<p><strong>Mathematical Formula:</strong></p>
							<p>
								<div style="text-align: center;">
								  <math>
									<mi>f</mi>
									<mo>(</mo>
									<mi>x</mi>
									<mo>)</mo>
									<mo>=</mo>
									<munderover>
									  <mo>&#8721;</mo> <!-- Summation symbol -->
									  <mrow>
										<mi>i</mi>
										<mo>=</mo>
										<mn>1</mn>
									  </mrow>
									  <mi>n</mi>
									</munderover>
									<mi>K</mi>
									<mo>(</mo>
									<mi>x</mi>
									<mo>,</mo>
									<msub>
									  <mi>x</mi>
									  <mi>i</mi>
									</msub>
									<mo>)</mo>
									<msub>
									  <mi>y</mi>
									  <mi>i</mi>
									</msub>
								  </math>
								</div>
							</p>
							<ul>
								<li>✅ 
									  <math>
										<mi>K</mi>
										<mo>(</mo>
										<mi>x</mi>
										<mo>,</mo>
										<msub>
										  <mi>x</mi>
										  <mi>i</mi>
										</msub>
										<mo>)</mo>
									  </math>
								→ Kernel function measuring similarity.</li>
								<li>✅ 
									  <math>
										<msub>
										  <mi>y</mi>
										  <mi>i</mi>
										</msub>
									  </math>
								→ Observed values.</li>
							</ul>
						</div>

						<div class="content-box">
							<h3>2️⃣ Python Implementation</h3>
							<pre class="code-block">
		import numpy as np
		import matplotlib.pyplot as plt
		from sklearn.neighbors import KernelDensity

		#  Generate synthetic data
		np.random.seed(42)
		X = np.linspace(-3, 3, 100)[:, np.newaxis]
		y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])

		#  Fit Kernel Regression
		kde = KernelDensity(kernel='gaussian', bandwidth=0.5)
		kde.fit(X)

		#  Predict and plot
		X_pred = np.linspace(-3, 3, 100)[:, np.newaxis]
		log_density = kde.score_samples(X_pred)

		plt.plot(X_pred, np.exp(log_density), color="red", label="Kernel Regression Fit")
		plt.scatter(X, y, color="blue", alpha=0.5, label="Data Points")
		plt.legend()
		plt.show()
							</pre>
				</div>

				<div class="content-box">
					<h3>3️⃣ Advanced Concepts & Research Extensions</h3>
					<ul>
						<li>✅ Limitations: Sensitive to bandwidth selection, computationally expensive.</li>
						<li>✅ Advanced Improvements:
							<ul>
								<li>✅ Adaptive Kernel Regression: Adjusts bandwidth dynamically.</li>
								<li>✅ Gaussian Process Regression: Extends Kernel Regression using probability distributions.</li>
							</ul>
						</li>
					</ul>
				</div>

				<div class="content-box">
					<h3>4️⃣ Pros & Cons</h3>
					<table>
						<tr><th>Pros ✅</th><th>Cons ❌</th></tr>
						<tr><td>✅ Works well with non-linear data</td><td>❌ Slow for large datasets</td></tr>
						<tr><td>✅ No assumption about data distribution</td><td>❌ Requires careful bandwidth tuning</td></tr>
					</table>
				</div>
			</div>
			`,	
			'expectation_maximization': `
					<div class="ml-section">
						<h2> Expectation Maximization Algorithm</h2>

						<div class="content-box">
							<h3>1️⃣ Introduction</h3>
							<p><strong>Definition:</strong> Expectation Maximization (EM) is an iterative algorithm used to estimate missing or latent variables in probabilistic models.</p>
							<p><strong>Mathematical Formula:</strong></p>
							<p>
								<div style="text-align: center;">
								  <math>
									<mi>Q</mi>
									<mo>(</mo>
									<mi>&theta;</mi>
									<mo>|</mo>
									<msup>
									  <mi>&theta;</mi>
									  <mo>(t)</mo>
									</msup>
									<mo>)</mo>
									<mo>=</mo>
									<mi>E</mi>
									<mo>[</mo>
									<mo>log</mo>
									<mo>&#x2061;</mo> <!-- Function Application -->
									<mi>P</mi>
									<mo>(</mo>
									<mi>X</mi>
									<mo>,</mo>
									<mi>Z</mi>
									<mo>|</mo>
									<mi>&theta;</mi>
									<mo>)</mo>
									<mo>|</mo>
									<mi>X</mi>
									<mo>,</mo>
									<msup>
									  <mi>&theta;</mi>
									  <mo>(t)</mo>
									</msup>
									<mo>]</mo>
								  </math>
								</div>							
							</p>
							<ul>
								<li>✅ Expectation Step (E-Step): Estimates missing data based on current parameters.</li>
								<li>✅ Maximization Step (M-Step): Updates model parameters to maximize likelihood.</li>
							</ul>
						</div>

						<div class="content-box">
							<h3>2️⃣ Python Implementation</h3>
							<pre class="code-block">
		import numpy as np
		from sklearn.mixture import GaussianMixture

		#  Generate synthetic data
		np.random.seed(42)
		X = np.concatenate([np.random.normal(-2, 1, 100), np.random.normal(2, 1, 100)]).reshape(-1, 1)

		#  Fit Gaussian Mixture Model (EM Algorithm)
		gmm = GaussianMixture(n_components=2, covariance_type='full')
		gmm.fit(X)

		#  Predict clusters
		clusters = gmm.predict(X)
		print("Cluster Assignments:", clusters)
							</pre>
						</div>

						<div class="content-box">
							<h3>3️⃣ Advanced Concepts & Research Extensions</h3>
							<ul>
								<li>✅ Limitations: Sensitive to initial parameters, may converge to local optima.</li>
								<li>✅ Advanced Improvements:
									<ul>
										<li>✅ Variational Bayes EM: Incorporates Bayesian priors for robust estimation.</li>
										<li>✅ Hidden Markov Models (HMMs): Uses EM for sequential data.</li>
									</ul>
								</li>
							</ul>
						</div>

						<div class="content-box">
							<h3>4️⃣ Pros & Cons</h3>
							<table>
								<tr><th>Pros ✅</th><th>Cons ❌</th></tr>
								<tr><td>✅ Works with incomplete data</td><td>❌ Prone to local optima</td></tr>
								<tr><td>✅ Handles probabilistic models</td><td>❌ Requires good initialization</td></tr>
							</table>
						</div>
					</div>

			`,				
			'entropy': `
				<div class="ml-section">
					<h2> Entropy & Information Gain</h2>

					<div class="content-box">
						<h3>1️⃣ Introduction</h3>
						<p><strong>Definition:</strong> Entropy is a measure of randomness or impurity in a dataset. It is used in decision tree algorithms to determine the best feature splits.</p>
						<p><strong>Mathematical Formula:</strong></p>
						<p>
							<div style="text-align: center;">
							  <math>
								<mi>H</mi>
								<mo>(</mo>
								<mi>S</mi>
								<mo>)</mo>
								<mo>=</mo>
								<mo>-</mo>
								<munderover>
								  <mo>&#8721;</mo> <!-- Summation symbol -->
								  <mrow>
									<mi>i</mi>
									<mo>=</mo>
									<mn>1</mn>
								  </mrow>
								  <mi>c</mi>
								</munderover>
								<msub>
								  <mi>p</mi>
								  <mi>i</mi>
								</msub>
								<mo>&#x22C5;</mo> <!-- Dot operator for multiplication -->
								<mo>log</mo>
								<msub>
								  <mn>2</mn>
								  <mrow></mrow>
								</msub>
								<msub>
								  <mi>p</mi>
								  <mi>i</mi>
								</msub>
							  </math>
							</div>
						</p>
						<ul>
							<li>✅ <strong>H(S)</strong> : Entropy of dataset \( S \).</li>
							<li>✅ <strong>p_i</strong>  : Probability of class \( i \).</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>2️⃣ Python Implementation</h3>
						<pre class="code-block">
				import numpy as np
				from scipy.stats import entropy

				#  Calculate entropy for a simple dataset
				labels = np.array([0, 0, 1, 1, 1, 1, 0, 0, 0, 1])
				values, counts = np.unique(labels, return_counts=True)

				#  Compute entropy
				entropy_value = entropy(counts, base=2)
				print("Entropy:", entropy_value)
						</pre>
					</div>

					<div class="content-box">
						<h3>3️⃣ Advanced Concepts & Research Extensions</h3>
						<ul>
							<li>✅ Limitations: Entropy may not work well with highly imbalanced data.</li>
							<li>✅ Advanced Improvements:
								<ul>
									<li>✅ Gini Impurity: An alternative to entropy for decision tree splitting.</li>
									<li>✅ Mutual Information: Used in feature selection techniques.</li>
								</ul>
							</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>4️⃣ Pros & Cons</h3>
						<table>
							<tr><th>Pros ✅</th><th>Cons ❌</th></tr>
							<tr><td>✅ Helps in feature selection</td><td>❌ May bias toward attributes with more values</td></tr>
							<tr><td>✅ Forms the basis for decision trees</td><td>❌ Computationally expensive for large datasets</td></tr>
						</table>
					</div>
				</div>				
			`,	
			
			
			'id3': `
				<div class="ml-section">
					<h2> ID3 Algorithm (Decision Trees)</h2>

					<div class="content-box">
						<h3>1️⃣ Introduction</h3>
						<p><strong>Definition:</strong> ID3 (Iterative Dichotomiser 3) is a decision tree learning algorithm that builds a tree using entropy and information gain.</p>
						<p><strong>Mathematical Formula:</strong></p>
						<p>
							<div style="text-align: center;">
							  <math>
								<mi>IG</mi>
								<mo>(</mo>
								<mi>S</mi>
								<mo>,</mo>
								<mi>A</mi>
								<mo>)</mo>
								<mo>=</mo>
								<mi>H</mi>
								<mo>(</mo>
								<mi>S</mi>
								<mo>)</mo>
								<mo>-</mo>
								<munderover>
								  <mo>&#8721;</mo> <!-- Summation symbol -->
								  <mrow>
									<mi>v</mi>
									<mo>&#x2208;</mo> <!-- Element of (∈) -->
									<mi>Values</mi>
									<mo>(</mo>
									<mi>A</mi>
									<mo>)</mo>
								  </mrow>
								</munderover>
								<mfrac>
								  <mrow>
									<mo>|</mo><msub><mi>S</mi><mi>v</mi></msub><mo>|</mo>
								  </mrow>
								  <mrow>
									<mo>|</mo><mi>S</mi><mo>|</mo>
								  </mrow>
								</mfrac>
								<mo>&#x22C5;</mo> <!-- Multiplication dot -->
								<mi>H</mi>
								<mo>(</mo>
								<msub><mi>S</mi><mi>v</mi></msub>
								<mo>)</mo>
							  </math>
							</div>					
						</p>
						<ul>
							<li>✅ \( IG(S, A) \) → Information Gain when splitting on attribute \( A \).</li>
							<li>✅ \( H(S) \) → Entropy of dataset \( S \).</li>
							<li>✅ \( S_v \) → Subset of \( S \) where attribute \( A \) has value \( v \).</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>2️⃣ Python Implementation</h3>
						<pre class="code-block">
		from sklearn.tree import DecisionTreeClassifier
		from sklearn.datasets import load_iris
		from sklearn.model_selection import train_test_split
		from sklearn import tree

		#  Load dataset
		iris = load_iris()
		X, y = iris.data, iris.target

		#  Split data
		X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

		#  Train Decision Tree (ID3 Algorithm)
		clf = DecisionTreeClassifier(criterion='entropy')
		clf.fit(X_train, y_train)

		#  Visualize the tree
		tree.plot_tree(clf, filled=True)
						</pre>
					</div>

					<div class="content-box">
						<h3>3️⃣ Advanced Concepts & Research Extensions</h3>
						<ul>
							<li>✅ Limitations: ID3 tends to overfit when dealing with noisy data.</li>
							<li>✅ Advanced Improvements:
								<ul>
									<li>✅ C4.5 Algorithm: An improvement over ID3 that handles continuous attributes.</li>
									<li>✅ Random Forests: Uses multiple decision trees to improve accuracy.</li>
								</ul>
							</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>4️⃣ Pros & Cons</h3>
						<table>
							<tr><th>Pros ✅</th><th>Cons ❌</th></tr>
							<tr><td>✅ Simple and easy to understand</td><td>❌ Overfits small datasets</td></tr>
							<tr><td>✅ Works with categorical and numerical data</td><td>❌ Biased toward attributes with many values</td></tr>
						</table>
					</div>
				</div>

			`,	


			'bootstraping': `
				<div class="ml-section">
					<h2> Boosting (AdaBoost & Gradient Boosting)</h2>

					<div class="content-box">
						<h3>1️⃣ Introduction</h3>
						<p><strong>Definition:</strong> Boosting is an ensemble learning method that improves model accuracy by training sequential weak learners and adjusting their weights to focus on hard-to-classify instances.</p>
						<p><strong>Mathematical Concept:</strong> Given dataset \( D \), boosting assigns weights \( w_i \) to samples and updates them iteratively:</p>
						<p>
							<div style="text-align: center;">
							  <math>
								<msub>
								  <mi>w</mi>
								  <mrow>
									<mi>i</mi>
								  </mrow>
								</msub>
								<msup>
								  <mrow>
									<mo>(</mo>
									<mi>t</mi>
									<mo>+</mo>
									<mn>1</mn>
									<mo>)</mo>
								  </mrow>
								</msup>
								<mo>=</mo>
								<msub>
								  <mi>w</mi>
								  <mrow>
									<mi>i</mi>
								  </mrow>
								</msub>
								<msup>
								  <mrow>
									<mo>(</mo>
									<mi>t</mi>
									<mo>)</mo>
								  </mrow>
								</msup>
								<mo>&#x22C5;</mo> <!-- Dot operator for multiplication -->
								<msup>
								  <mi>e</mi>
								  <mrow>
									<mo>-</mo>
									<msub>
									  <mi>&alpha;</mi>
									  <mi>t</mi>
									</msub>
									<mo>&#x22C5;</mo>
									<msub>
									  <mi>y</mi>
									  <mi>i</mi>
									</msub>
									<mo>&#x22C5;</mo>
									<msub>
									  <mi>h</mi>
									  <mi>t</mi>
									</msub>
									<mo>(</mo>
									<msub>
									  <mi>x</mi>
									  <mi>i</mi>
									</msub>
									<mo>)</mo>
								  </mrow>
								</msup>
							  </math>
							</div>						
						</p>
						<ul>
							<li>✅ \( \alpha_t \) → Model weight.</li>
							<li>✅ \( y_i \) → True label.</li>
							<li>✅ \( h_t(x_i) \) → Prediction from weak learner.</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>2️⃣ Python Implementation</h3>
				<pre class="code-block">
	from sklearn.ensemble import AdaBoostClassifier
	from sklearn.tree import DecisionTreeClassifier
	from sklearn.datasets import make_classification
	from sklearn.model_selection import train_test_split

	#  Generate synthetic data
	X, y = make_classification(n_samples=1000, random_state=42)

	#  Split data
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

	#  Train AdaBoost Classifier
	adaboost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=42)
	adaboost.fit(X_train, y_train)

	#  Evaluate Model
	print("Accuracy:", adaboost.score(X_test, y_test))
						</pre>
					</div>

					<div class="content-box">
						<h3>3️⃣ Advanced Concepts & Research Extensions</h3>
						<ul>
							<li>✅ Limitations: Sensitive to noisy data.</li>
							<li>✅ Advanced Improvements:
								<ul>
									<li>✅ Gradient Boosting (XGBoost, LightGBM): Optimizes using gradient descent.</li>
									<li>✅ CatBoost: Efficient for categorical features.</li>
								</ul>
							</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>4️⃣ Pros & Cons</h3>
						<table>
							<tr><th>Pros ✅</th><th>Cons ❌</th></tr>
							<tr><td>✅ Improves weak learners</td><td>❌ Computationally expensive</td></tr>
							<tr><td>✅ Works well on imbalanced data</td><td>❌ Prone to overfitting</td></tr>
						</table>
					</div>
				</div>
			`,	


		
			'bagging': `
				<div class="ml-section">
					<h2> Bagging (Bootstrap Aggregating)</h2>

					<div class="content-box">
						<h3>1️⃣ Introduction</h3>
						<p><strong>Definition:</strong> Bagging is an ensemble learning method that improves accuracy by training multiple models on different subsets of data and averaging their predictions.</p>
						<p><strong>Mathematical Concept:</strong> Given dataset \( D \), bagging creates \( B \) bootstrap samples 
						  <math>
							<msub>
							  <mi>D</mi>
							  <mn>1</mn>
							</msub>
							<mo>,</mo>
							<msub>
							  <mi>D</mi>
							  <mn>2</mn>
							</msub>
							<mo>,</mo>
							<mo>...</mo>
							<mo>,</mo>
							<msub>
							  <mi>D</mi>
							  <mi>B</mi>
							</msub>
						  </math>
						, trains models 
							<math>
							  <msub>
								<mi>f</mi>
								<mn>1</mn>
							  </msub>
							  <mo>,</mo>
							  <msub>
								<mi>f</mi>
								<mn>2</mn>
							  </msub>
							  <mo>,</mo>
							  <mo>&#x2026;</mo> 
							  <mo>,</mo>
							  <msub>
								<mi>f</mi>
								<mi>B</mi>
							  </msub>
							</math>
						, and averages the predictions:</p>
						<p>
							<div style="text-align: center;">
							  <math>
								<msup>
								  <mi>y</mi>
								  <mo>^</mo>
								</msup>
								<mo>=</mo>
								<mfrac>
								  <mn>1</mn>
								  <mi>B</mi>
								</mfrac>
								<munderover>
								  <mo>&#8721;</mo> <!-- Summation symbol -->
								  <mrow>
									<mi>b</mi>
									<mo>=</mo>
									<mn>1</mn>
								  </mrow>
								  <mi>B</mi>
								</munderover>
								<msub>
								  <mi>f</mi>
								  <mi>b</mi>
								</msub>
								<mo>(</mo>
								<mi>x</mi>
								<mo>)</mo>
							  </math>
							</div>					
						</p>
					</div>

					<div class="content-box">
						<h3>2️⃣ Python Implementation</h3>
						<pre class="code-block">
		from sklearn.ensemble import BaggingClassifier
		from sklearn.tree import DecisionTreeClassifier
		from sklearn.datasets import make_classification
		from sklearn.model_selection import train_test_split

		#  Generate synthetic data
		X, y = make_classification(n_samples=1000, random_state=42)

		#  Split data
		X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

		#  Train Bagging Classifier
		bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)
		bagging.fit(X_train, y_train)

		#  Evaluate Model
		print("Accuracy:", bagging.score(X_test, y_test))
						</pre>
					</div>

					<div class="content-box">
						<h3>3️⃣ Advanced Concepts & Research Extensions</h3>
						<ul>
							<li>✅ Limitations: Works poorly if base learners are too weak.</li>
							<li>✅ Advanced Improvements:
								<ul>
									<li>✅ Random Forest: Uses bagging with feature selection.</li>
									<li>✅ Boosting (Adaboost, Gradient Boosting): Focuses on misclassified samples.</li>
								</ul>
							</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>4️⃣ Pros & Cons</h3>
						<table>
							<tr><th>Pros ✅</th><th>Cons ❌</th></tr>
							<tr><td>✅ Reduces variance & overfitting</td><td>❌ Computationally expensive</td></tr>
							<tr><td>✅ Works well with complex datasets</td><td>❌ Doesn't improve weak base models</td></tr>
						</table>
					</div>
				</div>
			
			`,	
			'random_forest': `
					<div class="ml-section">
						<h2> Random Forest</h2>

						<div class="content-box">
							<h3>1️⃣ Introduction</h3>
							<p><strong>Definition:</strong> Random Forest is an ensemble learning method that builds multiple decision trees and combines their results to improve accuracy and reduce overfitting.</p>
							<p><strong>Mathematical Concept:</strong> Random Forest aggregates predictions from \( n \) decision trees:</p>
							<p>
								<math>
								  <msup>
									<mi>y</mi>
									<mo>^</mo>
								  </msup>
								  <mo>=</mo>
								  <mfrac>
									<mn>1</mn>
									<mi>n</mi>
								  </mfrac>
								  <munderover>
									<mo>&#8721;</mo>
									<mrow>
									  <mi>i</mi>
									  <mo>=</mo>
									  <mn>1</mn>
									</mrow>
									<mi>n</mi>
								  </munderover>
								  <msub>
									<mi>f</mi>
									<mi>i</mi>
								  </msub>
								  <mo>(</mo>
								  <mi>x</mi>
								  <mo>)</mo>
								</math>
							</p>
							<ul>
								<li>✅ \( f_i(x) \) → Prediction from individual decision tree.</li>
								<li>✅ \( \hat{y} \) → Final averaged prediction.</li>
							</ul>
						</div>

						<div class="content-box">
							<h3>2️⃣ Python Implementation</h3>
							<pre class="code-block">
		from sklearn.ensemble import RandomForestClassifier
		from sklearn.datasets import make_classification
		from sklearn.model_selection import train_test_split

		#  Generate synthetic data
		X, y = make_classification(n_samples=1000, random_state=42)

		#  Split data
		X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

		#  Train Random Forest Classifier
		rf = RandomForestClassifier(n_estimators=100, random_state=42)
		rf.fit(X_train, y_train)

		#  Evaluate Model
		print("Accuracy:", rf.score(X_test, y_test))
							</pre>
						</div>

						<div class="content-box">
							<h3>3️⃣ Advanced Concepts & Research Extensions</h3>
							<ul>
								<li>✅ Limitations: Computationally expensive for large datasets.</li>
								<li>✅ Advanced Improvements:
									<ul>
										<li>✅ Extra Trees (Extremely Randomized Trees): Uses more randomness for feature splits.</li>
										<li>✅ Feature Importance: Helps in feature selection by ranking important attributes.</li>
									</ul>
								</li>
							</ul>
						</div>

						<div class="content-box">
							<h3>4️⃣ Pros & Cons</h3>
							<table>
								<tr><th>Pros ✅</th><th>Cons ❌</th></tr>
								<tr><td>✅ Reduces overfitting compared to a single decision tree</td><td>❌ Slow training on large datasets</td></tr>
								<tr><td>✅ Works well with both classification & regression</td><td>❌ Requires tuning of hyperparameters</td></tr>
							</table>
						</div>
					</div>
			`,	

			'mlp': `
				<div class="ml-section">
					<h2> Multilayer Perceptron (MLP)</h2>

					<div class="content-box">
						<h3>1️⃣ Introduction</h3>
						<p><strong>Definition:</strong> A Multilayer Perceptron (MLP) is a type of artificial neural network that consists of an input layer, one or more hidden layers, and an output layer.</p>
						<p><strong>Mathematical Formula:</strong>
						<math>
						  <mi>y</mi>
						  <mo>=</mo>
						  <mi>f</mi>
						  <mo>(</mo>
						  <mi>W</mi>
						  <mo>&#x22C5;</mo>
						  <mi>X</mi>
						  <mo>+</mo>
						  <mi>b</mi>
						  <mo>)</mo>
						</math>
						</p>
					</div>

					<div class="content-box">
						<h3>2️⃣ Python Implementation</h3>
						<pre class="code-block">
			import tensorflow as tf
			from tensorflow.keras.models import Sequential
			from tensorflow.keras.layers import Dense

			#  Define MLP Model
			model = Sequential([
				Dense(32, activation='relu', input_shape=(10,)),
				Dense(16, activation='relu'),
				Dense(1, activation='sigmoid')
			])

			#  Compile the model
			model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

			#  Print Model Summary
			model.summary()
						</pre>
					</div>

					<div class="content-box">
						<h3>3️⃣ Advanced Concepts & Research Extensions</h3>
						<ul>
							<li>✅ <strong>Dropout Regularization:</strong> Prevents overfitting.</li>
							<li>✅ <strong>Batch Normalization:</strong> Speeds up training.</li>
							<li>✅ <strong>Optimization Algorithms:</strong> Adam, RMSprop.</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>4️⃣ Pros & Cons</h3>
						<table class="styled-table">
							<tr><th>Pros ✅</th><th>Cons ❌</th></tr>
							<tr><td>✅ Can model complex relationships</td><td>❌ Requires large datasets</td></tr>
							<tr><td>✅ Works well for structured data</td><td>❌ Computationally expensive</td></tr>
						</table>
					</div>

					<div class="content-box">
						<h3>5️⃣ Real-World Applications</h3>
						<ul>
							<li>✅ <strong>Financial Fraud Detection 💳</strong></li>
							<li>✅ <strong>Medical Diagnosis 🏥</strong></li>
							<li>✅ <strong>Stock Market Prediction 📈</strong></li>
							<li>✅ <strong>Chatbots & NLP 🤖</strong></li>
						</ul>
					</div>
				</div>
			`,
			
			'backpropagation': `
				<div class="ml-section">
					<h2> Backpropagation Algorithm</h2>

					<div class="content-box">
						<h3>1️⃣ Introduction</h3>
						<p><strong>Definition:</strong> Backpropagation is a supervised learning algorithm used to train artificial neural networks by adjusting weights based on the error.</p>
						<p><strong>Mathematical Formula:</strong></p>
						<p>
							<math>
							  <msub>
								<mi>W</mi>
								<mi>new</mi>
							  </msub>
							  <mo>=</mo>
							  <msub>
								<mi>W</mi>
								<mi>old</mi>
							  </msub>
							  <mo>-</mo>
							  <mi>&eta;</mi>
							  <mfrac>
								<mrow>
								  <mo>&#x2202;</mo> 
								  <mi>L</mi>
								</mrow>
								<mrow>
								  <mo>&#x2202;</mo> 
								  <mi>W</mi>
								</mrow>
							  </mfrac>
							</math>
						</p>
						<ul>
							<li>✅ \( L \) : Loss function (e.g., Mean Squared Error)</li>
							<li>✅ \( W \) : Weights of the network</li>
							<li>✅ \( \eta \) : Learning rate (step size for weight updates)</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>2️⃣ Python Implementation</h3>
						<pre class="code-block">
		import numpy as np

		#  Sigmoid activation function
		def sigmoid(x):
			return 1 / (1 + np.exp(-x))

		#  Derivative of Sigmoid
		def sigmoid_derivative(x):
			return x * (1 - x)

		#  Training Data (AND Gate)
		X = np.array([[0,0], [0,1], [1,0], [1,1]])
		y = np.array([[0], [0], [0], [1]])

		#  Initialize Weights
		weights = np.random.rand(2,1)
		bias = np.random.rand(1)
		learning_rate = 0.1

		#  Training Backpropagation for 10000 iterations
		for epoch in range(10000):
			# Forward Propagation
			hidden = sigmoid(np.dot(X, weights) + bias)
			
			# Compute Error
			error = y - hidden
			
			# Backpropagation
			adjustment = error * sigmoid_derivative(hidden)
			weights += np.dot(X.T, adjustment) * learning_rate
			bias += np.sum(adjustment) * learning_rate

		#  Output Final Weights
		print("Final Weights:", weights)
		print("Final Bias:", bias)
						</pre>
					</div>

					<div class="content-box">
						<h3>3️⃣ Advanced Concepts & Research Extensions</h3>
						<ul>
							<li>✅ <strong>Gradient Vanishing Problem:</strong> Deep networks struggle with very small gradients.</li>
							<li>✅ <strong>Batch Normalization:</strong> Speeds up training and stabilizes learning.</li>
							<li>✅ <strong>Adaptive Learning Rates:</strong> Optimizers like Adam and RMSprop improve training speed.</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>4️⃣ Pros & Cons</h3>
						<table class="styled-table">
							<tr><th>Pros ✅</th><th>Cons ❌</th></tr>
							<tr><td>✅ Efficient for training deep networks</td><td>❌ Prone to vanishing gradients</td></tr>
							<tr><td>✅ Works well with non-linear data</td><td>❌ Computationally expensive</td></tr>
						</table>
					</div>

					<div class="content-box">
						<h3>5️⃣ Real-World Applications</h3>
						<ul>
							<li>✅ <strong>Image Recognition 📷</strong> – Used in deep CNN models.</li>
							<li>✅ <strong>Speech Recognition 🎙️</strong> – Powers voice assistants like Siri & Alexa.</li>
							<li>✅ <strong>Autonomous Driving 🚗</strong> – Helps AI systems learn driving behavior.</li>
						</ul>
					</div>
				</div>
			`,
			
			'gradient_descent': `
				<div class="ml-section">
					<h2> Gradient Descent</h2>

					<div class="content-box">
						<h3>1️⃣ Introduction</h3>
						<p><strong>Definition:</strong> Gradient Descent is an optimization algorithm used to minimize the loss function in machine learning models by iteratively adjusting parameters.</p>
						<p><strong>Mathematical Formula:</strong></p>
						<p>
							<math>
							  <msub>
								<mi>W</mi>
								<mi>new</mi>
							  </msub>
							  <mo>=</mo>
							  <msub>
								<mi>W</mi>
								<mi>old</mi>
							  </msub>
							  <mo>-</mo>
							  <mi>&eta;</mi>
							  <mo>&#x2207;</mo> <!-- Gradient symbol (∇) -->
							  <mi>L</mi>
							  <mo>(</mo>
							  <mi>W</mi>
							  <mo>)</mo>
							</math>
						</p>
						<ul>
							<li>✅ \( L(W) \) → Loss function</li>
							<li>✅ \( W \) → Model parameters (weights, biases)</li>
							<li>✅ \( \eta \) → Learning rate (step size for updates)</li>
							<li>✅ \( \nabla L(W) \) → Gradient of loss function with respect to \( W \)</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>2️⃣ Python Implementation</h3>
					<pre class="code-block">
		import numpy as np

		# Define a simple loss function (Mean Squared Error)
		def loss_function(W, X, y):
			y_pred = np.dot(X, W)
			return np.mean((y - y_pred)  2)

		# Compute gradient
		def compute_gradient(W, X, y):
			y_pred = np.dot(X, W)
			return -2 * np.dot(X.T, (y - y_pred)) / len(y)

		# Gradient Descent Algorithm
		def gradient_descent(X, y, lr=0.01, epochs=1000):
			W = np.random.rand(X.shape[1], 1)  # Initialize weights randomly
			for epoch in range(epochs):
				grad = compute_gradient(W, X, y)
				W -= lr * grad  # Update weights
			return W

		# Generate synthetic data
		np.random.seed(42)
		X = np.random.rand(100, 2)
		y = 3 * X[:, 0] + 5 * X[:, 1] + np.random.randn(100) * 0.1  # True relation

		# Train using Gradient Descent
		W_final = gradient_descent(X, y.reshape(-1, 1))
		print("Final Weights:", W_final)
						</pre>
					</div>

					<div class="content-box">
						<h3>3️⃣ Advanced Concepts & Research Extensions</h3>
						<ul>
							<li>✅ <strong>Momentum-based Gradient Descent:</strong> Uses past gradients to accelerate updates.</li>
							<li>✅ <strong>Adaptive Learning Rate Methods:</strong> Adam, RMSprop, and AdaGrad dynamically adjust the learning rate.</li>
							<li>✅ <strong>Stochastic Gradient Descent (SGD):</strong> Uses mini-batches instead of full dataset updates.</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>4️⃣ Pros & Cons</h3>
						<table class="styled-table">
							<tr><th>Pros ✅</th><th>Cons ❌</th></tr>
							<tr><td>✅ Simple and effective optimization method</td><td>❌ Can get stuck in local minima</td></tr>
							<tr><td>✅ Works well for differentiable functions</td><td>❌ Choosing the right learning rate is difficult</td></tr>
						</table>
					</div>

					<div class="content-box">
						<h3>5️⃣ Real-World Applications</h3>
						<ul>
							<li>✅ <strong>Neural Network Training 🤖</strong> – Used in deep learning models.</li>
							<li>✅ <strong>Linear & Logistic Regression 📊</strong> – Optimization for regression models.</li>
							<li>✅ <strong>Recommender Systems 🎥</strong> – Fine-tuning collaborative filtering algorithms.</li>
						</ul>
					</div>
				</div>
			`,


			'rbf': `
				<div class="ml-section">
					<h2> Radial Basis Function (RBF) Network</h2>

					<div class="content-box">
						<h3>1️⃣ Introduction</h3>
						<p><strong>Definition:</strong> A Radial Basis Function (RBF) Network is a type of artificial neural network that uses radial basis functions as activation functions. It is particularly useful for function approximation and pattern recognition.</p>
						<p><strong>Mathematical Formula:</strong></p>
						<p>
							<math>
							  <mi>y</mi>
							  <mo>(</mo>
							  <mi>x</mi>
							  <mo>)</mo>
							  <mo>=</mo>
							  <munderover>
								<mo>&#8721;</mo> <!-- Summation symbol -->
								<mrow>
								  <mi>i</mi>
								  <mo>=</mo>
								  <mn>1</mn>
								</mrow>
								<mi>N</mi>
							  </munderover>
							  <msub>
								<mi>w</mi>
								<mi>i</mi>
							  </msub>
							  <mo>&#x22C5;</mo> <!-- Multiplication dot -->
							  <mi>&phi;</mi>
							  <mo>(</mo>
							  <mo>&#x2225;</mo> <!-- Double vertical bars (norm) -->
							  <mi>x</mi>
							  <mo>-</mo>
							  <msub>
								<mi>c</mi>
								<mi>i</mi>
							  </msub>
							  <mo>&#x2225;</mo>
							  <mo>)</mo>
							</math>
						</p>
						<ul>
							<li>✅ \( \phi(||x - c_i||) \) → Radial basis function (commonly Gaussian).</li>
							<li>✅ \( w_i \) → Weights associated with each basis function.</li>
							<li>✅ \( c_i \) → Centers of the basis functions.</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>2️⃣ Python Implementation</h3>
						<pre class="code-block">
			import numpy as np
			from sklearn.cluster import KMeans
			from scipy.spatial.distance import cdist
			from sklearn.metrics.pairwise import rbf_kernel
			from sklearn.linear_model import Ridge

			# Generate synthetic data
			np.random.seed(42)
			X = np.linspace(-1, 1, 100).reshape(-1, 1)
			y = np.sin(3 * X) + np.random.randn(100, 1) * 0.1  # True function with noise

			#  Select RBF centers using K-Means clustering
			kmeans = KMeans(n_clusters=10, random_state=42).fit(X)
			centers = kmeans.cluster_centers_

			#  Compute RBF Kernel
			gamma = 1.0  # Defines the spread of RBF function
			X_rbf = rbf_kernel(X, centers, gamma=gamma)

			#  Train a Linear Model on RBF-transformed data
			model = Ridge(alpha=0.1)
			model.fit(X_rbf, y)

			#  Make Predictions
			y_pred = model.predict(X_rbf)

			#  Display Results
			print("Final Model Weights:", model.coef_)
						</pre>
					</div>

					<div class="content-box">
						<h3>3️⃣ Advanced Concepts & Research Extensions</h3>
						<ul>
							<li>✅ <strong>Hybrid RBF Networks:</strong> Combining RBF with deep learning architectures.</li>
							<li>✅ <strong>Adaptive RBF Centers:</strong> Dynamically selecting basis function centers.</li>
							<li>✅ <strong>Support Vector Regression (SVR) with RBF:</strong> Using RBF kernels in SVR for high-dimensional regression.</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>4️⃣ Pros & Cons</h3>
						<table class="styled-table">
							<tr><th>Pros ✅</th><th>Cons ❌</th></tr>
							<tr><td>✅ Excellent for function approximation</td><td>❌ Sensitive to the number of basis functions</td></tr>
							<tr><td>✅ Works well with non-linear data</td><td>❌ Computationally expensive with many centers</td></tr>
						</table>
					</div>

					<div class="content-box">
						<h3>5️⃣ Real-World Applications</h3>
						<ul>
							<li>✅ <strong>Handwriting Recognition ✍️</strong> – Used in OCR systems.</li>
							<li>✅ <strong>Medical Diagnosis 🏥</strong> – Pattern recognition in medical imaging.</li>
							<li>✅ <strong>Time-Series Forecasting 📊</strong> – Modeling non-linear trends in financial data.</li>
						</ul>
					</div>
				</div>
			`,

			
			'autoregressive_ann': `
				<div class="ml-section">
					<h2> Autoregressive Time Series Using ANN</h2>

					<div class="content-box">
						<h3>1️⃣ Introduction</h3>
						<p><strong>Definition:</strong> Autoregressive (AR) time series modeling using Artificial Neural Networks (ANN) is a technique where past values of a time series are used to predict future values.</p>
						<p><strong>Mathematical Formula:</strong></p>
						<p>
							<math>
							  <msub>
								<mi>y</mi>
								<mi>t</mi>
							  </msub>
							  <mo>=</mo>
							  <mi>f</mi>
							  <mo>(</mo>
							  <msub>
								<mi>y</mi>
								<mrow>
								  <mi>t</mi>
								  <mo>-</mo>
								  <mn>1</mn>
								</mrow>
							  </msub>
							  <mo>,</mo>
							  <msub>
								<mi>y</mi>
								<mrow>
								  <mi>t</mi>
								  <mo>-</mo>
								  <mn>2</mn>
								</mrow>
							  </msub>
							  <mo>,</mo>
							  <mo>...</mo>
							  <mo>,</mo>
							  <msub>
								<mi>y</mi>
								<mrow>
								  <mi>t</mi>
								  <mo>-</mo>
								  <mi>p</mi>
								</mrow>
							  </msub>
							  <mo>)</mo>
							</math>
						</p>
						<ul>
							<li>✅ \( y_t \) : Value at time \( t \).</li>
							<li>✅ \( f \) : Neural network function.</li>
							<li>✅ \( p \) : Number of lag observations.</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>2️⃣ Python Implementation</h3>
						<pre class="code-block">
			import numpy as np
			import tensorflow as tf
			from tensorflow.keras.models import Sequential
			from tensorflow.keras.layers import Dense, LSTM
			from sklearn.preprocessing import MinMaxScaler

			# Generate synthetic time-series data
			np.random.seed(42)
			time = np.arange(0, 100, 0.1)
			y = np.sin(time) + np.random.randn(len(time)) * 0.1  # Noisy sine wave

			# Prepare Data for ANN
			def create_sequences(data, lookback=5):
				X, y = [], []
				for i in range(len(data) - lookback):
					X.append(data[i:i+lookback])
					y.append(data[i+lookback])
				return np.array(X), np.array(y)

			lookback = 10
			X, y_data = create_sequences(y, lookback)

			# Normalize Data
			scaler = MinMaxScaler()
			X = scaler.fit_transform(X)
			y_data = scaler.fit_transform(y_data.reshape(-1, 1))

			#  Split Data into Training & Test
			train_size = int(len(X) * 0.8)
			X_train, X_test = X[:train_size], X[train_size:]
			y_train, y_test = y_data[:train_size], y_data[train_size:]

			# Define ANN Model for Time Series Forecasting
			model = Sequential([
				Dense(16, activation='relu', input_shape=(lookback,)),
				Dense(8, activation='relu'),
				Dense(1)  # Output layer for forecasting
			])

			# Compile & Train Model
			model.compile(optimizer='adam', loss='mse')
			model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))

			# Evaluate Model
			loss = model.evaluate(X_test, y_test)
			print(f"Test Loss: {loss:.4f}")
						</pre>
					</div>
					<div class="content-box">
						<h3>3️⃣ Advanced Concepts & Research Extensions</h3>
						<ul>
							<li>✅ <strong>Recurrent Neural Networks (RNNs):</strong> Using RNNs like LSTM for sequential forecasting.</li>
							<li>✅ <strong>Attention Mechanism:</strong> Enhancing ANN models for better long-term forecasting.</li>
							<li>✅ <strong>Hybrid Models:</strong> Combining AR models with deep learning architectures.</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>4️⃣ Pros & Cons</h3>
						<table class="styled-table">
							<tr><th>Pros ✅</th><th>Cons ❌</th></tr>
							<tr><td>✅ Captures non-linear dependencies</td><td>❌ Needs large amounts of data for accuracy</td></tr>
							<tr><td>✅ Works well with complex time-series data</td><td>❌ Computationally expensive for real-time predictions</td></tr>
						</table>
					</div>

					<div class="content-box">
						<h3>5️⃣ Real-World Applications</h3>
						<ul>
							<li>✅ <strong>Stock Market Prediction 📈</strong> – Predicting future stock prices.</li>
							<li>✅ <strong>Weather Forecasting 🌦️</strong> – Forecasting temperature trends.</li>
							<li>✅ <strong>Energy Consumption Forecasting ⚡</strong> – Predicting electricity demand.</li>
						</ul>
					</div>
				</div>
			`,

			
			'reward_signal': `
				<div class="ml-section">
					<h2> Reward Signal in Reinforcement Learning</h2>

					<div class="content-box">
						<h3>1️⃣ Introduction</h3>
						<p><strong>Definition:</strong> A reward signal in reinforcement learning is a numerical value given to an agent as feedback for its actions. The goal of the agent is to maximize the cumulative reward over time.</p>
						<p><strong>Mathematical Formula:</strong></p>
						<p>
							<math>
							  <msub>
								<mi>R</mi>
								<mi>t</mi>
							  </msub>
							  <mo>=</mo>
							  <munderover>
								<mo>&#8721;</mo> <!-- Summation symbol -->
								<mrow>
								  <mi>k</mi>
								  <mo>=</mo>
								  <mn>0</mn>
								</mrow>
								<mo>&#8734;</mo> <!-- Infinity symbol -->
							  </munderover>
							  <msup>
								<mi>&gamma;</mi>
								<mi>k</mi>
							  </msup>
							  <msub>
								<mi>r</mi>
								<mrow>
								  <mi>t</mi>
								  <mo>+</mo>
								  <mi>k</mi>
								</mrow>
							  </msub>
							</math>
						</p>
						<ul>
							<li>✅ \( R_t \) : Total reward at time \( t \).</li>
							<li>✅ \( \gamma \) : Discount factor (between 0 and 1).</li>
							<li>✅ \( r_t \) : Instantaneous reward at time \( t \).</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>2️⃣ Python Implementation</h3>
						<pre class="code-block">
			import numpy as np

			# Define a simple reward function
			def reward_function(action):
				rewards = {0: -1, 1: 1, 2: 5}  # Action 2 gives the highest reward
				return rewards.get(action, 0)

			# Simulate an agent taking actions
			actions = [0, 1, 2, 2, 0, 1, 2]
			total_reward = 0
			gamma = 0.9  # Discount factor

			# Compute total discounted reward
			for t, action in enumerate(actions):
				total_reward += (gamma  t) * reward_function(action)

			print(f"Total Discounted Reward: {total_reward:.2f}")
						</pre>
					</div>

					<div class="content-box">
						<h3>3️⃣ Advanced Concepts & Research Extensions</h3>
						<ul>
							<li>✅ <strong>Shaped Rewards:</strong> Designing more informative rewards to speed up learning.</li>
							<li>✅ <strong>Sparse Rewards:</strong> Handling cases where rewards are infrequent (e.g., games like chess).</li>
							<li>✅ <strong>Multi-Objective Rewards:</strong> Optimizing for multiple reward functions simultaneously.</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>4️⃣ Pros & Cons</h3>
						<table class="styled-table">
							<tr><th>Pros ✅</th><th>Cons ❌</th></tr>
							<tr><td>✅ Direct feedback for learning optimal policies</td><td>❌ Sparse rewards make learning difficult</td></tr>
							<tr><td>✅ Can encode complex goals in reinforcement learning</td><td>❌ Reward hacking (agent exploits poorly designed rewards)</td></tr>
						</table>
					</div>

					<div class="content-box">
						<h3>5️⃣ Real-World Applications</h3>
						<ul>
							<li>✅ <strong>Game AI 🎮</strong> – Used in Deep Q-Learning for playing Atari games.</li>
							<li>✅ <strong>Robotics 🤖</strong> – Training robots to perform tasks efficiently.</li>
							<li>✅ <strong>Self-Driving Cars 🚗</strong> – Reward functions help vehicles make optimal driving decisions.</li>
						</ul>
					</div>
				</div>
			`,

			'action_policy': `
				<div class="ml-section">
					<h2> Action Policy in Reinforcement Learning</h2>

					<div class="content-box">
						<h3>1️⃣ Introduction</h3>
						<p><strong>Definition:</strong> An action policy in reinforcement learning defines how an agent selects actions based on its current state. Policies can be deterministic or stochastic.</p>
						<p><strong>Mathematical Representation:</strong></p>
						<p>
							<math>
							  <mi>&pi;</mi>
							  <mo>(</mo>
							  <mi>a</mi>
							  <mo>|</mo>
							  <mi>s</mi>
							  <mo>)</mo>
							  <mo>=</mo>
							  <mi>P</mi>
							  <mo>(</mo>
							  <msub>
								<mi>A</mi>
								<mi>t</mi>
							  </msub>
							  <mo>=</mo>
							  <mi>a</mi>
							  <mo>|</mo>
							  <msub>
								<mi>S</mi>
								<mi>t</mi>
							  </msub>
							  <mo>=</mo>
							  <mi>s</mi>
							  <mo>)</mo>
							</math>
						</p>
						<ul>
							<li>✅ \( \pi(a | s) \) → Probability of taking action \( a \) in state \( s \).</li>
							<li>✅ \( A_t \) → Action chosen at time \( t \).</li>
							<li>✅ \( S_t \) → Current state at time \( t \).</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>2️⃣ Python Implementation</h3>
						<pre class="code-block">
			import numpy as np

			# Define a simple stochastic policy (Softmax)
			def softmax_policy(Q_values, temperature=1.0):
				exp_values = np.exp(Q_values / temperature)
				return exp_values / np.sum(exp_values)

			# Q-values for three actions
			Q_values = np.array([1.2, 2.5, 0.8])

			# Compute action probabilities
			action_probabilities = softmax_policy(Q_values)
			print("Action Probabilities:", action_probabilities)

			# Choose action based on probability
			action = np.random.choice(len(Q_values), p=action_probabilities)
			print("Selected Action:", action)
						</pre>
					</div>

					<div class="content-box">
						<h3>3️⃣ Advanced Concepts & Research Extensions</h3>
						<ul>
							<li>✅ <strong>Exploration vs Exploitation:</strong> Balancing between trying new actions (exploration) and using known good actions (exploitation).</li>
							<li>✅ <strong>ε-Greedy Policy:</strong> Selects the best action with probability \( 1 - \epsilon \), but explores randomly with probability \( \epsilon \).</li>
							<li>✅ <strong>Policy Gradient Methods:</strong> Learning policies directly using deep learning (e.g., REINFORCE algorithm).</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>4️⃣ Pros & Cons</h3>
						<table class="styled-table">
							<tr><th>Pros ✅</th><th>Cons ❌</th></tr>
							<tr><td>✅ Enables decision-making in complex environments</td><td>❌ Can get stuck in suboptimal policies</td></tr>
							<tr><td>✅ Works well in deep reinforcement learning</td><td>❌ Balancing exploration and exploitation is challenging</td></tr>
						</table>
					</div>

					<div class="content-box">
						<h3>5️⃣ Real-World Applications</h3>
						<ul>
							<li>✅ <strong>Game AI 🎮</strong> – Policies help AI agents make optimal moves in games like chess and Go.</li>
							<li>✅ <strong>Autonomous Vehicles 🚗</strong> – Policies dictate how self-driving cars navigate roads.</li>
							<li>✅ <strong>Healthcare AI 🏥</strong> – Used in treatment recommendation systems.</li>
						</ul>
					</div>
				</div>
			`,


			'mdp': `
				<div class="ml-section">
					<h2> Markov Decision Process (MDP)</h2>

					<div class="content-box">
						<h3>1️⃣ Introduction</h3>
						<p><strong>Definition:</strong> A Markov Decision Process (MDP) is a mathematical framework used to model decision-making problems where outcomes are partly random and partly under the control of an agent.</p>
						<p><strong>Mathematical Representation:</strong></p>
						<p>
							<math>
							  <mi>MDP</mi>
							  <mo>=</mo>
							  <mo>(</mo>
							  <mi>S</mi>
							  <mo>,</mo>
							  <mi>A</mi>
							  <mo>,</mo>
							  <mi>P</mi>
							  <mo>,</mo>
							  <mi>R</mi>
							  <mo>,</mo>
							  <mi>&gamma;</mi>
							  <mo>)</mo>
							</math>
						</p>
						<ul>
							<li>✅ \( S \) → Set of states.</li>
							<li>✅ \( A \) → Set of actions.</li>
							<li>✅ \( P(s' | s, a) \) → Transition probability of reaching state \( s' \) from state \( s \) after action \( a \).</li>
							<li>✅ \( R(s, a) \) → Reward function.</li>
							<li>✅ \( \gamma \) → Discount factor (0 ≤ γ ≤ 1).</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>2️⃣ Python Implementation</h3>
						<pre class="code-block">
import numpy as np

# Define MDP Components
states = ["S1", "S2", "S3"]
actions = ["A1", "A2"]

# Transition Probability Matrix (P[s, a, s'])
P = {
	"S1": {"A1": {"S1": 0.2, "S2": 0.8}, "A2": {"S1": 0.5, "S3": 0.5}},
	"S2": {"A1": {"S1": 0.1, "S3": 0.9}, "A2": {"S2": 1.0}},
	"S3": {"A1": {"S3": 1.0}, "A2": {"S1": 0.3, "S2": 0.7}},
}

# Reward Function
R = {"S1": {"A1": 5, "A2": 2}, "S2": {"A1": 3, "A2": -1}, "S3": {"A1": 0, "A2": 4}}

# Discount Factor
gamma = 0.9

# Value Iteration Algorithm
V = {s: 0 for s in states}  # Initialize values

for _ in range(100):  # Iterate until convergence
	V_new = V.copy()
	for s in states:
		V_new[s] = max(sum(P[s][a][s_prime] * (R[s][a] + gamma * V[s_prime]) for s_prime in P[s][a]) for a in actions)
	V = V_new

print("Optimal Value Function:", V)
						</pre>
					</div>

					<div class="content-box">
						<h3>3️⃣ Advanced Concepts & Research Extensions</h3>
						<ul>
							<li>✅ <strong>Partially Observable MDPs (POMDPs):</strong> When states are not fully observable.</li>
							<li>✅ <strong>Deep MDPs:</strong> Using deep learning models to approximate MDP solutions.</li>
							<li>✅ <strong>Multi-Agent MDPs:</strong> Extending MDPs for multi-agent environments.</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>4️⃣ Pros & Cons</h3>
						<table class="styled-table">
							<tr><th>Pros ✅</th><th>Cons ❌</th></tr>
							<tr><td>✅ Provides a formal framework for decision-making</td><td>❌ Requires accurate transition probabilities</td></tr>
							<tr><td>✅ Used in AI planning and robotics</td><td>❌ Computationally expensive for large state spaces</td></tr>
						</table>
					</div>

					<div class="content-box">
						<h3>5️⃣ Real-World Applications</h3>
						<ul>
							<li>✅ <strong>Robotics 🤖</strong> – MDPs help in robot motion planning.</li>
							<li>✅ <strong>Healthcare 🏥</strong> – Optimizing treatment strategies for patients.</li>
							<li>✅ <strong>Finance 📈</strong> – Modeling stock market decision processes.</li>
						</ul>
					</div>
				</div>
			`,

			
			'q_learning': `
				<div class="ml-section">
					<h2> Q-Learning (Reinforcement Learning)</h2>

					<div class="content-box">
						<h3>1️⃣ Introduction</h3>
						<p><strong>Definition:</strong> Q-Learning is an off-policy reinforcement learning algorithm that learns an optimal action-selection policy for an agent by iteratively updating Q-values.</p>
						<p><strong>Mathematical Representation:</strong></p>
						<p>
							<math>
							  <msub>
								<mi>Q</mi>
								<mrow>
								  <mi>s</mi>
								  <mo>,</mo>
								  <mi>a</mi>
								</mrow>
							  </msub>
							  <mo>=</mo>
							  <msub>
								<mi>Q</mi>
								<mrow>
								  <mi>s</mi>
								  <mo>,</mo>
								  <mi>a</mi>
								</mrow>
							  </msub>
							  <mo>+</mo>
							  <mi>&alpha;</mi>
							  <mo>[</mo>
							  <mi>r</mi>
							  <mo>+</mo>
							  <mi>&gamma;</mi>
							  <mo>&InvisibleTimes;</mo>
							  <munder>
								<mo>max</mo>
								<mrow>
								  <mi>a</mi><mo>'</mo>
								</mrow>
							  </munder>
							  <msub>
								<mi>Q</mi>
								<mrow>
								  <msup>
									<mi>s</mi>
									<mo>'</mo>
								  </msup>
								  <mo>,</mo>
								  <msup>
									<mi>a</mi>
									<mo>'</mo>
								  </msup>
								</mrow>
							  </msub>
							  <mo>-</mo>
							  <msub>
								<mi>Q</mi>
								<mrow>
								  <mi>s</mi>
								  <mo>,</mo>
								  <mi>a</mi>
								</mrow>
							  </msub>
							  <mo>]</mo>
							</math>
						</p>
						<ul>
							<li>✅ \( Q(s, a) \) : Q-value for taking action \( a \) in state \( s \).</li>
							<li>✅ \( \alpha \) : Learning rate.</li>
							<li>✅ \( \gamma \) : Discount factor (0 ≤ γ ≤ 1).</li>
							<li>✅ \( r \) : Reward received after taking action \( a \).</li>
							<li>✅ \( s' \) : Next state after action \( a \).</li>
							<li>✅ \( \max_{a'} Q(s', a') \) : Best Q-value for the next state.</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>2️⃣ Python Implementation</h3>
						<pre class="code-block">
import numpy as np

# Initialize Environment
states = ["S1", "S2", "S3", "S4"]
actions = ["A1", "A2"]
Q = np.zeros((len(states), len(actions)))  # Initialize Q-table

# Define Reward Table
rewards = np.array([
	[0, 10],  # S1: Reward for A1 and A2
	[-10, 20],  # S2
	[5, 15],  # S3
	[0, 0]  # S4 (Terminal State)
])

alpha = 0.1  # Learning rate
gamma = 0.9  # Discount factor
episodes = 1000  # Number of training episodes

# Q-Learning Algorithm
for episode in range(episodes):
	state = np.random.choice(len(states))  # Start at a random state
	while state != 3:  # Loop until reaching terminal state
		action = np.random.choice(len(actions))  # Choose random action
		next_state = np.random.choice(len(states))  # Move to next state
		reward = rewards[state, action]
		
		# Q-value update formula
		Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])
		state = next_state  # Move to next state

print("Final Q-Table:")
print(Q)
						</pre>
					</div>

					<div class="content-box">
						<h3>3️⃣ Advanced Concepts & Research Extensions</h3>
						<ul>
							<li>✅ <strong>Deep Q-Networks (DQN):</strong> Using deep learning instead of Q-tables.</li>
							<li>✅ <strong>Double Q-Learning:</strong> Reduces overestimation bias in Q-learning.</li>
							<li>✅ <strong>Multi-Agent Q-Learning:</strong> Extends Q-learning for multi-agent environments.</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>4️⃣ Pros & Cons</h3>
						<table class="styled-table">
							<tr><th>Pros ✅</th><th>Cons ❌</th></tr>
							<tr><td>✅ Simple and effective for small environments</td><td>❌ Inefficient for large state spaces</td></tr>
							<tr><td>✅ Can handle stochastic environments</td><td>❌ Requires extensive exploration</td></tr>
						</table>
					</div>

					<div class="content-box">
						<h3>5️⃣ Real-World Applications</h3>
						<ul>
							<li>✅ <strong>Game AI 🎮</strong> – Training agents to play games like Atari & Chess.</li>
							<li>✅ <strong>Robotics 🤖</strong> – Reinforcement learning in robotic control systems.</li>
							<li>✅ <strong>Finance & Trading 📈</strong> – Q-learning applied in algorithmic trading.</li>
						</ul>
					</div>
				</div>
			`,

			'sas_viya_intro': `
				<div class="ml-section">
					<h2> Introduction to SAS Viya</h2>

					<div class="content-box">
						<h3>1️⃣ What is SAS Viya?</h3>
						<p><strong>Definition:</strong> SAS Viya is a cloud-enabled, scalable, and high-performance analytics platform designed for data science, machine learning, and artificial intelligence.</p>
						<p>It supports a variety of programming languages, including SAS, Python, R, and REST APIs, making it a flexible tool for big data processing and model deployment.</p>
					</div>

					<div class="content-box">
						<h3>2️⃣ Why Use SAS Viya for Machine Learning?</h3>
						<ul>
							<li>✅ <strong>High-Performance Computing:</strong> Supports in-memory distributed processing.</li>
							<li>✅ <strong>Automated Machine Learning (AutoML):</strong> Quickly builds models with optimized hyperparameters.</li>
							<li>✅ <strong>Seamless Integration:</strong> Works with Python, R, and cloud-based services.</li>
							<li>✅ <strong>Interactive Interface:</strong> Provides both GUI-based (SAS Studio) and code-based solutions.</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>3️⃣ Overview of Practical Exercises</h3>
						<p>To help readers gain hands-on experience, we provide 14 practical exercises that demonstrate how to use SAS Viya for machine learning tasks.</p>
						<p>Each practical focuses on different aspects of data handling, preprocessing, modeling, and evaluation in SAS Viya.</p>
					</div>

				</div>
			`,


			'python_libraries': `
				<div class="ml-section">
					<h2> Introduction to Python for Machine Learning</h2>

					<div class="content-box">
						<h3>1️⃣ Why Use Python for Machine Learning?</h3>
						<p><strong>Definition:</strong> Python is the most widely used programming language for machine learning due to its simplicity, extensive libraries, and strong community support.</p>
						<p>Python provides powerful libraries such as NumPy, Pandas, Scikit-Learn, TensorFlow, and PyTorch, making it a go-to language for data processing, model building, and deep learning applications.</p>
					</div>

					<div class="content-box">
						<h3>2️⃣ Key Features of Python for ML</h3>
						<ul>
							<li>✅ <strong>Easy-to-Use Syntax:</strong> Allows quick prototyping and experimentation.</li>
							<li>✅ <strong>Rich Ecosystem:</strong> Comes with numerous ML/DL libraries (Scikit-Learn, TensorFlow, PyTorch, etc.).</li>
							<li>✅ <strong>Visualization Capabilities:</strong> Matplotlib and Seaborn for detailed data analysis.</li>
							<li>✅ <strong>Scalability & Performance:</strong> Compatible with cloud and distributed computing platforms.</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>3️⃣ Overview of Practical Exercises</h3>
						<p>To help readers gain hands-on experience, we provide 14 practical exercises demonstrating Python's capabilities in machine learning.</p>
						<p>Each practical covers different aspects of data preprocessing, modeling, optimization, and evaluation in Python.</p>
					</div>

				</div>
			`,

			'viyaP1': `
				<h1 class="blue-bolt"> Linear Regression – Predicting Diabetes Progression</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> SASHELP.DIABETES</p>
					<p><strong>Problem Statement:</strong> Predict disease progression based on patient health metrics using <strong>Linear Regression</strong> in SAS Viya.</p>
					<ul>
						<li>Train a <strong>Linear Regression model</strong> using <code>PROC REG</code>.</li>
						<li>Evaluate the model using <strong>MSE</strong> and <strong>R² score</strong>.</li>
						<li>Compare model performance when using standardized vs. raw data.</li>
					</ul>
				</div>
			`,

			'viyaP2': `
				<h1 class="blue-bolt"> Logistic Regression – Classifying Breast Cancer Tumors</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> SASHELP.BCANCER</p>
					<p><strong>Problem Statement:</strong> Classify tumors as <strong>benign</strong> or <strong>malignant</strong> using <strong>Logistic Regression</strong> in SAS Viya.</p>
					<ul>
						<li>✅ Train a <strong>Logistic Regression model</strong> using <code>PROC LOGISTIC</code>.</li>
						<li>✅ Analyze the <strong>confusion matrix</strong>, precision, and recall.</li>
						<li>✅ Compare logistic regression with <strong>SVM</strong>.</li>
					</ul>
				</div>
			`,

			'viyaP3': `
				<h1 class="blue-bolt"> Decision Trees – Classifying Iris Flower Species</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> SASHELP.IRIS</p>
					<p><strong>Problem Statement:</strong> Classify <strong>Iris flower species</strong> using a <strong>Decision Tree Classifier</strong> in SAS Viya.</p>
					<ul>
						<li>✅ Train a <strong>Decision Tree</strong> using <code>PROC HPSPLIT</code>.</li>
						<li>✅ Tune <strong>max_depth</strong> and <strong>min_samples_split</strong> to prevent overfitting.</li>
						<li>✅ Visualize the tree structure.</li>
					</ul>
				</div>
			`,

			'viyaP4': `
				<h1 class="blue-bolt"> Random Forest – Wine Quality Prediction</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> SASHELP.WINE</p>
					<p><strong>Problem Statement:</strong> Classify wines into <strong>three quality categories</strong> using <strong>Random Forest</strong> in SAS Viya.</p>
					<ul>
						<li>✅ Train a <strong>Random Forest Classifier</strong> using <code>PROC HPFOREST</code>.</li>
						<li>✅ Compare performance with a <strong>Decision Tree</strong>.</li>
						<li>✅ Identify important features in classification.</li>
					</ul>
				</div>
			`,

			'viyaP5': `
				<h1 class="blue-bolt"> Support Vector Machines (SVM) – Handwritten Digit Recognition</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> SASHELP.DIGITS</p>
					<p><strong>Problem Statement:</strong> Train an <strong>SVM classifier</strong> to recognize handwritten digits using SAS Viya.</p>
					<ul>
						<li>✅ Train an <strong>SVM model</strong> using <code>PROC SVM</code>.</li>
						<li>✅ Experiment with different <strong>kernel types</strong> (linear, RBF, polynomial).</li>
						<li>✅ Compare results with <strong>Decision Trees</strong>.</li>
					</ul>
				</div>
			`,

			'viyaP6': `
				<h1 class="blue-bolt"> K-Means Clustering – Clustering Handwritten Digits</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> SASHELP.DIGITS</p>
					<p><strong>Problem Statement:</strong> Cluster handwritten digit images into 10 groups using <strong>K-Means Clustering</strong> in SAS Viya.</p>
					<ul>
						<li>✅ Train a <strong>K-Means model</strong> using <code>PROC FASTCLUS</code>.</li>
						<li>✅ Evaluate cluster quality using <strong>Adjusted Rand Index (ARI)</strong>.</li>
						<li>✅ Visualize the clustering results.</li>
					</ul>
				</div>
			`,

			'viyaP13': `
				<h1 class="blue-bolt"> Generative Adversarial Networks (GANs) – Image Generation</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> SASHELP.MNIST</p>
					<p><strong>Problem Statement:</strong> Train a <strong>Generative Adversarial Network (GAN)</strong> in SAS Viya to generate realistic handwritten digits.</p>
					<ul>
						<li>✅ Implement a <strong>basic GAN architecture</strong> with a <strong>generator</strong> and <strong>discriminator</strong>.</li>
						<li>✅ Train the model using <code>PROC DNN</code> in SAS Viya.</li>
						<li>✅ Visualize generated digits and analyze model convergence.</li>
					</ul>
				</div>
			`,

			'viyaP7': `
				<h1 class="blue-bolt"> Principal Component Analysis (PCA) – Dimensionality Reduction</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> SASHELP.DIGITS</p>
					<p><strong>Problem Statement:</strong> Reduce the dimensionality of the <strong>Digits dataset</strong> using <strong>PCA</strong> in SAS Viya.</p>
					<ul>
						<li>✅ Perform <strong>PCA</strong> using <code>PROC PRINCOMP</code>.</li>
						<li>✅ Determine the minimum number of components needed to retain <strong>95% variance</strong>.</li>
						<li>✅ Visualize the reduced dataset in <strong>2D scatter plots</strong>.</li>
					</ul>
				</div>
			`,

			'viyaP8': `
				<h1 class="blue-bolt"> XGBoost – Predicting Wine Quality</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> SASHELP.WINE</p>
					<p><strong>Problem Statement:</strong> Use <strong>XGBoost</strong> in SAS Viya to classify wine quality.</p>
					<ul>
						<li>✅ Train a <strong>Gradient Boosting model</strong> using <code>PROC GRADBOOST</code>.</li>
						<li>✅ Compare <strong>XGBoost</strong> with <strong>Random Forest</strong> and <strong>Decision Trees</strong>.</li>
						<li>✅ Perform <strong>hyperparameter tuning</strong> for better accuracy.</li>
					</ul>
				</div>
			`,

			'viyaP9': `
				<h1 class="blue-bolt"> Artificial Neural Networks (ANN) – Fashion Item Classification</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> SASHELP.FASHION</p>
					<p><strong>Problem Statement:</strong> Train an <strong>Artificial Neural Network (ANN)</strong> in SAS Viya.</p>
					<ul>
						<li>✅ Train an ANN model using <code>PROC NNET</code>.</li>
						<li>✅ Tune hyperparameters such as <strong>number of hidden layers</strong> and <strong>neurons</strong>.</li>
						<li>✅ Compare with a simple <strong>Logistic Regression model</strong>.</li>
					</ul>
				</div>
			`,

			'viyaP10': `
				<h1 class="blue-bolt"> Convolutional Neural Networks (CNN) – CIFAR-10 Image Classification</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> SASHELP.CIFAR10</p>
					<p><strong>Problem Statement:</strong> Train a <strong>Convolutional Neural Network (CNN)</strong> in SAS Viya to classify images.</p>
					<ul>
						<li>✅ Train a <strong>CNN model</strong> using <code>PROC DNN</code>.</li>
						<li>✅ Experiment with architectures like <strong>ResNet</strong> and <strong>VGG</strong>.</li>
						<li>✅ Apply <strong>data augmentation</strong> to improve generalization.</li>
					</ul>
				</div>
			`,

			'viyaP11': `
				<h1 class="blue-bolt"> Recurrent Neural Networks (RNN) – Sentiment Analysis</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> SASHELP.IMDB</p>
					<p><strong>Problem Statement:</strong> Train an <strong>RNN model</strong> in SAS Viya for sentiment analysis.</p>
					<ul>
						<li>✅ Train an <strong>LSTM-based RNN</strong> using <code>PROC DNN</code>.</li>
						<li>✅ Use <strong>word embeddings</strong> for better text representation.</li>
						<li>✅ Compare RNN with <strong>Logistic Regression</strong>.</li>
					</ul>
				</div>
			`,

			'viyaP12': `
				<h1 class="blue-bolt"> Transformer Model (BERT) – Fake News Detection</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> SASHELP.FAKENEWS</p>
					<p><strong>Problem Statement:</strong> Fine-tune a <strong>pre-trained BERT model</strong> in SAS Viya for fake news detection.</p>
					<ul>
						<li>✅ Train a <strong>BERT model</strong> using <code>PROC NLP</code> in SAS Viya.</li>
						<li>✅ Use <strong>transfer learning</strong> for text classification.</li>
						<li>✅ Compare BERT performance with <strong>traditional NLP models</strong>.</li>
					</ul>
				</div>
			`,

			'viyaP13': `
				<h1 class="blue-bolt"> Generative Adversarial Networks (GANs) – Image Generation</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> SASHELP.MNIST</p>
					<p><strong>Problem Statement:</strong> Train a <strong>Generative Adversarial Network (GAN)</strong> in SAS Viya to generate realistic handwritten digits.</p>
					<ul>
						<li>✅ Implement a <strong>basic GAN architecture</strong> with a <strong>generator</strong> and <strong>discriminator</strong>.</li>
						<li>✅ Train the model using <code>PROC DNN</code> in SAS Viya.</li>
						<li>✅ Visualize generated digits and analyze model convergence.</li>
					</ul>
				</div>
			`,

			'viyaP14': `
				<h1 class="blue-bolt"> Reinforcement Learning – Training an AI to Play CartPole</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> OpenAI Gym – CartPole-v1 (Integrated with SAS Viya)</p>
					<p><strong>Problem Statement:</strong> Train a <strong>Deep Q-Network (DQN)</strong> in SAS Viya to balance a pole on a moving cart.</p>
					<ul>
						<li>✅ Integrate SAS Viya with <strong>OpenAI Gym</strong> for Reinforcement Learning.</li>
						<li>✅ Implement a <strong>Deep Q-Network (DQN)</strong> using <code>PROC DNN</code>.</li>
						<li>✅ Optimize agent performance through hyperparameter tuning.</li>
					</ul>
				</div>
			`,


			'pythonP1': `
				<h1 class="blue-bolt"> Linear Regression – Predicting Diabetes Progression</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> sklearn.datasets.load_diabetes()</p>
					<p><strong>Problem Statement:</strong> Predict disease progression based on patient health metrics using Linear Regression.</p>
					<ul>
						<li>✅ Evaluate the model using MSE and R² score.</li>
						<li>✅ Compare model performance when using standardized vs. raw data.</li>
					</ul>
					<h3>📢 Discussion Topic</h3>
					<p><strong>Key Limitation:</strong> Linear regression assumes a linear relationship between features and the target variable, which is often unrealistic for complex medical data.</p>
					<p><strong>Research Inspiration:</strong> How can we integrate deep learning regression models to capture non-linearity? Can we use graph-based regression for medical datasets?</p>
				</div>
			`,

			'pythonP2': `
				<h1 class="blue-bolt"> Logistic Regression – Classifying Breast Cancer Tumors</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> sklearn.datasets.load_breast_cancer()</p>
					<p><strong>Problem Statement:</strong> Classify tumors as benign or malignant using Logistic Regression.</p>
					<ul>
						<li>✅ Analyze the confusion matrix, precision, and recall.</li>
						<li>✅ Compare logistic regression with SVM for classification performance.</li>
					</ul>
					<h3>📢 Discussion Topic</h3>
					<p><strong>Key Limitation:</strong> Logistic regression struggles with imbalanced datasets, which is common in medical diagnosis where malignant cases are rare.</p>
					<p><strong>Research Inspiration:</strong> How can cost-sensitive learning or meta-learning techniques be used to handle class imbalance in medical AI?</p>
				</div>
			`,

			'pythonP3': `
				<h1 class="blue-bolt"> Decision Trees – Classifying Iris Flower Species</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> sklearn.datasets.load_iris()</p>
					<p><strong>Problem Statement:</strong> Classify Iris flower species using a Decision Tree Classifier.</p>
					<ul>
						<li>✅ Visualize the tree using graphviz.</li>
						<li>✅ Tune max_depth and min_samples_split to prevent overfitting.</li>
					</ul>
					<h3>📢 Discussion Topic</h3>
					<p><strong>Key Limitation:</strong> Decision trees tend to overfit, making them unreliable for high-dimensional data.</p>
					<p><strong>Research Inspiration:</strong> Can we develop adaptive tree-based models that dynamically prune based on active learning techniques?</p>
				</div>
			`,

			'pythonP4': `
				<h1 class="blue-bolt"> Random Forest – Wine Quality Prediction</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> sklearn.datasets.load_wine()</p>
					<p><strong>Problem Statement:</strong> Classify wines into three quality categories using Random Forest Classifier.</p>
					<ul>
						<li>✅ Compare performance with a Decision Tree.</li>
						<li>✅ Identify important features in classification.</li>
					</ul>
					<h3>📢 Discussion Topic</h3>
					<p><strong>Key Limitation:</strong> Random forests are computationally expensive and difficult to interpret.</p>
					<p><strong>Research Inspiration:</strong> Can we improve model efficiency using quantum computing or Neuro-Symbolic AI to improve interpretability?</p>
				</div>
			`,

			'pythonP5': `
				<h1 class="blue-bolt"> Support Vector Machines (SVM) – Handwritten Digit Recognition</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> sklearn.datasets.load_digits()</p>
					<p><strong>Problem Statement:</strong> Train an SVM classifier to recognize handwritten digits.</p>
					<ul>
						<li>✅ Experiment with different kernel types (linear, RBF, polynomial).</li>
						<li>✅ Compare results with a simple KNN classifier.</li>
					</ul>
					<h3>📢 Discussion Topic</h3>
					<p><strong>Key Limitation:</strong> SVM scales poorly with large datasets, making it unsuitable for real-time applications.</p>
					<p><strong>Research Inspiration:</strong> Can quantized SVMs or memory-efficient kernel approximations enable SVM to compete with deep learning?</p>
				</div>
			`,

			'pythonP6': `
				<h1 class="blue-bolt"> K-Means Clustering – Clustering Handwritten Digits</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> sklearn.datasets.load_digits()</p>
					<p><strong>Problem Statement:</strong> Cluster handwritten digit images into 10 groups using K-Means Clustering.</p>
					<ul>
						<li>✅ Visualize cluster centroids and compare with actual labels.</li>
						<li>✅ Evaluate cluster purity using Adjusted Rand Index (ARI).</li>
					</ul>
					<h3>📢 Discussion Topic</h3>
					<p><strong>Key Limitation:</strong> K-Means requires manual selection of k, and struggles with non-spherical clusters.</p>
					<p><strong>Research Inspiration:</strong> Can self-supervised learning or autoencoder-assisted clustering improve cluster quality in real-world, high-dimensional data?</p>
				</div>
			`,

			'pythonP7': `
				<h1 class="blue-bolt"> Principal Component Analysis (PCA) – Dimensionality Reduction</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> sklearn.datasets.load_digits()</p>
					<p><strong>Problem Statement:</strong> Reduce the dimensionality of the Digits dataset from 64 to 2 using PCA.</p>
					<ul>
						<li>✅ Determine the minimum number of components needed to retain 95% variance.</li>
						<li>✅ Visualize the reduced dataset in 2D scatter plots.</li>
					</ul>
					<h3>📢 Discussion Topic</h3>
					<p><strong>Key Limitation:</strong> PCA assumes linear relationships, which fails to capture complex non-linear structures.</p>
					<p><strong>Research Inspiration:</strong> How can non-linear alternatives like Kernel PCA, t-SNE, or UMAP improve dimensionality reduction for deep learning?</p>
				</div>
			`,

			'pythonP8': `
				<h1 class="blue-bolt"> XGBoost – Predicting Wine Quality</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> sklearn.datasets.load_wine()</p>
					<p><strong>Problem Statement:</strong> Use XGBoost to classify wines into three categories and optimize hyperparameters.</p>
					<ul>
						<li>✅ Compare XGBoost with Random Forest and Decision Trees.</li>
						<li>✅ Perform hyperparameter tuning using GridSearchCV.</li>
					</ul>
					<h3>📢 Discussion Topic</h3>
					<p><strong>Key Limitation:</strong> XGBoost models are prone to overfitting on small datasets and require fine-tuning.</p>
					<p><strong>Research Inspiration:</strong> Can Neural Boosting or Hybrid DL-XGBoost models help bridge the gap between tree-based models and deep learning?</p>
				</div>
			`,

			'pythonP9': `
				<h1 class="blue-bolt"> Artificial Neural Networks (ANN) – Fashion Item Classification</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> tensorflow.keras.datasets.fashion_mnist</p>
					<p><strong>Problem Statement:</strong> Train an ANN model on Fashion-MNIST for clothing classification.</p>
					<ul>
						<li>✅ Tune the number of hidden layers and neurons.</li>
						<li>✅ Compare with logistic regression.</li>
					</ul>
					<h3>📢 Discussion Topic</h3>
					<p><strong>Key Limitation:</strong> ANN models struggle with vanishing gradients and require large datasets for generalization.</p>
					<p><strong>Research Inspiration:</strong> How can attention mechanisms or self-supervised learning make ANNs more data-efficient?</p>
				</div>
			`,

			'pythonP10': `
				<h1 class="blue-bolt"> Convolutional Neural Networks (CNN) – CIFAR-10 Image Classification</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> tensorflow.keras.datasets.cifar10</p>
					<p><strong>Problem Statement:</strong> Train a CNN to classify images into 10 categories.</p>
					<ul>
						<li>✅ Experiment with different architectures like ResNet and VGG.</li>
						<li>✅ Use data augmentation for better generalization.</li>
					</ul>
					<h3>📢 Discussion Topic</h3>
					<p><strong>Key Limitation:</strong> CNNs require huge labeled datasets and are prone to adversarial attacks.</p>
					<p><strong>Research Inspiration:</strong> Can self-supervised CNNs or transformer-based vision models (ViT) reduce data dependency?</p>
				</div>
			`,

			'pythonP11': `
				<h1 class="blue-bolt"> Recurrent Neural Networks (RNN) – IMDB Sentiment Analysis</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> tensorflow.keras.datasets.imdb</p>
					<p><strong>Problem Statement:</strong> Train an RNN model (LSTM/GRU) to classify IMDB movie reviews as positive or negative.</p>
					<ul>
						<li>✅ Use word embeddings for better text representation.</li>
						<li>✅ Compare RNN with logistic regression.</li>
					</ul>
					<h3>📢 Discussion Topic</h3>
					<p><strong>Key Limitation:</strong> RNNs struggle with long-term dependencies and suffer from vanishing gradients.</p>
					<p><strong>Research Inspiration:</strong> How do transformer-based models (GPT, BERT) overcome these limitations, and what’s next beyond transformers?</p>
				</div>
			`,


			'pythonP12': `
				<h1 class="blue-bolt"> Transformer Model (BERT) – Fake News Detection</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> Hugging Face datasets: "fake_news"</p>
					<p><strong>Problem Statement:</strong> Fine-tune BERT to classify news articles as real or fake.</p>
					<ul>
						<li>✅ Compare BERT with traditional NLP models.</li>
					</ul>
					<h3>📢 Discussion Topic</h3>
					<p><strong>Key Limitation:</strong> BERT is computationally expensive, requiring high-end GPUs for real-time inference.</p>
					<p><strong>Research Inspiration:</strong> How can low-rank approximation and knowledge distillation reduce BERT’s computational footprint while maintaining accuracy?</p>
				</div>
			`,

			'pythonP13': `
				<h1 class="blue-bolt"> Generative Adversarial Networks (GANs) – Image Generation</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> tensorflow.keras.datasets.mnist</p>
					<p><strong>Problem Statement:</strong> A research team wants to generate realistic handwritten digits using GANs. Train a Generative Adversarial Network (GAN) on the MNIST dataset.</p>
					<ul>
						<li>✅ Implement a basic GAN architecture with a generator and discriminator.</li>
						<li>✅ Train the model and visualize generated digits.</li>
						<li>✅ Experiment with DCGAN (Deep Convolutional GANs) for better results.</li>
					</ul>
					<h3>📢 Discussion Topic</h3>
					<p><strong>Key Limitation:</strong> GANs suffer from mode collapse, where the generator produces limited variations of outputs.</p>
					<p><strong>Research Inspiration:</strong> How can diffusion models (e.g., Stable Diffusion) or self-supervised GAN training improve the quality and diversity of generated samples?</p>
				</div>
			`,

			'pythonP14': `
				<h1 class="blue-bolt"> Reinforcement Learning – Training an AI to Play CartPole</h1>
				<div class="content-box">
					<p><strong>Dataset:</strong> OpenAI Gym – CartPole-v1</p>
					<p><strong>Problem Statement:</strong> Train a Deep Q-Network (DQN) to balance a pole on a moving cart.</p>
					<ul>
						<li>✅ Implement Q-learning and improve with Deep Q-Learning.</li>
						<li>✅ Compare policy-based and value-based RL methods.</li>
					</ul>
					<h3>📢 Discussion Topic</h3>
					<p><strong>Key Limitation:</strong> RL models require massive training and are data inefficient.</p>
					<p><strong>Research Inspiration:</strong> Can meta-learning or self-supervised reinforcement learning enable RL agents to generalize across environments?</p>
				</div>
			`,

		};

		contentArea.innerHTML = content[topic] || `<p>⏳ <strong>Under Processing...</strong> Stay tuned! </p>`;

		document.querySelectorAll('.sub-menu div').forEach(item => {
			item.classList.remove('active');
		});
		if (element) {
			element.classList.add('active');
		}
	}
    </script>
</body>
</html>
