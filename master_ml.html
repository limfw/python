<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Master - Machine Learning with SAS Viya & Python</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="header"> 
	            <p>Master - Machine Learning Techniques for Data Mining</p>
    </div>
    <div class="main-container">
        <div class="sidebar">
            <h2 class="orange-accent">Course Content</h2>
			<div class="menu-item"><a href="index.html" style="text-decoration: none; color: inherit;">Basic Python</a></div>
			<div class="menu-item"><a href="ml.html" style="text-decoration: none; color: inherit;">Basic Machine Learning</a></div>			
            <div class="menu-item" onclick="showContent('welcome')">Intermediate - Overview</div>
			
            <div class="menu-item" onclick="toggleSubMenu('ml_inter')">Machine Learning - Intermediate</div>
            <div id="ml_inter" class="sub-menu">
                <div class="menu-item" onclick="toggleSubMenu('ml_foundt')">Foundation of Machine Learning </div>
                <div id="ml_foundt" class="sub-menu">
                    <div onclick="showContent('ml_history')">Historical Development of ML</div>
                    <div onclick="showContent('ml_math')">Mathematical Foundations</div>
                </div>
                
                <div class="menu-item" onclick="toggleSubMenu('ml_arch')">Model Architecture</div>
                <div id="ml_arch" class="sub-menu">
                    <div onclick="showContent('ml_decision_trees')">Decision Trees and Ensemble Methods</div>
                    <div onclick="showContent('ml_neural_networks')">Neural Networks and Universal Approximation</div>
					<div onclick="showContent('ml_svmss')">Support Vector Machines</div>
                </div>
                
                <div class="menu-item" onclick="toggleSubMenu('ml_anl')">Model Analysis</div>
                <div id="ml_anl" class="sub-menu">
                    <div onclick="showContent('ml_fs')">Feature Selection and Dimensionality Reduction</div>
                    <div onclick="showContent('ml_ce')">Cross-Validation and Model Evaluation</div>
                    <div onclick="showContent('ml_pm')">Performance Metrics and Statistical Tests</div>					
                    <div onclick="showContent('ml_ord')">Overfitting, Regularization and Data Imbalance</div>
                </div>

                <div class="menu-item" onclick="toggleSubMenu('ml_modell')">Applications and Implementation</div>
                <div id="ml_modell" class="sub-menu">
                    <div onclick="showContent('ml_bench')">Benchmark ML Problems</div>
                    <div onclick="showContent('ml_science')">ML for Scientific & Engineering Applications</div>
                </div>
                                           
            </div>
			<div class="menu-item" onclick="toggleSubMenu('sas_ml')">SAS Viya for ML</div>
            <div id="sas_ml" class="sub-menu">
                    <div onclick="showContent('sas_viya_intro')">Introduction to SAS Viya : Master</div>
						<div onclick="showContent('viyaP1')">Practical 1</div>
						<div onclick="showContent('viyaP2')">Practical 2</div>
						<div onclick="showContent('viyaP3')">Practical 3</div>
						<div onclick="showContent('viyaP4')">Practical 4</div>
						<div onclick="showContent('viyaP5')">Practical 5</div>
						<div onclick="showContent('viyaP6')">Practical 6</div>
						<div onclick="showContent('viyaP7')">Practical 7</div>	
                </div>
			<div class="menu-item" onclick="toggleSubMenu('python_ml')">Python for ML</div>
            <div id="python_ml" class="sub-menu">
                    <div onclick="showContent('python_libraries')">Python Libraries for ML : Master</div>
						<div onclick="showContent('pythonP1')">Practical 1</div>
						<div onclick="showContent('pythonP2')">Practical 2</div>
						<div onclick="showContent('pythonP3')">Practical 3</div>
						<div onclick="showContent('pythonP4')">Practical 4</div>
						<div onclick="showContent('pythonP5')">Practical 5</div>
						<div onclick="showContent('pythonP6')">Practical 6</div>
						<div onclick="showContent('pythonP7')">Practical 7</div>
                </div>			
			<div class="menu-item" onclick="toggleSubMenu('Others')">Others</div>
			<div id="Others" class="sub-menu">
					<div onclick="showContent('decoding_regression')">Decoding Regression</div>
					<div onclick="showContent('geometric_deep_learning')">Geometric Deep Learning</div>
					<div onclick="showContent('feature_crosses')">Feature Crosses</div>
					<div onclick="showContent('embeddings')">Embeddings</div>
					<div onclick="showContent('genetic_algorithms')">Genetic Algorithms</div>		
                </div>	

        </div>
		
		
        <div class="content" id="content-area">
            <h1 class="blue-bolt">Master Machine Learning Techniques for Data Mining</h1>
				<p>
					This course focuses on the theoretical foundations and practical applications of machine learning. 
					The aim is to provide you with a deep understanding of ML principles, mathematical frameworks, and research methodologies
					while equipping you with hands-on skills to implement ML models in various domains.
				</p>

				<p>
					The main topics that will be discussed include:
					model selection and evaluation, feature selection, supervised and unsupervised learning, neural networks, 
					Bayesian learning, support vector machines, clustering, density estimation, ensemble learning, 
					reinforcement learning, and deep learning architectures.
				</p>

				<p>
					This course also emphasizes practical implementation and research-focused analysis using 
					<strong>Python</strong> and <strong>SAS Viya</strong>. You will learn how to design, analyze, and optimize ML models using 
					Scikit-learn, TensorFlow, PyTorch, and SAS Viya‚Äôs advanced ML tools for large-scale processing.
				</p>

				<p>
					By the end of this course, you will be able to critically evaluate ML models, optimize performance, 
					and apply ML techniques to both real-world problems and research-driven innovations. 
					You will also gain a strong foundation in machine learning theory, enabling you to contribute to ML research and advancements.
				</p>
        </div>
    </div>
    
        <div class="footer">
            <p>Prepare by üòÄ version 1.0 - 2025 ! &  Build a Better Future - 2025 ! </p>
        </div>
    <script>
	function toggleSubMenu(id) {
		var submenu = document.getElementById(id);
		if (submenu) {
			submenu.style.display = submenu.style.display === 'block' ? 'none' : 'block';
		}
	}

	function showContent(topic, element) {
		let contentArea = document.getElementById('content-area');

		let content = {
			'welcome': `
					<h1 class="blue-bolt">üìå Course Overview</h1>

					<div class="content-box">
						<p><strong>Course Objective:</strong> This course is designed to provide a practical yet research-oriented approach to machine learning, leveraging the power of Python and SAS Viya.</p>
						<p>You will explore advanced ML concepts, model architectures, and evaluation techniques, with an emphasis on both theoretical foundations and real-world applications.</p>
					</div>

					<h2 class="blue-bolt">üîç Key Topics Covered</h2>

					<div class="content-box">
						<p>Throughout this course, we will cover essential topics, including:</p>
						<ul>
							<li>‚úî Supervised & Unsupervised Learning(Classification, Regression, Clustering).</li>
							<li>‚úî Decision Trees & Neural Networks (MLP, Deep Learning Architectures).</li>
							<li>‚úî Ensemble Methods (Bagging, Boosting, Random Forests).</li>
							<li>‚úî Reinforcement Learning & Markov Decision Processes.</li>
							<li>‚úî Model Selection, Validation & Robustness in Research.</li>
							<li>‚úî Hands-on implementation using Python & SAS Viya.</li>
						</ul>
					</div>

					<h2 class="blue-bolt">üõ† Practical Implementation</h2>

					<div class="content-box">
						<p>This course combines theoretical depth with practical applications, allowing students to:</p>
						<ul>
							<li>‚úî Work with Python libraries like Scikit-learn, TensorFlow, and PyTorch.</li>
							<li>‚úî Use SAS Viya for large-scale ML model building.</li>
							<li>‚úî Implement end-to-end ML workflows from data preprocessing to model deployment.</li>
						</ul>
					</div>

					<h2 class="blue-bolt">üéØ Learning Outcomes</h2>

					<div class="content-box">
						<p>By the end of this course, you will have the skills to:</p>
						<ul>
							<li>‚úî Build, train, and evaluate ML models for real-world applications.</li>
							<li>‚úî Understand ML model reliability and robustness for scientific research.</li>
							<li>‚úî Deploy AI-powered solutions in diverse domains (business, engineering, and research).</li>
							<li>‚úî Apply theoretical concepts to advance ML research and innovation.</li>
						</ul>
					</div>
			`,
					
			'ml_history': `
				<h1 class="blue-bolt">üìå Historical Development of Machine Learning</h1>

				<div class="content-box">
					<p><strong>Objective:</strong> This lecture explores how Machine Learning (ML) has evolved over time, identifying key breakthroughs, algorithms, and trends that have shaped modern ML research and applications.</p>
					<ul>
						<li>‚úî Understand the historical milestones in ML.</li>
						<li>‚úî Recognize how statistical learning and deep learning evolved.</li>
						<li>‚úî Examine the impact of AI Winters and breakthroughs in computing power.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">1Ô∏è‚É£ Introduction to Machine Learning History</h2>

				<div class="content-box">
					<p><strong>Definition:</strong> Machine Learning is a field of artificial intelligence (AI) that enables computers to learn from data and make decisions without explicit programming.</p>
					<p><strong>Relationship with AI & Data Science:</strong></p>
					<ul>
						<li>‚úî Artificial Intelligence (AI): A broader concept including ML, deep learning, and expert systems.</li>
						<li>‚úî Data Science: The process of extracting insights from data, often using ML algorithms.</li>
					</ul>
					<p><strong>ML Paradigms:</strong></p>
					<ul>
						<li>‚úî Supervised Learning: Trains models on labeled data (e.g., regression, classification).</li>
						<li>‚úî Unsupervised Learning: Identifies patterns in unlabeled data (e.g., clustering, dimensionality reduction).</li>
						<li>‚úî Reinforcement Learning: Uses rewards and penalties to train decision-making models.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">2Ô∏è‚É£ Early Foundations (1950s - 1970s): Birth of AI & ML</h2>

				<div class="content-box">
					<p><strong>Milestones:</strong></p>
					<ul>
						<li>‚úî Alan Turing (1950): Proposed the "Turing Test" to measure machine intelligence.</li>
						<li>‚úî 1952 - Checkers AI (Arthur Samuel): The first self-learning program.</li>
						<li>‚úî 1957 - Perceptron (Frank Rosenblatt):
							<ul>
								<li>‚úÖ First ML model inspired by biological neurons.</li>
								<li>‚úÖ Introduced weights and learning via updates.</li>
								<li>‚ö† Limitation: Failed to solve the XOR problem (linear separability issue).</li>
							</ul>
						</li>
						<li>‚úî 1960s - Symbolic AI vs. Statistical AI:
							<ul>
								<li>‚úÖ Symbolic AI: Rule-based expert systems.</li>
								<li>‚úÖ Statistical AI: Early probabilistic models.</li>
							</ul>
						</li>
						<li>‚úî 1970s - AI Winter #1: High expectations but limited computing power led to disillusionment.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">3Ô∏è‚É£ Statistical Learning & Rise of Practical ML (1980s - 1990s)</h2>

				<div class="content-box">
					<p><strong>Breakthroughs:</strong></p>
					<ul>
						<li>‚úî 1986 - Backpropagation Algorithm (Rumelhart, Hinton, Williams):
							<ul>
								<li>‚úÖ Solved the XOR problem.</li>
								<li>‚úÖ Enabled training of multi-layer perceptrons (MLP).</li>
							</ul>
						</li>
						<li>‚úî 1989 - Support Vector Machines (SVMs) (Vapnik & Cortes):
							<ul>
								<li>‚úÖ Introduced the kernel trick for non-linear classification.</li>
								<li>‚úÖ Maximized the decision margin for better generalization.</li>
							</ul>
						</li>
						<li>‚úî Ensemble Learning:
							<ul>
								<li>‚úÖ 1995 - Random Forests (Breiman): Multiple decision trees for better accuracy.</li>
								<li>‚úÖ 1997 - AdaBoost (Freund & Schapire): Improved weak classifiers iteratively.</li>
							</ul>
						</li>
						<li>‚úî 1990s - AI Winter #2: Computational limits slowed deep learning progress.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">4Ô∏è‚É£ The Rise of Machine Learning & Data Revolution (2000s - 2010s)</h2>

				<div class="content-box">
					<p><strong>Key Advancements:</strong></p>
					<ul>
						<li>‚úî 2006 - Deep Learning Revolution (Hinton, Bengio, LeCun):
							<ul>
								<li>‚úÖ Introduced unsupervised pre-training for deep networks.</li>
								<li>‚úÖ Developed Restricted Boltzmann Machines (RBMs) & Autoencoders.</li>
							</ul>
						</li>
						<li>‚úî 2009 - ImageNet Dataset (Fei-Fei Li):** Enabled large-scale deep learning research.</li>
						<li>‚úî 2012 - AlexNet (Krizhevsky, Sutskever, Hinton):
							<ul>
								<li>‚úÖ Demonstrated superiority of deep CNNs.</li>
								<li>‚úÖ Marked the beginning of deep learning dominance.</li>
							</ul>
						</li>
						<li>‚úî 2014 - Generative Adversarial Networks (GANs) (Goodfellow et al.): Enabled AI-generated media.</li>
						<li>‚úî 2017 - Transformers & Attention Mechanism (Vaswani et al.):
							<ul>
								<li>‚úÖ Replaced CNNs & RNNs with self-attention**.</li>
								<li>‚úÖ Led to models like GPT, BERT, and modern LLMs.</li>
							</ul>
						</li>
					</ul>
				</div>

				<h2 class="blue-bolt">5Ô∏è‚É£ Modern AI: Deep Learning, Foundation Models & Beyond (2020s - Present)</h2>

				<div class="content-box">
					<p><strong>Recent Trends:</strong></p>
					<ul>
						<li>‚úî Self-Supervised Learning (SimCLR, MoCo, BYOL): Learning representations without labels.</li>
						<li>‚úî Large Language Models (GPT, BERT, DeepSeek, OpenAI models): Scaling ML for text understanding.</li>
						<li>‚úî Multimodal AI (CLIP, DALL-E, Sora): Combining vision, text, and speech.</li>
						<li>‚úî AI for Scientific Discovery (AlphaFold, AI in healthcare & physics): ML-driven research acceleration.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">6Ô∏è‚É£ Ethical Considerations & Future of ML</h2>

				<div class="content-box">
					<p><strong>Challenges & Research Directions:</strong></p>
					<ul>
						<li>‚úî Bias & Fairness: Avoiding discrimination in AI models.</li>
						<li>‚úî Explainability & Trustworthiness: SHAP, LIME, XAI techniques.</li>
						<li>‚úî AI Alignment & Safety: Reinforcement Learning with Human Feedback (RLHF).</li>
						<li>‚úî Future Research Areas: Neuromorphic computing, Quantum AI, Causal ML.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">üîç Lecture Summary & Key Takeaways</h2>

				<div class="content-box">
					<ul>
						<li>‚úî ML evolved from rule-based AI to statistical learning to deep learning.</li>
						<li>‚úî Breakthroughs like backpropagation, SVMs, and transformers shaped modern ML.</li>
						<li>‚úî Trends focus on large-scale AI, self-supervised learning, and ethical AI.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">üìö Recommended Readings</h2>

				<div class="content-box">
					<ul>
						<li>üìñ <strong>The Perceptron</strong> - F. Rosenblatt (1958).</li>
						<li>üìñ <strong>A Training Algorithm for Optimal Margin Classifiers</strong> - Cortes & Vapnik (1995).</li>
						<li>üìñ <strong>Deep Learning</strong> - Ian Goodfellow, Yoshua Bengio, Aaron Courville (2016).</li>
					</ul>
				</div>		
			`,

			'ml_math': `
				<h1 class="blue-bolt">üìå Mathematical Foundations for Machine Learning</h1>

				<div class="content-box">
					<p><strong>Overview:</strong> A strong mathematical foundation is crucial for understanding the inner workings of machine learning models.</p>
					<p>This section covers key mathematical concepts required to build, optimize, and analyze ML models.</p>
				</div>

				<h2 class="blue-bolt">‚û§ Calculus: Derivatives, Gradients & Hessians</h2>

				<div class="content-box">
					<p><strong>Importance in ML:</strong> Calculus is the backbone of optimization techniques used in training machine learning models.</p>
					<ul>
						<li>‚úî <strong>Derivatives</strong> ‚Äì Measure the rate of change; used in gradient-based optimization.</li>
						<li>‚úî <strong>Gradients</strong> ‚Äì Multi-dimensional derivatives used in <strong>Gradient Descent</strong>.</li>
						<li>‚úî <strong>Hessians</strong> ‚Äì Second-order derivatives used in advanced optimization (Newton‚Äôs method).</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Linear Algebra: Vector Spaces, Eigenvalues & Eigenvectors</h2>

				<div class="content-box">
					<p><strong>Why it matters:</strong> Linear algebra is fundamental for data transformations, dimensionality reduction, and ML algorithms.</p>
					<ul>
						<li>‚úî <strong>Vector Spaces</strong> ‚Äì Foundation of numerical data representation in ML.</li>
						<li>‚úî <strong>Eigenvalues & Eigenvectors</strong> ‚Äì Core concepts in <strong>Principal Component Analysis (PCA)</strong> and <strong>Singular Value Decomposition (SVD)</strong>.</li>
						<li>‚úî <strong>Matrix Operations</strong> ‚Äì Crucial for <strong>neural networks, covariance matrices, and transformations</strong>.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Probability & Statistics: Bayes‚Äô Theorem, Distributions & Hypothesis Testing</h2>

				<div class="content-box">
					<p><strong>Why it matters:</strong> ML models make decisions based on probabilities and statistical reasoning.</p>
					<ul>
						<li>‚úî <strong>Bayes' Theorem</strong> ‚Äì Key concept for probabilistic models (e.g., Na√Øve Bayes classifier).</li>
						<li>‚úî <strong>Probability Distributions</strong> ‚Äì Normal, Bernoulli, Poisson distributions (used in modeling uncertainty).</li>
						<li>‚úî <strong>Hypothesis Testing</strong> ‚Äì Used to validate ML model performance (e.g., t-tests, p-values).</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Information Theory: Entropy & KL Divergence</h2>

				<div class="content-box">
					<p><strong>Application in ML:</strong> Information theory helps in feature selection, model regularization, and compression.</p>
					<ul>
						<li>‚úî <strong>Entropy</strong> ‚Äì Measures uncertainty in probability distributions.</li>
						<li>‚úî <strong>KL Divergence</strong> ‚Äì Measures the difference between two probability distributions (used in <strong>model selection and variational inference</strong>).</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Optimization Methods: Gradient Descent, Newton‚Äôs Method & Lagrange Multipliers</h2>

				<div class="content-box">
					<p><strong>Core to ML training:</strong> Optimization methods help minimize loss functions and improve model accuracy.</p>
					<ul>
						<li>‚úî <strong>Gradient Descent</strong> ‚Äì Most common optimization method in <strong>deep learning</strong>.</li>
						<li>‚úî <strong>Newton‚Äôs Method</strong> ‚Äì Second-order optimization for convex problems.</li>
						<li>‚úî <strong>Lagrange Multipliers</strong> ‚Äì Used in constrained optimization (e.g., <strong>SVMs</strong> and <strong>regularization</strong>).</li>
					</ul>
				</div>

				<h2 class="blue-bolt">üéØ Key Takeaways</h2>

				<div class="content-box">
					<ul>
						<li>‚úî <strong>Understanding derivatives, gradients, and Hessians</strong> is essential for optimizing ML models.</li>
						<li>‚úî <strong>Linear algebra underpins PCA, SVD, and neural network computations.</strong></li>
						<li>‚úî <strong>Probability & statistics provide a framework for uncertainty modeling and inference.</strong></li>
						<li>‚úî <strong>Information theory concepts like entropy and KL divergence aid in model evaluation.</strong></li>
						<li>‚úî <strong>Optimization techniques drive model training efficiency and performance.</strong></li>
					</ul>
				</div>
				`,
				
			'ml_decision_trees': `
				<h1 class="blue-bolt">üìå Decision Trees & Ensemble Methods</h1>

				<div class="content-box">
					<p><strong>Overview:</strong> Decision trees are fundamental supervised learning models that make predictions using <strong>hierarchical, rule-based structures</strong>. Ensemble methods enhance decision trees by combining multiple weak learners to achieve <strong>higher accuracy and robustness</strong>.</p>
				</div>

				<h2 class="blue-bolt">‚û§ Key Concepts in Decision Trees</h2>

				<div class="content-box">
					<p><strong>How Decision Trees Work:</strong> A tree structure is formed where internal nodes represent decision rules, branches represent outcomes, and leaves represent final predictions.</p>
					<ul>
						<li>‚úî <strong>Information Gain</strong> ‚Äì Measures reduction in uncertainty when splitting nodes.</li>
						<li>‚úî <strong>Gini Impurity</strong> ‚Äì Measures impurity of a node (used in CART).</li>
						<li>‚úî <strong>Entropy</strong> ‚Äì Measures randomness in data (used in ID3 algorithm).</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Preventing Overfitting: Pruning Techniques</h2>

				<div class="content-box">
					<p><strong>Overfitting occurs when a decision tree becomes too complex</strong> and memorizes noise instead of generalizing.</p>
					<ul>
						<li>‚úî <strong>Pre-pruning (Early Stopping)</strong> ‚Äì Limits tree depth to prevent unnecessary splits.</li>
						<li>‚úî <strong>Post-pruning</strong> ‚Äì Removes weak branches <strong>after training</strong> to improve generalization.</li>
						<li>‚úî <strong>Minimum Split Criterion</strong> ‚Äì Requires a node to have a minimum number of samples before splitting.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Ensemble Methods: Boosting & Bagging</h2>

				<div class="content-box">
					<p><strong>Why Ensembles?</strong> Instead of relying on a <strong>single decision tree</strong>, ensemble methods combine multiple models to enhance predictive power and reduce variance.</p>
					<ul>
						<li>‚úî <strong>Random Forest</strong> ‚Äì Builds multiple trees using <strong>bootstrap aggregation (bagging)</strong>.</li>
						<li>‚úî <strong>AdaBoost (Adaptive Boosting)</strong> ‚Äì Assigns <strong>higher weights to misclassified instances</strong>, focusing on difficult cases.</li>
						<li>‚úî <strong>XGBoost (Extreme Gradient Boosting)</strong> ‚Äì A highly optimized gradient boosting algorithm for structured data.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">üéØ Key Takeaways</h2>

				<div class="content-box">
					<ul>
						<li>‚úî <strong>Decision trees split data using entropy, information gain, and Gini impurity.</strong></li>
						<li>‚úî <strong>Pruning helps prevent overfitting by simplifying tree structures.</strong></li>
						<li>‚úî <strong>Ensemble methods like Random Forest, AdaBoost, and XGBoost improve accuracy and robustness.</strong></li>
						<li>‚úî <strong>Boosting algorithms focus on difficult examples to enhance learning efficiency.</strong></li>
					</ul>
				</div>
			`,

			'ml_neural_networks': `
				<h1 class="blue-bolt">üìå Neural Networks & Universal Approximation</h1>

				<div class="content-box">
					<p><strong>Overview:</strong> Neural networks are a class of machine learning models inspired by the human brain, capable of learning complex patterns from data.</p>
					<p>This lecture covers the **universal approximation theorem**, fundamental neural network architectures, optimization strategies, and techniques to address training challenges.</p>
				</div>

				<h2 class="blue-bolt">‚û§ Multilayer Perceptron (MLP)</h2>

				<div class="content-box">
					<p><strong>What is an MLP?</strong> A fully connected neural network where each layer consists of neurons connected to all neurons in the next layer.</p>
					<ul>
						<li>‚úî <strong>Activation Functions:</strong> Introduce non-linearity into the network.</li>
						<li>‚úî <strong>Common Activation Functions:</strong>
							<ul>
								<li>‚ñ∂ <strong>Sigmoid</strong> ‚Äì Used in early neural networks but suffers from vanishing gradients.</li>
								<li>‚ñ∂ <strong>ReLU (Rectified Linear Unit)</strong> ‚Äì Helps mitigate vanishing gradients.</li>
								<li>‚ñ∂ <strong>Leaky ReLU & ELU</strong> ‚Äì Alternative to ReLU that allows small negative outputs.</li>
							</ul>
						</li>
						<li>‚úî <strong>Backpropagation:</strong> Algorithm for training neural networks by computing gradients using chain rule differentiation.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Radial-Basis Function Networks (RBFN)</h2>

				<div class="content-box">
					<p><strong>What is RBFN?</strong> A neural network model that uses radial basis functions as activation functions.</p>
					<ul>
						<li>‚úî <strong>Role in Function Approximation:</strong> Can approximate any continuous function.</li>
						<li>‚úî <strong>Key Features:</strong>
							<ul>
								<li>‚ñ∂ Uses <strong>Gaussian kernels</strong> to compute distance-based activations.</li>
								<li>‚ñ∂ More interpretable than deep networks.</li>
								<li>‚ñ∂ Effective for regression and classification tasks.</li>
							</ul>
						</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Deep Neural Networks (DNNs)</h2>

				<div class="content-box">
					<p><strong>What makes a network "deep"?</strong> A DNN consists of multiple hidden layers that allow hierarchical feature extraction.</p>
					<p><strong>Challenges in Deep Learning:</strong></p>
					<ul>
						<li>‚úî <strong>Vanishing Gradient Problem:</strong> Gradients diminish as they propagate backward, slowing learning.</li>
						<li>‚úî <strong>Solutions:</strong>
							<ul>
								<li>‚ñ∂ <strong>Batch Normalization:</strong> Normalizes activations to stabilize learning.</li>
								<li>‚ñ∂ <strong>ReLU:</strong> Helps prevent gradients from shrinking.</li>
								<li>‚ñ∂ <strong>Residual Connections (ResNets):</strong> Allow gradients to bypass multiple layers.</li>
							</ul>
						</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Optimization Techniques</h2>

				<div class="content-box">
					<p><strong>Gradient-based optimization algorithms improve convergence and performance.</strong></p>
					<ul>
						<li>‚úî <strong>Adam (Adaptive Moment Estimation):</strong> Uses first and second moments of gradients for adaptive learning.</li>
						<li>‚úî <strong>RMSProp (Root Mean Square Propagation):</strong> Maintains a moving average of squared gradients to adjust learning rates.</li>
						<li>‚úî <strong>Nesterov Momentum:</strong> Improves momentum-based gradient descent by looking ahead at future gradients.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">üéØ Key Takeaways</h2>

				<div class="content-box">
					<ul>
						<li>‚úî <strong>MLPs use backpropagation and activation functions to learn representations.</strong></li>
						<li>‚úî <strong>RBFNs are effective function approximators using radial basis functions.</strong></li>
						<li>‚úî <strong>DNNs require techniques like BatchNorm and ReLU to mitigate vanishing gradients.</strong></li>
						<li>‚úî <strong>Optimization techniques like Adam and RMSProp improve training efficiency.</strong></li>
					</ul>
				</div>
			`,

			'ml_svmss': `
				<h1 class="blue-bolt">üìå Support Vector Machines (SVMs)</h1>

				<div class="content-box">
					<p><strong>Overview:</strong> Support Vector Machines (SVMs) are powerful supervised learning models used for classification and regression.</p>
					<p>SVMs are based on the principle of <strong>finding the optimal hyperplane</strong> that best separates data points into different classes.</p>
				</div>

				<h2 class="blue-bolt">‚û§ Hard-Margin vs. Soft-Margin SVM</h2>

				<div class="content-box">
					<p><strong>What is the Margin?</strong> The margin is the distance between the decision boundary (hyperplane) and the nearest data points.</p>
					<ul>
						<li>‚úî <strong>Hard-Margin SVM:</strong>
							<ul>
								<li>‚ñ∂ Assumes data is <strong>linearly separable</strong> (no overlap between classes).</li>
								<li>‚ñ∂ Finds a hyperplane that <strong>maximizes the margin</strong> with no tolerance for misclassification.</li>
								<li>‚ñ∂ Not suitable for noisy or non-linearly separable data.</li>
							</ul>
						</li>
						<li>‚úî <strong>Soft-Margin SVM:</strong>
							<ul>
								<li>‚ñ∂ Allows <strong>some misclassification</strong> by introducing a penalty term (C-parameter).</li>
								<li>‚ñ∂ Useful when data is <strong>not perfectly separable</strong>.</li>
								<li>‚ñ∂ Balances margin maximization and classification error.</li>
							</ul>
						</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Kernel Trick: Polynomial, RBF, and Sigmoid</h2>

				<div class="content-box">
					<p><strong>What is the Kernel Trick?</strong> The kernel trick allows SVMs to operate in <strong>higher-dimensional spaces</strong> without explicitly computing transformations.</p>
					<ul>
						<li>‚úî <strong>Polynomial Kernel:</strong>
							<ul>
								<li>‚ñ∂ Extends linear SVMs to capture curved decision boundaries.</li>
								<li>‚ñ∂ Useful when data has polynomial relationships.</li>
							</ul>
						</li>
						<li>‚úî <strong>Radial Basis Function (RBF) Kernel:</strong>
							<ul>
								<li>‚ñ∂ Maps data into an <strong>infinite-dimensional space<>/strong>.</li>
								<li>‚ñ∂ Effective for handling <strong>non-linearly separable data</strong>.</li>
							</ul>
						</li>
						<li>‚úî <strong>Sigmoid Kernel:</strong>
							<ul>
								<li>‚ñ∂ Mimics behavior of a <strong>neural network activation function</strong>.</li>
								<li>‚ñ∂ Used in <strong>some classification problems but less common than RBF</strong>.</li>
							</ul>
						</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Connection to Maximum Margin Classifiers</h2>

				<div class="content-box">
					<p><strong>Why SVMs are Maximum Margin Classifiers?</strong></p>
					<ul>
						<li>‚úî SVMs aim to <strong>maximize the margin</strong> between data points from different classes.</li>
						<li>‚úî A <strong>wider margin</strong> reduces overfitting and improves generalization.</li>
						<li>‚úî Works best with <strong>high-dimensional feature spaces</strong>.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Dual Formulation with Lagrangian Optimization</h2>

				<div class="content-box">
					<p><strong>Mathematical Foundation:</strong> The SVM optimization problem can be solved using the <strong>dual formulation</strong> with <strong>Lagrange multipliers</strong>.</p>
					<ul>
						<li>‚úî The <strong>primal problem</strong> involves finding a hyperplane that separates data while maximizing margin.</li>
						<li>‚úî The <strong>dual problem</strong> reformulates the objective function using <strong>Lagrange multipliers</strong> to simplify computations.</li>
						<li>‚úî The <strong>kernel trick</strong> applies directly in the dual formulation, allowing computation in high-dimensional spaces.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">üéØ Key Takeaways</h2>

				<div class="content-box">
					<ul>
						<li>‚úî <strong>Hard-margin SVM works only for linearly separable data, while soft-margin SVM handles noise.</strong></li>
						<li>‚úî <strong>The kernel trick enables SVMs to handle non-linear classification tasks.</strong></li>
						<li>‚úî <strong>Maximum margin classifiers improve generalization by maximizing decision boundaries.</strong></li>
						<li>‚úî <strong>Dual formulation with Lagrangian optimization makes SVMs computationally efficient.</strong></li>
					</ul>
				</div>
			`,

			'ml_fs': `
				<h1 class="blue-bolt">üìå Feature Selection & Dimensionality Reduction</h1>

				<div class="content-box">
					<p><strong>Overview:</strong> Feature selection and dimensionality reduction techniques help improve model performance by eliminating irrelevant or redundant features.</p>
				</div>

				<h2 class="blue-bolt">‚û§ Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), Independent Component Analysis (ICA)</h2>

				<div class="content-box">
					<ul>
						<li>‚úî <strong>PCA:</strong> Uses eigenvectors and eigenvalues to project data onto principal components for variance maximization.</li>
						<li>‚úî <strong>LDA:</strong> Maximizes class separability by finding a linear combination of features.</li>
						<li>‚úî <strong>ICA:</strong> Extracts statistically independent signals from mixed data.</li>
						<li>‚úî <strong>Whitening:</strong> Standardizes features to remove correlations before applying PCA or ICA.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Autoencoders as Feature Extractors</h2>

				<div class="content-box">
					<p><strong>What are Autoencoders?</strong> Neural networks trained to encode data into lower-dimensional representations.</p>
					<ul>
						<li>‚úî Useful for **unsupervised feature learning**.</li>
						<li>‚úî Reduces dimensionality while preserving **non-linear structures** in data.</li>
						<li>‚úî Common in **deep learning applications**.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Recursive Feature Elimination (RFE)</h2>

				<div class="content-box">
					<p><strong>What is RFE?</strong> An iterative process to remove least important features and retrain the model.</p>
					<ul>
						<li>‚úî Helps in improving **model efficiency and interpretability**.</li>
						<li>‚úî Commonly used with **tree-based models and linear models**.</li>
					</ul>
				</div>
			`,
			'ml_ce': `
				<h1 class="blue-bolt">üìå Cross-Validation & Model Evaluation</h1>

				<div class="content-box">
					<p><strong>Overview:</strong> Cross-validation techniques help evaluate machine learning models to ensure robustness and avoid overfitting.</p>
				</div>

				<h2 class="blue-bolt">‚û§ k-Fold Cross-Validation & Bias-Variance Tradeoff</h2>

				<div class="content-box">
					<ul>
						<li>‚úî <strong>k-Fold Cross-Validation:</strong> Splits data into k subsets and trains models k times to estimate performance.</li>
						<li>‚úî Bias-Variance Tradeoff: Balances model flexibility and generalization.</li>
						<li>‚úî Common k values: 5-fold, 10-fold.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Train-Test Split: Why and How?</h2>

				<div class="content-box">
					<p><strong>Importance of Train-Test Splits:</strong> Prevents models from memorizing training data and ensures generalization.</p>
					<ul>
						<li>‚úî Common splits: 70/30, 80/20, 90/10.</li>
						<li>‚úî <strong>Stratified Splitting:</strong> Ensures class distribution remains consistent.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Bootstrapping & Monte Carlo Methods</h2>

				<div class="content-box">
					<ul>
						<li>‚úî <strong>Bootstrapping:</strong> Creates multiple datasets by resampling with replacement.</li>
						<li>‚úî <strong>Monte Carlo Methods:</strong> Uses repeated random sampling to estimate model performance.</li>
					</ul>
				</div>
			`,

			'ml_pm': `
				<h1 class="blue-bolt">üìå Performance Metrics & Statistical Tests</h1>

				<div class="content-box">
					<p><strong>Overview:</strong> Proper evaluation metrics ensure models are correctly assessed for classification and regression tasks.</p>
				</div>

				<h2 class="blue-bolt">‚û§ Precision, Recall, F1-Score, and AUC-ROC</h2>

				<div class="content-box">
					<ul>
						<li>‚úî <strong>Precision:</strong> Measures true positive rate among predicted positives.</li>
						<li>‚úî <strong>Recall:</strong> Measures ability to detect actual positives.</li>
						<li>‚úî <strong>F1-Score:</strong> Harmonic mean of precision and recall.</li>
						<li>‚úî <strong>AUC-ROC:</strong> Evaluates classifier performance at different thresholds.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Confusion Matrices & Misclassification Costs</h2>

				<div class="content-box">
					<p><strong>Confusion Matrix:</strong> Provides insights into false positives and false negatives.</p>
					<ul>
						<li>‚úî Useful for imbalanced datasets.</li>
						<li>‚úî Helps in identifying misclassification patterns.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Statistical Significance Tests for ML Models</h2>

				<div class="content-box">
					<ul>
						<li>‚úî <strong>t-Test:</strong> Determines if two means are statistically different.</li>
						<li>‚úî <strong>Chi-Square Test:</strong> Checks for independence between categorical variables.</li>
					</ul>
				</div>
			`,

			'ml_ord': `
				<h1 class="blue-bolt">üìå Overfitting, Regularization & Data Imbalance</h1>

				<div class="content-box">
					<p><strong>Overview:</strong> Overfitting occurs when models learn noise instead of patterns. Regularization and data balancing techniques help improve generalization.</p>
				</div>

				<h2 class="blue-bolt">‚û§ L1 vs. L2 Regularization (Lasso vs. Ridge Regression)</h2>

				<div class="content-box">
					<ul>
						<li>‚úî <strong>L1 Regularization (Lasso):</strong> Shrinks coefficients to zero, useful for feature selection.</li>
						<li>‚úî <strong>L2 Regularization (Ridge):</strong> Penalizes large coefficients but does not force them to zero.</li>
						<li>‚úî Elastic Net: Combination of L1 and L2 for best of both worlds.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Dropout & Early Stopping in Neural Networks</h2>

				<div class="content-box">
					<p><strong>Why Regularization in Deep Learning?</strong> Prevents networks from memorizing training data.</p>
					<ul>
						<li>‚úî <strong>Dropout:</strong> Randomly disables neurons during training to force robustness.</li>
						<li>‚úî <strong>Early Stopping:</strong> Stops training once validation loss stops improving.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Techniques for Handling Imbalanced Data</h2>

				<div class="content-box">
					<ul>
						<li>‚úî <strong>SMOTE (Synthetic Minority Over-sampling Technique):</strong> Generates synthetic minority samples.</li>
						<li>‚úî <strong>Weighted Loss Functions:</strong> Assigns higher penalty for misclassified minority class samples.</li>
						<li>‚úî <strong>Undersampling:</strong> Removes excess majority class samples to balance dataset.</li>
					</ul>
				</div>
			`,

			'ml_bench': `
				<h1 class="blue-bolt">üìå Benchmark ML Problems</h1>

				<div class="content-box">
					<p><strong>Overview:</strong> Benchmark ML problems help evaluate algorithms by testing their ability to solve well-known challenges.</p>
				</div>

				<h2 class="blue-bolt">‚û§ XOR Problem: Why Perceptron Fails & Solving with MLP</h2>

				<div class="content-box">
					<p><strong>The XOR problem is a classic example where single-layer perceptrons fail.</strong></p>
					<ul>
						<li>‚úî <strong>Why Perceptron Fails?</strong> Perceptron is a linear classifier, while XOR is not linearly separable.</li>
						<li>‚úî <strong>Solution:</strong> Using Multilayer Perceptron (MLP) with hidden layers introduces non-linearity.</li>
						<li>‚úî Activation functions like ReLU and Sigmoid enable learning complex decision boundaries.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Iris Flower Classification: Linear Separability & Decision Boundaries</h2>

				<div class="content-box">
					<p><strong>The Iris dataset is a fundamental classification problem in ML.</strong></p>
					<ul>
						<li>‚úî Classes: Setosa, Versicolor, and Virginica.</li>
						<li>‚úî Key Challenge: Understanding linear vs. non-linear separability.</li>
						<li>‚úî Decision Boundaries: Logistic Regression for linear separation, SVM or neural networks for complex cases.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ Function Approximation: Using ML to Model Complex Functions</h2>

				<div class="content-box">
					<p><strong>ML models can approximate continuous functions, a core concept in regression and deep learning.</strong></p>
					<ul>
						<li>‚úî Polynomial Regression: Models non-linear relationships with polynomial features.</li>
						<li>‚úî Neural Networks: Universal approximators capable of modeling arbitrary functions.</li>
						<li>‚úî Gaussian Processes: Bayesian approach to function approximation with uncertainty estimation.</li>
					</ul>
				</div>
			`,
			'ml_science': `
				<h1 class="blue-bolt">üìå Machine Learning for Scientific & Engineering Applications</h1>

				<div class="content-box">
					<p><strong>Overview:</strong> ML is increasingly used in scientific and engineering domains to analyze data, predict trends, and optimize systems.</p>
				</div>

				<h2 class="blue-bolt">‚û§ Time-Series Forecasting in Engineering</h2>

				<div class="content-box">
					<p><strong>ML plays a key role in predictive maintenance, energy forecasting, and system modeling.</strong></p>
					<ul>
						<li>‚úî ARIMA & Exponential Smoothing: Traditional statistical models.</li>
						<li>‚úî LSTMs & Transformer Networks: Advanced models for capturing sequential dependencies.</li>
						<li>‚úî Applications: Load forecasting, traffic prediction, failure detection.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ ML in Physics, Biology, and Medicine</h2>

				<div class="content-box">
					<p><strong>ML accelerates discoveries in physics, genomics, and healthcare.</strong></p>
					<ul>
						<li>‚úî Physics: Identifying patterns in high-energy particle collisions.</li>
						<li>‚úî Biology: Analyzing genetic sequences, protein folding (e.g., AlphaFold).</li>
						<li>‚úî Medicine: Disease diagnosis, medical image processing, drug discovery.</li>
					</ul>
				</div>

				<h2 class="blue-bolt">‚û§ ML in Material Discovery, Climate Modeling, and Finance</h2>

				<div class="content-box">
					<ul>
						<li>‚úî Material Discovery: Predicting new materials with optimal properties using ML models.</li>
						<li>‚úî Climate Modeling: Analyzing weather patterns and predicting climate change impacts.</li>
						<li>‚úî Finance: Risk assessment, fraud detection, and algorithmic trading.</li>
					</ul>
				</div>
			`,

			'sas_viya_intro': `
				<div class="ml-section">
					<h2>üìå Introduction to SAS Viya</h2>

					<div class="content-box">
						<h3>1Ô∏è‚É£ What is SAS Viya?</h3>
						<p><strong>Definition:</strong> SAS Viya is a cloud-enabled, scalable, and high-performance analytics platform designed for data science, machine learning, and artificial intelligence.</p>
						<p>It supports a variety of programming languages, including **SAS, Python, R, and REST APIs**, making it a flexible tool for **big data processing and model deployment**.</p>
					</div>

					<div class="content-box">
						<h3>2Ô∏è‚É£ Why Use SAS Viya for Machine Learning?</h3>
						<ul>
							<li>‚úî <strong>High-Performance Computing:</strong> Supports in-memory distributed processing.</li>
							<li>‚úî <strong>Automated Machine Learning (AutoML):</strong> Quickly builds models with optimized hyperparameters.</li>
							<li>‚úî <strong>Seamless Integration:</strong> Works with Python, R, and cloud-based services.</li>
							<li>‚úî <strong>Interactive Interface:</strong> Provides both GUI-based (SAS Studio) and code-based solutions.</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>3Ô∏è‚É£ Overview of Practical Exercises</h3>
						<p>To help readers gain hands-on experience, we provide 7 practical exercises that demonstrate how to use SAS Viya for machine learning tasks.</p>
						<p>Each practical focuses on different aspects of data handling, preprocessing, modeling, and evaluation in SAS Viya.</p>
					</div>

				</div>
			`,


			'python_libraries': `
				<div class="ml-section">
					<h2>üìå Introduction to Python for Machine Learning</h2>

					<div class="content-box">
						<h3>1Ô∏è‚É£ Why Use Python for Machine Learning?</h3>
						<p><strong>Definition:</strong> Python is the most widely used programming language for machine learning due to its simplicity, extensive libraries, and strong community support.</p>
						<p>Python provides powerful libraries such as NumPy, Pandas, Scikit-Learn, TensorFlow, and PyTorch, making it a go-to language for data processing, model building, and deep learning applications.</p>
					</div>

					<div class="content-box">
						<h3>2Ô∏è‚É£ Key Features of Python for ML</h3>
						<ul>
							<li>‚úî <strong>Easy-to-Use Syntax:</strong> Allows quick prototyping and experimentation.</li>
							<li>‚úî <strong>Rich Ecosystem:</strong> Comes with numerous ML/DL libraries (Scikit-Learn, TensorFlow, PyTorch, etc.).</li>
							<li>‚úî <strong>Visualization Capabilities:</strong> Matplotlib and Seaborn for detailed data analysis.</li>
							<li>‚úî <strong>Scalability & Performance:</strong> Compatible with cloud and distributed computing platforms.</li>
						</ul>
					</div>

					<div class="content-box">
						<h3>3Ô∏è‚É£ Overview of Practical Exercises</h3>
						<p>To help readers gain hands-on experience, we provide 7 practical exercises demonstrating Python's capabilities in machine learning.</p>
						<p>Each practical covers different aspects of data preprocessing, modeling, optimization, and evaluation in Python.</p>
					</div>

				</div>
			`,
			
			
		};

		contentArea.innerHTML = content[topic] || `<p>‚è≥ <strong>Under Processing...</strong> Stay tuned! üöÄ</p>`;

		document.querySelectorAll('.sub-menu div').forEach(item => {
			item.classList.remove('active');
		});
		if (element) {
			element.classList.add('active');
		}
	}
    </script>
</body>
</html>
